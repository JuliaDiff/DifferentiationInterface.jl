var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/","page":"API","title":"API","text":"DifferentiationInterface","category":"page"},{"location":"api/#DifferentiationInterface","page":"API","title":"DifferentiationInterface","text":"DifferentiationInterface\n\nAn interface to various automatic differentiation backends in Julia.\n\n\n\n\n\n","category":"module"},{"location":"api/#Argument-wrappers","page":"API","title":"Argument wrappers","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Context\nConstant","category":"page"},{"location":"api/#DifferentiationInterface.Context","page":"API","title":"DifferentiationInterface.Context","text":"Context\n\nAbstract supertype for additional context arguments, which can be passed to differentiation operators after the active input x but are not differentiated.\n\nSee also\n\nConstant\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.Constant","page":"API","title":"DifferentiationInterface.Constant","text":"Constant\n\nConcrete type of Context argument which is kept constant during differentiation.\n\nNote that an operator can be prepared with an arbitrary value of the constant. However, same-point preparation must occur with the exact value that will be reused later.\n\nExample\n\njulia> using DifferentiationInterface\n\njulia> import ForwardDiff\n\njulia> f(x, c) = c * sum(abs2, x);\n\njulia> gradient(f, AutoForwardDiff(), [1.0, 2.0], Constant(10))\n2-element Vector{Float64}:\n 20.0\n 40.0\n\njulia> gradient(f, AutoForwardDiff(), [1.0, 2.0], Constant(100))\n2-element Vector{Float64}:\n 200.0\n 400.0\n\n\n\n\n\n","category":"type"},{"location":"api/#First-order","page":"API","title":"First order","text":"","category":"section"},{"location":"api/#Pushforward","page":"API","title":"Pushforward","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_pushforward\nprepare_pushforward_same_point\npushforward\npushforward!\nvalue_and_pushforward\nvalue_and_pushforward!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_pushforward","page":"API","title":"DifferentiationInterface.prepare_pushforward","text":"prepare_pushforward(f,     backend, x, tx, [contexts...]) -> prep\nprepare_pushforward(f!, y, backend, x, tx, [contexts...]) -> prep\n\nCreate a prep object that can be given to pushforward and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pushforward_same_point","page":"API","title":"DifferentiationInterface.prepare_pushforward_same_point","text":"prepare_pushforward_same_point(f,     backend, x, tx, [contexts...]) -> prep_same\nprepare_pushforward_same_point(f!, y, backend, x, tx, [contexts...]) -> prep_same\n\nCreate an prep_same object that can be given to pushforward and its variants if they are applied at the same point x and with the same contexts.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward","page":"API","title":"DifferentiationInterface.pushforward","text":"pushforward(f,     [prep,] backend, x, tx, [contexts...]) -> ty\npushforward(f!, y, [prep,] backend, x, tx, [contexts...]) -> ty\n\nCompute the pushforward of the function f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named jvp.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward!","page":"API","title":"DifferentiationInterface.pushforward!","text":"pushforward!(f,     dy, [prep,] backend, x, tx, [contexts...]) -> ty\npushforward!(f!, y, dy, [prep,] backend, x, tx, [contexts...]) -> ty\n\nCompute the pushforward of the function f at point x with a tuple of tangents tx, overwriting ty.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named jvp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward","page":"API","title":"DifferentiationInterface.value_and_pushforward","text":"value_and_pushforward(f,     [prep,] backend, x, tx, [contexts...]) -> (y, ty)\nvalue_and_pushforward(f!, y, [prep,] backend, x, tx, [contexts...]) -> (y, ty)\n\nCompute the value and the pushforward of the function f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named value_and_jvp.\n\ninfo: Info\nRequired primitive for forward mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward!","page":"API","title":"DifferentiationInterface.value_and_pushforward!","text":"value_and_pushforward!(f,     dy, [prep,] backend, x, tx, [contexts...]) -> (y, ty)\nvalue_and_pushforward!(f!, y, dy, [prep,] backend, x, tx, [contexts...]) -> (y, ty)\n\nCompute the value and the pushforward of the function f at point x with a tuple of tangents tx, overwriting ty.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named value_and_jvp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#Pullback","page":"API","title":"Pullback","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_pullback\nprepare_pullback_same_point\npullback\npullback!\nvalue_and_pullback\nvalue_and_pullback!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_pullback","page":"API","title":"DifferentiationInterface.prepare_pullback","text":"prepare_pullback(f,     backend, x, ty, [contexts...]) -> prep\nprepare_pullback(f!, y, backend, x, ty, [contexts...]) -> prep\n\nCreate a prep object that can be given to pullback and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pullback_same_point","page":"API","title":"DifferentiationInterface.prepare_pullback_same_point","text":"prepare_pullback_same_point(f,     backend, x, ty, [contexts...]) -> prep_same\nprepare_pullback_same_point(f!, y, backend, x, ty, [contexts...]) -> prep_same\n\nCreate an prep_same object that can be given to pullback and its variants if they are applied at the same point x and with the same contexts.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback","page":"API","title":"DifferentiationInterface.pullback","text":"pullback(f,     [prep,] backend, x, ty, [contexts...]) -> tx\npullback(f!, y, [prep,] backend, x, ty, [contexts...]) -> tx\n\nCompute the pullback of the function f at point x with a tuple of tangents ty.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named vjp.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback!","page":"API","title":"DifferentiationInterface.pullback!","text":"pullback!(f,     dx, [prep,] backend, x, ty, [contexts...]) -> tx\npullback!(f!, y, dx, [prep,] backend, x, ty, [contexts...]) -> tx\n\nCompute the pullback of the function f at point x with a tuple of tangents ty, overwriting dx.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named vjp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback","page":"API","title":"DifferentiationInterface.value_and_pullback","text":"value_and_pullback(f,     [prep,] backend, x, ty, [contexts...]) -> (y, tx)\nvalue_and_pullback(f!, y, [prep,] backend, x, ty, [contexts...]) -> (y, tx)\n\nCompute the value and the pullback of the function f at point x with a tuple of tangents ty.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named value_and_vjp.\n\ninfo: Info\nRequired primitive for reverse mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback!","page":"API","title":"DifferentiationInterface.value_and_pullback!","text":"value_and_pullback!(f,     dx, [prep,] backend, x, ty, [contexts...]) -> (y, tx)\nvalue_and_pullback!(f!, y, dx, [prep,] backend, x, ty, [contexts...]) -> (y, tx)\n\nCompute the value and the pullback of the function f at point x with a tuple of tangents ty, overwriting dx.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named value_and_vjp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#Derivative","page":"API","title":"Derivative","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_derivative\nderivative\nderivative!\nvalue_and_derivative\nvalue_and_derivative!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_derivative","page":"API","title":"DifferentiationInterface.prepare_derivative","text":"prepare_derivative(f,     backend, x, [contexts...]) -> prep\nprepare_derivative(f!, y, backend, x, [contexts...]) -> prep\n\nCreate a prep object that can be given to derivative and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative","page":"API","title":"DifferentiationInterface.derivative","text":"derivative(f,     [prep,] backend, x, [contexts...]) -> der\nderivative(f!, y, [prep,] backend, x, [contexts...]) -> der\n\nCompute the derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative!","page":"API","title":"DifferentiationInterface.derivative!","text":"derivative!(f,     der, [prep,] backend, x, [contexts...]) -> der\nderivative!(f!, y, der, [prep,] backend, x, [contexts...]) -> der\n\nCompute the derivative of the function f at point x, overwriting der.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative","page":"API","title":"DifferentiationInterface.value_and_derivative","text":"value_and_derivative(f,     [prep,] backend, x, [contexts...]) -> (y, der)\nvalue_and_derivative(f!, y, [prep,] backend, x, [contexts...]) -> (y, der)\n\nCompute the value and the derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative!","page":"API","title":"DifferentiationInterface.value_and_derivative!","text":"value_and_derivative!(f,     der, [prep,] backend, x, [contexts...]) -> (y, der)\nvalue_and_derivative!(f!, y, der, [prep,] backend, x, [contexts...]) -> (y, der)\n\nCompute the value and the derivative of the function f at point x, overwriting der.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#Gradient","page":"API","title":"Gradient","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_gradient\ngradient\ngradient!\nvalue_and_gradient\nvalue_and_gradient!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_gradient","page":"API","title":"DifferentiationInterface.prepare_gradient","text":"prepare_gradient(f, backend, x, [contexts...]) -> prep\n\nCreate a prep object that can be given to gradient and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient","page":"API","title":"DifferentiationInterface.gradient","text":"gradient(f, [prep,] backend, x, [contexts...]) -> grad\n\nCompute the gradient of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient!","page":"API","title":"DifferentiationInterface.gradient!","text":"gradient!(f, grad, [prep,] backend, x, [contexts...]) -> grad\n\nCompute the gradient of the function f at point x, overwriting grad.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient","page":"API","title":"DifferentiationInterface.value_and_gradient","text":"value_and_gradient(f, [prep,] backend, x, [contexts...]) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient!","page":"API","title":"DifferentiationInterface.value_and_gradient!","text":"value_and_gradient!(f, grad, [prep,] backend, x, [contexts...]) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x, overwriting grad.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jacobian","page":"API","title":"Jacobian","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_jacobian\njacobian\njacobian!\nvalue_and_jacobian\nvalue_and_jacobian!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_jacobian","page":"API","title":"DifferentiationInterface.prepare_jacobian","text":"prepare_jacobian(f,     backend, x, [contexts...]) -> prep\nprepare_jacobian(f!, y, backend, x, [contexts...]) -> prep\n\nCreate a prep object that can be given to jacobian and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian","page":"API","title":"DifferentiationInterface.jacobian","text":"jacobian(f,     [prep,] backend, x, [contexts...]) -> jac\njacobian(f!, y, [prep,] backend, x, [contexts...]) -> jac\n\nCompute the Jacobian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian!","page":"API","title":"DifferentiationInterface.jacobian!","text":"jacobian!(f,     jac, [prep,] backend, x, [contexts...]) -> jac\njacobian!(f!, y, jac, [prep,] backend, x, [contexts...]) -> jac\n\nCompute the Jacobian matrix of the function f at point x, overwriting jac.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian","page":"API","title":"DifferentiationInterface.value_and_jacobian","text":"value_and_jacobian(f,     [prep,] backend, x, [contexts...]) -> (y, jac)\nvalue_and_jacobian(f!, y, [prep,] backend, x, [contexts...]) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian!","page":"API","title":"DifferentiationInterface.value_and_jacobian!","text":"value_and_jacobian!(f,     jac, [prep,] backend, x, [contexts...]) -> (y, jac)\nvalue_and_jacobian!(f!, y, jac, [prep,] backend, x, [contexts...]) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x, overwriting jac.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#Second-order","page":"API","title":"Second order","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SecondOrder","category":"page"},{"location":"api/#DifferentiationInterface.SecondOrder","page":"API","title":"DifferentiationInterface.SecondOrder","text":"SecondOrder\n\nCombination of two backends for second-order differentiation.\n\ndanger: Danger\nSecondOrder backends do not support first-order operators.\n\nConstructor\n\nSecondOrder(outer_backend, inner_backend)\n\nFields\n\nouter::AbstractADType: backend for the outer differentiation\ninner::AbstractADType: backend for the inner differentiation\n\n\n\n\n\n","category":"type"},{"location":"api/#Second-derivative","page":"API","title":"Second derivative","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_second_derivative\nsecond_derivative\nsecond_derivative!\nvalue_derivative_and_second_derivative\nvalue_derivative_and_second_derivative!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_second_derivative","page":"API","title":"DifferentiationInterface.prepare_second_derivative","text":"prepare_second_derivative(f, backend, x, [contexts...]) -> prep\n\nCreate a prep object that can be given to second_derivative and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative","page":"API","title":"DifferentiationInterface.second_derivative","text":"second_derivative(f, [prep,] backend, x, [contexts...]) -> der2\n\nCompute the second derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative!","page":"API","title":"DifferentiationInterface.second_derivative!","text":"second_derivative!(f, der2, [prep,] backend, x, [contexts...]) -> der2\n\nCompute the second derivative of the function f at point x, overwriting der2.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_derivative_and_second_derivative","page":"API","title":"DifferentiationInterface.value_derivative_and_second_derivative","text":"value_derivative_and_second_derivative(f, [prep,] backend, x, [contexts...]) -> (y, der, der2)\n\nCompute the value, first derivative and second derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_derivative_and_second_derivative!","page":"API","title":"DifferentiationInterface.value_derivative_and_second_derivative!","text":"value_derivative_and_second_derivative!(f, der, der2, [prep,] backend, x, [contexts...]) -> (y, der, der2)\n\nCompute the value, first derivative and second derivative of the function f at point x, overwriting der and der2.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian-vector-product","page":"API","title":"Hessian-vector product","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_hvp\nprepare_hvp_same_point\nhvp\nhvp!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_hvp","page":"API","title":"DifferentiationInterface.prepare_hvp","text":"prepare_hvp(f, backend, x, tx, [contexts...]) -> prep\n\nCreate a prep object that can be given to hvp and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_hvp_same_point","page":"API","title":"DifferentiationInterface.prepare_hvp_same_point","text":"prepare_hvp_same_point(f, backend, x, tx, [contexts...]) -> prep_same\n\nCreate an prep_same object that can be given to hvp and its variants if they are applied at the same point x and with the same contexts.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp","page":"API","title":"DifferentiationInterface.hvp","text":"hvp(f, [prep,] backend, x, tx, [contexts...]) -> tg\n\nCompute the Hessian-vector product of f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp!","page":"API","title":"DifferentiationInterface.hvp!","text":"hvp!(f, tg, [prep,] backend, x, tx, [contexts...]) -> tg\n\nCompute the Hessian-vector product of f at point x with a tuple of tangents tx, overwriting tg.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian","page":"API","title":"Hessian","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_hessian\nhessian\nhessian!\nvalue_gradient_and_hessian\nvalue_gradient_and_hessian!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_hessian","page":"API","title":"DifferentiationInterface.prepare_hessian","text":"prepare_hessian(f, backend, x, [contexts...]) -> prep\n\nCreate a prep object that can be given to hessian and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian","page":"API","title":"DifferentiationInterface.hessian","text":"hessian(f, [prep,] backend, x, [contexts...]) -> hess\n\nCompute the Hessian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian!","page":"API","title":"DifferentiationInterface.hessian!","text":"hessian!(f, hess, [prep,] backend, x, [contexts...]) -> hess\n\nCompute the Hessian matrix of the function f at point x, overwriting hess.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_gradient_and_hessian","page":"API","title":"DifferentiationInterface.value_gradient_and_hessian","text":"value_gradient_and_hessian(f, [prep,] backend, x, [contexts...]) -> (y, grad, hess)\n\nCompute the value, gradient vector and Hessian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_gradient_and_hessian!","page":"API","title":"DifferentiationInterface.value_gradient_and_hessian!","text":"value_gradient_and_hessian!(f, grad, hess, [prep,] backend, x, [contexts...]) -> (y, grad, hess)\n\nCompute the value, gradient vector and Hessian matrix of the function f at point x, overwriting grad and hess.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#Backend-queries","page":"API","title":"Backend queries","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"check_available\ncheck_inplace\nDifferentiationInterface.outer\nDifferentiationInterface.inner","category":"page"},{"location":"api/#DifferentiationInterface.check_available","page":"API","title":"DifferentiationInterface.check_available","text":"check_available(backend)\n\nCheck whether backend is available (i.e. whether the extension is loaded).\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.check_inplace","page":"API","title":"DifferentiationInterface.check_inplace","text":"check_inplace(backend)\n\nCheck whether backend supports differentiation of in-place functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.outer","page":"API","title":"DifferentiationInterface.outer","text":"outer(backend::SecondOrder)\nouter(backend::AbstractADType)\n\nReturn the outer backend of a SecondOrder object, tasked with differentiation at the second order.\n\nFor any other backend type, this function acts like the identity.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.inner","page":"API","title":"DifferentiationInterface.inner","text":"inner(backend::SecondOrder)\ninner(backend::AbstractADType)\n\nReturn the inner backend of a SecondOrder object, tasked with differentiation at the first order.\n\nFor any other backend type, this function acts like the identity.\n\n\n\n\n\n","category":"function"},{"location":"api/#Backend-switch","page":"API","title":"Backend switch","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DifferentiateWith","category":"page"},{"location":"api/#DifferentiationInterface.DifferentiateWith","page":"API","title":"DifferentiationInterface.DifferentiateWith","text":"DifferentiateWith\n\nFunction wrapper that enforces differentiation with a \"substitute\" AD backend, possible different from the \"true\" AD backend that is called.\n\nFor instance, suppose a function f is not differentiable with Zygote because it involves mutation, but you know that it is differentiable with Enzyme. Then f2 = DifferentiateWith(f, AutoEnzyme()) is a new function that behaves like f, except that f2 is differentiable with Zygote (thanks to a chain rule which calls Enzyme under the hood). Moreover, any larger algorithm alg that calls f2 instead of f will also be differentiable with Zygote (as long as f was the only Zygote blocker).\n\ntip: Tip\nThis is mainly relevant for package developers who want to produce differentiable code at low cost, without writing the differentiation rules themselves. If you sprinkle a few DifferentiateWith in places where some AD backends may struggle, end users can pick from a wider variety of packages to differentiate your algorithms.\n\nwarning: Warning\nDifferentiateWith only supports out-of-place functions y = f(x) without additional context arguments. It only makes these functions differentiable if the true backend is either ForwardDiff or compatible with ChainRules. For any other true backend, the differentiation behavior is not altered by DifferentiateWith (it becomes a transparent wrapper).\n\nFields\n\nf: the function in question, with signature f(x)\nbackend::AbstractADType: the substitute backend to use for differentiation\n\nnote: Note\nFor the substitute AD backend to be called under the hood, its package needs to be loaded in addition to the package of the true AD backend.\n\nConstructor\n\nDifferentiateWith(f, backend)\n\nExample\n\njulia> using DifferentiationInterface\n\njulia> import Enzyme, ForwardDiff, Zygote\n\njulia> function f(x::Vector{Float64})\n           a = Vector{Float64}(undef, 1)  # type constraint breaks ForwardDiff\n           a[1] = sum(abs2, x)  # mutation breaks Zygote\n           return a[1]\n       end;\n\njulia> f2 = DifferentiateWith(f, AutoEnzyme());\n\njulia> f([3.0, 5.0]) == f2([3.0, 5.0])\ntrue\n\njulia> alg(x) = 7 * f2(x);\n\njulia> ForwardDiff.gradient(alg, [3.0, 5.0])\n2-element Vector{Float64}:\n 42.0\n 70.0\n\njulia> Zygote.gradient(alg, [3.0, 5.0])[1]\n2-element Vector{Float64}:\n 42.0\n 70.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Sparsity-detection","page":"API","title":"Sparsity detection","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DenseSparsityDetector","category":"page"},{"location":"api/#DifferentiationInterface.DenseSparsityDetector","page":"API","title":"DifferentiationInterface.DenseSparsityDetector","text":"DenseSparsityDetector\n\nSparsity pattern detector satisfying the detection API of ADTypes.jl.\n\nThe nonzeros in a Jacobian or Hessian are detected by computing the relevant matrix with dense AD, and thresholding the entries with a given tolerance (which can be numerically inaccurate). This process can be very slow, and should only be used if its output can be exploited multiple times to compute many sparse matrices.\n\ndanger: Danger\nIn general, the sparsity pattern you obtain can depend on the provided input x. If you want to reuse the pattern, make sure that it is input-agnostic.\n\nwarning: Warning\nDenseSparsityDetector functionality is now located in a package extension, please load the SparseArrays.jl standard library before you use it.\n\nFields\n\nbackend::AbstractADType is the dense AD backend used under the hood\natol::Float64 is the minimum magnitude of a matrix entry to be considered nonzero\n\nConstructor\n\nDenseSparsityDetector(backend; atol, method=:iterative)\n\nThe keyword argument method::Symbol can be either:\n\n:iterative: compute the matrix in a sequence of matrix-vector products (memory-efficient)\n:direct: compute the matrix all at once (memory-hungry but sometimes faster).\n\nNote that the constructor is type-unstable because method ends up being a type parameter of the DenseSparsityDetector object (this is not part of the API and might change).\n\nExamples\n\nusing ADTypes, DifferentiationInterface, SparseArrays\nimport ForwardDiff\n\ndetector = DenseSparsityDetector(AutoForwardDiff(); atol=1e-5, method=:direct)\n\nADTypes.jacobian_sparsity(diff, rand(5), detector)\n\n# output\n\n4×5 SparseMatrixCSC{Bool, Int64} with 8 stored entries:\n 1  1  ⋅  ⋅  ⋅\n ⋅  1  1  ⋅  ⋅\n ⋅  ⋅  1  1  ⋅\n ⋅  ⋅  ⋅  1  1\n\nSometimes the sparsity pattern is input-dependent:\n\nADTypes.jacobian_sparsity(x -> [prod(x)], rand(2), detector)\n\n# output\n\n1×2 SparseMatrixCSC{Bool, Int64} with 2 stored entries:\n 1  1\n\nADTypes.jacobian_sparsity(x -> [prod(x)], [0, 1], detector)\n\n# output\n\n1×2 SparseMatrixCSC{Bool, Int64} with 1 stored entry:\n 1  ⋅\n\n\n\n\n\n","category":"type"},{"location":"api/#Internals","page":"API","title":"Internals","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The following is not part of the public API.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Modules = [DifferentiationInterface]\nPublic = false\nFilter = t -> !(Symbol(t) in [:outer, :inner])","category":"page"},{"location":"api/#DifferentiationInterface.AutoZeroForward","page":"API","title":"DifferentiationInterface.AutoZeroForward","text":"AutoZeroForward <: ADTypes.AbstractADType\n\nTrivial backend that sets all derivatives to zero. Used in testing and benchmarking.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.AutoZeroReverse","page":"API","title":"DifferentiationInterface.AutoZeroReverse","text":"AutoZeroReverse <: ADTypes.AbstractADType\n\nTrivial backend that sets all derivatives to zero. Used in testing and benchmarking.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.DerivativePrep","page":"API","title":"DifferentiationInterface.DerivativePrep","text":"DerivativePrep\n\nAbstract type for additional information needed by derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverForward","page":"API","title":"DifferentiationInterface.ForwardOverForward","text":"ForwardOverForward\n\nTraits identifying second-order backends that compute HVPs in forward over forward mode (inefficient).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverReverse","page":"API","title":"DifferentiationInterface.ForwardOverReverse","text":"ForwardOverReverse\n\nTraits identifying second-order backends that compute HVPs in forward over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.GradientPrep","page":"API","title":"DifferentiationInterface.GradientPrep","text":"GradientPrep\n\nAbstract type for additional information needed by gradient and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HVPPrep","page":"API","title":"DifferentiationInterface.HVPPrep","text":"HVPPrep\n\nAbstract type for additional information needed by hvp and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HessianPrep","page":"API","title":"DifferentiationInterface.HessianPrep","text":"HessianPrep\n\nAbstract type for additional information needed by hessian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.InPlaceNotSupported","page":"API","title":"DifferentiationInterface.InPlaceNotSupported","text":"InPlaceNotSupported\n\nTrait identifying backends that do not support in-place functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.InPlaceSupported","page":"API","title":"DifferentiationInterface.InPlaceSupported","text":"InPlaceSupported\n\nTrait identifying backends that support in-place functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.JacobianPrep","page":"API","title":"DifferentiationInterface.JacobianPrep","text":"JacobianPrep\n\nAbstract type for additional information needed by jacobian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackFast","page":"API","title":"DifferentiationInterface.PullbackFast","text":"PullbackFast\n\nTrait identifying backends that support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackPrep","page":"API","title":"DifferentiationInterface.PullbackPrep","text":"PullbackPrep\n\nAbstract type for additional information needed by pullback and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackSlow","page":"API","title":"DifferentiationInterface.PullbackSlow","text":"PullbackSlow\n\nTrait identifying backends that do not support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardFast","page":"API","title":"DifferentiationInterface.PushforwardFast","text":"PushforwardFast\n\nTrait identifying backends that support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardPrep","page":"API","title":"DifferentiationInterface.PushforwardPrep","text":"PushforwardPrep\n\nAbstract type for additional information needed by pushforward and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardSlow","page":"API","title":"DifferentiationInterface.PushforwardSlow","text":"PushforwardSlow\n\nTrait identifying backends that do not support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverForward","page":"API","title":"DifferentiationInterface.ReverseOverForward","text":"ReverseOverForward\n\nTraits identifying second-order backends that compute HVPs in reverse over forward mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverReverse","page":"API","title":"DifferentiationInterface.ReverseOverReverse","text":"ReverseOverReverse\n\nTraits identifying second-order backends that compute HVPs in reverse over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.SecondDerivativePrep","page":"API","title":"DifferentiationInterface.SecondDerivativePrep","text":"SecondDerivativePrep\n\nAbstract type for additional information needed by second_derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADTypes.mode-Tuple{SecondOrder}","page":"API","title":"ADTypes.mode","text":"mode(backend::SecondOrder)\n\nReturn the outer mode of the second-order backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.basis-Tuple{ADTypes.AbstractADType, AbstractArray, Any}","page":"API","title":"DifferentiationInterface.basis","text":"basis(backend, a::AbstractArray, i)\n\nConstruct the i-th standard basis array in the vector space of a with element type eltype(a).\n\nNote\n\nIf an AD backend benefits from a more specialized basis array implementation, this function can be extended on the backend type.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.inplace_support-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.inplace_support","text":"inplace_support(backend)\n\nReturn InPlaceSupported or InPlaceNotSupported in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.multibasis-Tuple{ADTypes.AbstractADType, AbstractArray, Any}","page":"API","title":"DifferentiationInterface.multibasis","text":"multibasis(backend, a::AbstractArray, inds::AbstractVector)\n\nConstruct the sum of the i-th standard basis arrays in the vector space of a with element type eltype(a), for all i ∈ inds.\n\nNote\n\nIf an AD backend benefits from a more specialized basis array implementation, this function can be extended on the backend type.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.nested-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.nested","text":"nested(backend)\n\nReturn a possibly modified backend that can work while nested inside another differentiation procedure.\n\nAt the moment this function is pretty much useless.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pick_batchsize-Tuple{ADTypes.AbstractADType, Integer}","page":"API","title":"DifferentiationInterface.pick_batchsize","text":"pick_batchsize(backend::AbstractADType, dimension::Integer)\n\nPick a reasonable batch size for batched derivative evaluation with a given total dimension.\n\nReturns Val(1) for backends which have not overloaded it.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_derivative","page":"API","title":"DifferentiationInterface.prepare!_derivative","text":"prepare!_derivative(f,     prep, backend, x, [contexts...]) -> new_prep\nprepare!_derivative(f!, y, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_derivative but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare!_gradient","page":"API","title":"DifferentiationInterface.prepare!_gradient","text":"prepare!_gradient(f, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_gradient but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare!_hessian","page":"API","title":"DifferentiationInterface.prepare!_hessian","text":"prepare!_hessian(f, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_hessian but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare!_hvp","page":"API","title":"DifferentiationInterface.prepare!_hvp","text":"prepare!_hvp(f, backend, x, tx, [contexts...]) -> new_prep\n\nSame behavior as prepare_hvp but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare!_jacobian","page":"API","title":"DifferentiationInterface.prepare!_jacobian","text":"prepare!_jacobian(f,     prep, backend, x, [contexts...]) -> new_prep\nprepare!_jacobian(f!, y, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_jacobian but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare!_pullback","page":"API","title":"DifferentiationInterface.prepare!_pullback","text":"prepare!_pullback(f,     prep, backend, x, ty, [contexts...]) -> new_prep\nprepare!_pullback(f!, y, prep, backend, x, ty, [contexts...]) -> new_prep\n\nSame behavior as prepare_pullback but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare!_pushforward","page":"API","title":"DifferentiationInterface.prepare!_pushforward","text":"prepare!_pushforward(f,     prep, backend, x, tx, [contexts...]) -> new_prep\nprepare!_pushforward(f!, y, prep, backend, x, tx, [contexts...]) -> new_prep\n\nSame behavior as prepare_pushforward but can modify an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pullback_performance","text":"pullback_performance(backend)\n\nReturn PullbackFast or PullbackSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pushforward_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pushforward_performance","text":"pushforward_performance(backend)\n\nReturn PushforwardFast or PushforwardSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"tutorials/advanced/#Advanced-tutorial","page":"Advanced tutorial","title":"Advanced tutorial","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"We present contexts and sparsity handling with DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"using BenchmarkTools\nusing DifferentiationInterface\nimport ForwardDiff, Zygote\nusing SparseConnectivityTracer: TracerSparsityDetector\nusing SparseMatrixColorings","category":"page"},{"location":"tutorials/advanced/#Contexts","page":"Advanced tutorial","title":"Contexts","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Assume you want differentiate a multi-argument function with respect to the first argument.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"f_multiarg(x, c) = c * sum(abs2, x)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"The first way, which works with every backend, is to create a closure:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"f_singlearg(c) = x -> f_multiarg(x, c)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Let's see it in action:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"backend = AutoForwardDiff()\nx = float.(1:3)\n\ngradient(f_singlearg(10), backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"However, for performance reasons, it is sometimes preferrable to avoid closures and pass all arguments to the original function. We can do this by wrapping c into a Constant and giving this constant to the gradient operator.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"gradient(f_multiarg, backend, x, Constant(10))","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Preparation also works in this case, even if the constant changes before execution:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"prep_other_constant = prepare_gradient(f_multiarg, backend, x, Constant(-1))\ngradient(f_multiarg, prep_other_constant, backend, x, Constant(10))","category":"page"},{"location":"tutorials/advanced/#Sparsity","page":"Advanced tutorial","title":"Sparsity","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Sparse AD is very useful when Jacobian or Hessian matrices have a lot of zeros. So let us write functions that satisfy this property.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"f_sparse_vector(x::AbstractVector) = diff(x .^ 2) + diff(reverse(x .^ 2))\nf_sparse_scalar(x::AbstractVector) = sum(f_sparse_vector(x) .^ 2)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/#Dense-backends","page":"Advanced tutorial","title":"Dense backends","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"When we use the jacobian or hessian operator with a dense backend, we get a dense matrix with plenty of zeros.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"x = float.(1:8);","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"dense_first_order_backend = AutoForwardDiff()\nJ_dense = jacobian(f_sparse_vector, dense_first_order_backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"dense_second_order_backend = SecondOrder(AutoForwardDiff(), AutoZygote())\nH_dense = hessian(f_sparse_scalar, dense_second_order_backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"The results are correct but the procedure is very slow. By using a sparse backend, we can get the runtime to increase with the number of nonzero elements, instead of the total number of elements.","category":"page"},{"location":"tutorials/advanced/#Sparse-backends","page":"Advanced tutorial","title":"Sparse backends","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Recipe to create a sparse backend: combine a dense backend, a sparsity detector and a compatible coloring algorithm inside AutoSparse. The following are reasonable defaults:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"sparse_first_order_backend = AutoSparse(\n    dense_first_order_backend;\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\n\nsparse_second_order_backend = AutoSparse(\n    dense_second_order_backend;\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Now the resulting matrices are sparse:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jacobian(f_sparse_vector, sparse_first_order_backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"hessian(f_sparse_scalar, sparse_second_order_backend, x)","category":"page"},{"location":"tutorials/advanced/#Sparse-preparation","page":"Advanced tutorial","title":"Sparse preparation","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In the examples above, we didn't use preparation. Sparse preparation is more costly than dense preparation, but it is even more essential. Indeed, once preparation is done, sparse differentiation is much faster than dense differentiation, because it makes fewer calls to the underlying function.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Some result analysis functions from SparseMatrixColorings.jl can help you figure out what the preparation contains. First, it records the sparsity pattern itself (the one returned by the detector).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep = prepare_jacobian(f_sparse_vector, sparse_first_order_backend, x)\nsparsity_pattern(jac_prep)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In forward mode, each column of the sparsity pattern gets a color.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"column_colors(jac_prep)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"And the colors in turn define non-overlapping groups (for Jacobians at least, Hessians are a bit more complicated).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"column_groups(jac_prep)","category":"page"},{"location":"tutorials/advanced/#Sparsity-speedup","page":"Advanced tutorial","title":"Sparsity speedup","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"When preparation is used, the speedup due to sparsity becomes very visible in large dimensions.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"xbig = rand(1000)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep_dense = prepare_jacobian(f_sparse_vector, dense_first_order_backend, zero(xbig))\n@benchmark jacobian($f_sparse_vector, $jac_prep_dense, $dense_first_order_backend, $xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep_sparse = prepare_jacobian(f_sparse_vector, sparse_first_order_backend, zero(xbig))\n@benchmark jacobian($f_sparse_vector, $jac_prep_sparse, $sparse_first_order_backend, $xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Better memory use can be achieved by pre-allocating the matrix from the preparation result (so that it has the correct structure).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_buffer = similar(sparsity_pattern(jac_prep_sparse), eltype(xbig))\n@benchmark jacobian!($f_sparse_vector, $jac_buffer, $jac_prep_sparse, $sparse_first_order_backend, $xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"And for optimal speed, one should write non-allocating and type-stable functions.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"function f_sparse_vector!(y::AbstractVector, x::AbstractVector)\n    n = length(x)\n    for i in eachindex(y)\n        y[i] = abs2(x[i + 1]) - abs2(x[i]) + abs2(x[n - i]) - abs2(x[n - i + 1])\n    end\n    return nothing\nend\n\nybig = zeros(length(xbig) - 1)\nf_sparse_vector!(ybig, xbig)\nybig ≈ f_sparse_vector(xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In this case, the sparse Jacobian should also become non-allocating (for our specific choice of backend).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep_sparse_nonallocating = prepare_jacobian(f_sparse_vector!, zero(ybig), sparse_first_order_backend, zero(xbig))\njac_buffer = similar(sparsity_pattern(jac_prep_sparse_nonallocating), eltype(xbig))\n@benchmark jacobian!($f_sparse_vector!, $ybig, $jac_buffer, $jac_prep_sparse_nonallocating, $sparse_first_order_backend, $xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"","category":"page"},{"location":"explanation/operators/#Operators","page":"Operators","title":"Operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"tip: Tip\nIf there are some concepts you do not understand, take a look at the book The Elements of Differentiable Programming (Blondel and Roulet, 2024).","category":"page"},{"location":"explanation/operators/#List-of-operators","page":"Operators","title":"List of operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Given a function f(x) = y, there are several differentiation operators available. The terminology depends on:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"the type and shape of the input x\nthe type and shape of the output y\nthe order of differentiation","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Below we list and describe all the operators we support.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe package is thoroughly tested with inputs and outputs of the following types: Float64, Vector{Float64} and Matrix{Float64}. We also expect it to work on most kinds of Number and AbstractArray variables. Beyond that, you are in uncharted territory. We voluntarily keep the type annotations minimal, so that passing more complex objects or custom structs might work in some cases, but we make no guarantees about that yet.","category":"page"},{"location":"explanation/operators/#High-level-operators","page":"Operators","title":"High-level operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"These operators are computed using only the input x.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"operator order input x output y operator result type operator result shape\nderivative 1 Number Any similar to y size(y)\nsecond_derivative 2 Number Any similar to y size(y)\ngradient 1 Any Number similar to x size(x)\njacobian 1 AbstractArray AbstractArray AbstractMatrix (length(y), length(x))\nhessian 2 AbstractArray Number AbstractMatrix (length(x), length(x))","category":"page"},{"location":"explanation/operators/#Low-level-operators","page":"Operators","title":"Low-level operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"These operators are computed using the input x and another argument t of type NTuple, which contains one or more tangents. You can think of tangents as perturbations propagated through the function; they live either in the same space as x or in the same space as y.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"operator order input x output y element type of t operator result type operator result shape\npushforward (JVP) 1 Any Any similar to x similar to y size(y)\npullback (VJP) 1 Any Any similar to y similar to x size(x)\nhvp 2 Any Number similar to x similar to x size(x)","category":"page"},{"location":"explanation/operators/#Variants","page":"Operators","title":"Variants","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Several variants of each operator are defined:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"out-of-place operators return a new derivative object\nin-place operators mutate the provided derivative object","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"out-of-place in-place out-of-place + primal in-place + primal\nderivative derivative! value_and_derivative value_and_derivative!\nsecond_derivative second_derivative! value_derivative_and_second_derivative value_derivative_and_second_derivative!\ngradient gradient! value_and_gradient value_and_gradient!\nhessian hessian! value_gradient_and_hessian value_gradient_and_hessian!\njacobian jacobian! value_and_jacobian value_and_jacobian!\npushforward pushforward! value_and_pushforward value_and_pushforward!\npullback pullback! value_and_pullback value_and_pullback!\nhvp hvp! - -","category":"page"},{"location":"explanation/operators/#Mutation-and-signatures","page":"Operators","title":"Mutation and signatures","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Two kinds of functions are supported:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"out-of-place functions f(x) = y\nin-place functions f!(y, x) = nothing","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nIn-place functions only work with pushforward, pullback, derivative and jacobian. The other operators hvp, gradient and hessian require scalar outputs, so it makes no sense to mutate the number y.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"This results in various operator signatures (the necessary arguments and their order):","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"function signature out-of-place operator (returns result) in-place  operator (mutates result)\nout-of-place function f op(f, backend, x, [t]) op!(f, result, backend, x, [t])\nin-place function f! op(f!, y, backend, x, [t]) op!(f!, y, result, backend, x, [t])","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe positional arguments between f/f! and backend are always mutated, regardless of the bang ! in the operator name. In particular, for in-place functions f!(y, x), every variant of every operator will mutate y.","category":"page"},{"location":"explanation/operators/#Preparation","page":"Operators","title":"Preparation","text":"","category":"section"},{"location":"explanation/operators/#Principle","page":"Operators","title":"Principle","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"In many cases, AD can be accelerated if the function has been called at least once (e.g. to record a tape) or if some cache objects are pre-allocated. This preparation procedure is backend-specific, but we expose a common syntax to achieve it.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"operator preparation (different point) preparation (same point)\nderivative prepare_derivative -\ngradient prepare_gradient -\njacobian prepare_jacobian -\nsecond_derivative prepare_second_derivative -\nhessian prepare_hessian -\npushforward prepare_pushforward prepare_pushforward_same_point\npullback prepare_pullback prepare_pullback_same_point\nhvp prepare_hvp prepare_hvp_same_point","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"In addition, the preparation syntax depends on the number of arguments accepted by the function.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"function signature preparation signature\nout-of-place function prepare_op(f, backend, x, [t])\nin-place function prepare_op(f!, y, backend, x, [t])","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Preparation creates an object called prep which contains the the necessary information to speed up an operator and its variants. The idea is that you prepare only once, which can be costly, but then call the operator several times while reusing the same prep.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"op(f, backend, x, [t])  # slow because it includes preparation\nop(f, prep, backend, x, [t])  # fast because it skips preparation","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe prep object is the last argument before backend and it is always mutated, regardless of the bang ! in the operator name.","category":"page"},{"location":"explanation/operators/#Reusing-preparation","page":"Operators","title":"Reusing preparation","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Deciding whether it is safe to reuse the results of preparation is not easy. Here are the general rules that we strive to implement:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"For different-point preparation, the output prep of prepare_op(f, b, x, [t]) can be reused in op(f, prep, b, other_x, [other_t]), provided that:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"the inputs x and other_x have the same types and sizes\nthe tangents in t and other_t have the same types and sizes","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"For same-point preparation, the output prep of prepare_op_same_point(f, b, x, [t]) can be reused in op(f, prep, b, x, other_t), provided that:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"the input x remains exactly the same (as well as any Constant context)\nthe tangents in t and other_t have the same types and sizes","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThese rules hold for the majority of backends, but there are some exceptions. The most important exception is ReverseDiff and its taping mechanism, which is sensitive to control flow inside the function.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"","category":"page"},{"location":"dev_guide/#Dev-guide","page":"Dev guide","title":"Dev guide","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"This page is important reading if you want to contribute to DifferentiationInterface.jl. It is not part of the public API and the content below may become outdated, in which case you should refer to the source code as the ground truth.","category":"page"},{"location":"dev_guide/#General-principles","page":"Dev guide","title":"General principles","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"The package is structured around 8 operators:","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"derivative\nsecond_derivative\ngradient\njacobian\nhessian\npushforward\npullback\nhvp","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Most operators have 4 variants, which look like this in the first order: operator, operator!, value_and_operator, value_and_operator!.","category":"page"},{"location":"dev_guide/#New-operator","page":"Dev guide","title":"New operator","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"To implement a new operator for an existing backend, you need to write 5 methods: 1 for preparation and 4 corresponding to the variants of the operator (see above). For first-order operators, you may also want to support in-place functions, which requires another 5 methods (defined on f! instead of f).","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"The method prepare_operator must output a prep object of the correct type. For instance, prepare_gradient(f, backend, x) must return a DifferentiationInterface.GradientPrep. Assuming you don't need any preparation for said operator, you can use the trivial prep that are already defined, like DifferentiationInterface.NoGradientPrep. Otherwise, define a custom struct like MyGradientPrep <: DifferentiationInterface.GradientPrep and put the necessary storage in there.","category":"page"},{"location":"dev_guide/#New-backend","page":"Dev guide","title":"New backend","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"If you want to implement a new backend, for instance because you developed a new AD package called SuperDiff, please open a pull request to DifferentiationInterface.jl. Your AD package needs to be registered first.","category":"page"},{"location":"dev_guide/#Core-code","page":"Dev guide","title":"Core code","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"In the main package, you should define a new struct SuperDiffBackend which subtypes ADTypes.AbstractADType, and endow it with the fields you need to parametrize your differentiation routines. You also have to define ADTypes.mode and DifferentiationInterface.inplace_support on SuperDiffBackend.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"info: Info\nIn the end, this backend struct will need to be contributed to ADTypes.jl. However, putting it in the DifferentiationInterface.jl PR is a good first step for debugging.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"In a package extension named DifferentiationInterfaceSuperDiffExt, you need to implement at least pushforward or pullback (and their variants). The exact requirements depend on the differentiation mode you chose:","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"backend mode pushforward necessary pullback necessary\nADTypes.ForwardMode yes no\nADTypes.ReverseMode no yes\nADTypes.ForwardOrReverseMode yes yes\nADTypes.SymbolicMode yes yes","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Every other operator can be deduced from these two, but you can gain efficiency by implementing additional operators.","category":"page"},{"location":"dev_guide/#Tests-and-docs","page":"Dev guide","title":"Tests and docs","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Once that is done, you need to add your new backend to the test suite. Test files should be gathered in a folder named SuperDiff inside DifferentiationInterface/test/Back. They should use DifferentiationInterfaceTest.jl to check correctness against the default scenarios. Take inspiration from the tests of other backends to write your own. To activate tests in CI, modify the test workflow and add your package to the list. To run the tests locally, replace the following line in DifferentiationInterface/test/runtests.jl","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"GROUP = get(ENV, \"JULIA_DI_TEST_GROUP\", \"All\")","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"with the much cheaper version","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"GROUP = get(ENV, \"JULIA_DI_TEST_GROUP\", \"Back/SuperDiff\")","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"but don't forget to switch it back before pushing.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Finally, you need to add your backend to the documentation, modifying every page that involves a list of backends (including the README.md).","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"","category":"page"},{"location":"explanation/advanced/#Advanced-features","page":"Advanced features","title":"Advanced features","text":"","category":"section"},{"location":"explanation/advanced/#Contexts","page":"Advanced features","title":"Contexts","text":"","category":"section"},{"location":"explanation/advanced/#Additional-arguments","page":"Advanced features","title":"Additional arguments","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"For all operators provided DifferentiationInterface, there can be only one differentiated (or \"active\") argument, which we call x. However, the release v0.6 introduced the possibility of additional \"context\" arguments, which are not differentiated but still passed to the function after x.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"Contexts can be useful if you have a function y = f(x, a, b, c, ...) or f!(y, x, a, b, c, ...) and you want derivatives of y with respect to x only. Another option would be creating a closure, but that is sometimes undesirable.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"warning: Warning\nThis feature is still experimental, and will likely not be supported by all backends. At the moment, it only works with certain backends, among which ForwardDiff, Zygote and Enzyme.","category":"page"},{"location":"explanation/advanced/#Types-of-contexts","page":"Advanced features","title":"Types of contexts","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"Every context argument must be wrapped in a subtype of Context and come after the differentiated input x. Right now, there is only one kind of context, namely Constant, but we might add more. Semantically, calling","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"gradient(f, backend, x, Constant(c))","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"computes the partial gradient of f(x, c) with respect to x, while keeping c constant. Importantly, one can prepare an operator with an arbitrary value c' of the constant (subject to the usual restrictions on preparation).","category":"page"},{"location":"explanation/advanced/#Sparsity","page":"Advanced features","title":"Sparsity","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"When faced with sparse Jacobian or Hessian matrices, one can take advantage of their sparsity pattern to speed up the computation. DifferentiationInterface does this automatically if you pass a backend of type AutoSparse.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"tip: Tip\nTo know more about sparse AD, read the survey What Color Is Your Jacobian? Graph Coloring for Computing Derivatives (Gebremedhin et al., 2005).","category":"page"},{"location":"explanation/advanced/#AutoSparse-object","page":"Advanced features","title":"AutoSparse object","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"AutoSparse backends only support jacobian and hessian (as well as their variants), because other operators do not output matrices. An AutoSparse backend must be constructed from three ingredients:","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"An underlying (dense) backend, which can be SecondOrder or anything from ADTypes.jl\nA sparsity pattern detector like:\nTracerSparsityDetector from SparseConnectivityTracer.jl\nSymbolicsSparsityDetector from Symbolics.jl\nDenseSparsityDetector from DifferentiationInterface.jl (beware that this detector only gives a locally valid pattern)\nKnownJacobianSparsityDetector or KnownHessianSparsityDetector from ADTypes.jl (if you already know the pattern)\nA coloring algorithm from SparseMatrixColorings.jl, such as:\nGreedyColoringAlgorithm (our generic recommendation)\nConstantColoringAlgorithm (if you have already computed the optimal coloring and always want to return it)","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"note: Note\nSymbolic backends have built-in sparsity handling, so AutoSparse(AutoSymbolics()) and AutoSparse(AutoFastDifferentiation()) do not need additional configuration for pattern detection or coloring.","category":"page"},{"location":"explanation/advanced/#Cost-of-sparse-preparation","page":"Advanced features","title":"Cost of sparse preparation","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"The preparation step of jacobian or hessian with an AutoSparse backend can be long, because it needs to detect the sparsity pattern and perform a matrix coloring. But after preparation, the more zeros are present in the matrix, the greater the speedup will be compared to dense differentiation.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"danger: Danger\nThe result of preparation for an AutoSparse backend cannot be reused if the sparsity pattern changes.","category":"page"},{"location":"explanation/advanced/#Tuning-the-coloring-algorithm","page":"Advanced features","title":"Tuning the coloring algorithm","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"The complexity of sparse Jacobians or Hessians grows with the number of distinct colors in a coloring of the sparsity pattern. To reduce this number of colors, GreedyColoringAlgorithm has two main settings: the order used for vertices and the decompression method. Depending on your use case, you may want to modify either of these options to increase performance. See the documentation of SparseMatrixColorings.jl for details.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: DifferentiationInterface Logo)","category":"page"},{"location":"#DifferentiationInterface","page":"Home","title":"DifferentiationInterface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages) (Image: DOI)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Package Docs\nDifferentiationInterface (Image: Stable)     (Image: Dev)\nDifferentiationInterfaceTest (Image: Stable) (Image: Dev)","category":"page"},{"location":"","page":"Home","title":"Home","text":"An interface to various automatic differentiation (AD) backends in Julia.","category":"page"},{"location":"#Goal","page":"Home","title":"Goal","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a unified syntax to differentiate functions, including:","category":"page"},{"location":"","page":"Home","title":"Home","text":"First- and second-order operators (gradients, Jacobians, Hessians and more)\nIn-place and out-of-place differentiation\nPreparation mechanism (e.g. to pre-allocate a cache or record a tape)\nBuilt-in sparsity handling\nThorough validation on standard inputs and outputs (numbers, vectors, matrices)\nTesting and benchmarking utilities accessible to users with DifferentiationInterfaceTest","category":"page"},{"location":"#Compatibility","page":"Home","title":"Compatibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We support the following backends defined by ADTypes.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ChainRulesCore.jl\nDiffractor.jl\nEnzyme.jl\nFastDifferentiation.jl\nFiniteDiff.jl\nFiniteDifferences.jl\nForwardDiff.jl\nMooncake.jl\nPolyesterForwardDiff.jl\nReverseDiff.jl\nSymbolics.jl\nTracker.jl\nZygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that in some cases, going through DifferentiationInterface.jl might be slower than a direct call to the backend's API. This is mostly true for Enzyme.jl, whose handling of activities and multiple arguments unlocks additional performance. We are working on this challenge, and welcome any suggestions or contributions. Meanwhile, if differentiation fails or takes too long, consider using Enzyme.jl directly.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the stable version of the package, run the following code in a Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\"DifferentiationInterface\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install the development version, run this instead:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\n    url=\"https://github.com/gdalle/DifferentiationInterface.jl\",\n    subdir=\"DifferentiationInterface\"\n)","category":"page"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using DifferentiationInterface\nimport ForwardDiff, Enzyme, Zygote  # AD backends you want to use \n\nf(x) = sum(abs2, x)\n\nx = [1.0, 2.0]\n\nvalue_and_gradient(f, AutoForwardDiff(), x) # returns (5.0, [2.0, 4.0]) with ForwardDiff.jl\nvalue_and_gradient(f, AutoEnzyme(),      x) # returns (5.0, [2.0, 4.0]) with Enzyme.jl\nvalue_and_gradient(f, AutoZygote(),      x) # returns (5.0, [2.0, 4.0]) with Zygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"To improve your performance by up to several orders of magnitude compared to this example, take a look at the tutorial and its section on operator preparation.","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please cite both DifferentiationInterface.jl and its inspiration AbstractDifferentiation.jl, using the provided CITATION.bib file.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"explanation/backends/#Backends","page":"Backends","title":"Backends","text":"","category":"section"},{"location":"explanation/backends/#List","page":"Backends","title":"List","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We support the following dense backend choices from ADTypes.jl:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"AutoChainRules\nAutoDiffractor\nAutoEnzyme\nAutoFastDifferentiation\nAutoFiniteDiff\nAutoFiniteDifferences\nAutoForwardDiff\nAutoMooncake\nAutoPolyesterForwardDiff\nAutoReverseDiff\nAutoSymbolics\nAutoTracker\nAutoZygote","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"note: Note\nDifferentiationInterface.jl itself is compatible with Julia 1.6, the Long Term Support (LTS) version of the language. However, we were only able to test the following backends on Julia 1.6:AutoFiniteDifferences\nAutoForwardDiff\nAutoReverseDiff\nAutoTracker\nAutoZygoteWe strongly recommend that users upgrade to Julia 1.10 or above, where all backends are tested.","category":"page"},{"location":"explanation/backends/#Features","page":"Backends","title":"Features","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Given a backend object, you can use:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"check_available to know whether the required AD package is loaded\ncheck_inplace to know whether the backend supports in-place functions (all backends support out-of-place functions)","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"In theory, all we need from each backend is either a pushforward or a pullback: we can deduce every other operator from these two. In practice, many AD backends have custom implementations for high-level operators like gradient or jacobian, which we reuse whenever possible.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"details: Details\nIn the rough summary table below,✅ means that we reuse the custom implementation from the backend;\n❌ means that a custom implementation doesn't exist, so we use our default fallbacks;\n🔀 means it's complicated or not done yet. pf pb der grad jac hess hvp der2\nAutoChainRules ❌ ✅ ❌ ❌ ❌ ❌ ❌ ❌\nAutoDiffractor ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌\nAutoEnzyme (forward) ✅ ❌ ❌ ✅ ✅ ❌ ❌ ❌\nAutoEnzyme (reverse) ❌ ✅ ❌ ✅ ✅ ❌ 🔀 ❌\nAutoFastDifferentiation ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅\nAutoFiniteDiff 🔀 ❌ ✅ ✅ ✅ ✅ ❌ ❌\nAutoFiniteDifferences 🔀 ❌ ❌ ✅ ✅ ❌ ❌ ❌\nAutoForwardDiff ✅ ❌ ✅ ✅ ✅ ✅ ✅ ✅\nAutoMooncake ❌ ✅ ❌ ❌ ❌ ❌ ❌ ❌\nAutoPolyesterForwardDiff 🔀 ❌ 🔀 ✅ ✅ 🔀 🔀 🔀\nAutoReverseDiff ❌ 🔀 ❌ ✅ ✅ ✅ ❌ ❌\nAutoSymbolics ✅ ❌ ✅ ✅ ✅ ✅ ❌ ❌\nAutoTracker ❌ ✅ ❌ ✅ ❌ ❌ ❌ ❌\nAutoZygote ❌ ✅ ❌ ✅ ✅ ✅ 🔀 ❌","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Moreover, each context type is supported by a specific subset of backends:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":" Constant\nAutoChainRules ✅\nAutoDiffractor ❌\nAutoEnzyme (forward) ✅\nAutoEnzyme (reverse) ✅\nAutoFastDifferentiation ❌\nAutoFiniteDiff ✅\nAutoFiniteDifferences ✅\nAutoForwardDiff ✅\nAutoMooncake ✅\nAutoPolyesterForwardDiff ✅\nAutoReverseDiff ✅\nAutoSymbolics ❌\nAutoTracker ✅\nAutoZygote ✅","category":"page"},{"location":"explanation/backends/#Second-order","page":"Backends","title":"Second order","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For second-order operators like second_derivative, hessian and hvp, there are two main options. You can either use a single backend, or combine two of them within the SecondOrder struct:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"backend = SecondOrder(outer_backend, inner_backend)","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"The inner backend will be called first, and the outer backend will differentiate the generated code. In general, using a forward outer backend over a reverse inner backend will yield the best performance.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nSecond-order AD is tricky, and many backend combinations will fail (even if you combine a backend with itself). Be ready to experiment and open issues if necessary.","category":"page"},{"location":"explanation/backends/#Backend-switch","page":"Backends","title":"Backend switch","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"The wrapper DifferentiateWith allows you to switch between backends. It takes a function f and specifies that f should be differentiated with the substitute backend of your choice, instead of whatever true backend the surrounding code is trying to use. In other words, when someone tries to differentiate dw = DifferentiateWith(f, substitute_backend) with true_backend, then substitute_backend steps in and true_backend does not dive into the function f itself. At the moment, DifferentiateWith only works when true_backend is either ForwardDiff.jl or a ChainRules.jl-compatible backend.","category":"page"},{"location":"explanation/backends/#Implementations","page":"Backends","title":"Implementations","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"What follows is a list of implementation details from the package extensions of DifferentiationInterface.jl It is not part of the public API or protected by semantic versioning, and it may become outdated. When in doubt, refer to the code itself.","category":"page"},{"location":"explanation/backends/#ChainRulesCore","page":"Backends","title":"ChainRulesCore","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We only implement pullback, using the RuleConfig mechanism to call back into AD. Same-point preparation runs the forward sweep and returns the pullback closure.","category":"page"},{"location":"explanation/backends/#Diffractor","page":"Backends","title":"Diffractor","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We only implement pushforward.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nThe latest releases of Diffractor broke DifferentiationInterface.","category":"page"},{"location":"explanation/backends/#Enzyme","page":"Backends","title":"Enzyme","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Depending on the mode attribute inside AutoEnzyme, we implement either pushforward or pullback based on Enzyme.autodiff. When necessary, preparation chooses a number of chunks (for gradient and jacobian in forward mode, for jacobian only in reverse mode).","category":"page"},{"location":"explanation/backends/#FastDifferentiation","page":"Backends","title":"FastDifferentiation","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For every operator, preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"warning: Warning\nPreparation can be very slow for symbolic AD.","category":"page"},{"location":"explanation/backends/#FiniteDiff","page":"Backends","title":"FiniteDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Whenever possible, preparation creates a cache object. Pushforward is implemented rather slowly using a closure.","category":"page"},{"location":"explanation/backends/#FiniteDifferences","page":"Backends","title":"FiniteDifferences","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Nothing specific to mention.","category":"page"},{"location":"explanation/backends/#ForwardDiff","page":"Backends","title":"ForwardDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We implement pushforward directly using Dual numbers, and preparation allocates the necessary space. For higher level operators, preparation creates a config object, which can be type-unstable.","category":"page"},{"location":"explanation/backends/#PolyesterForwardDiff","page":"Backends","title":"PolyesterForwardDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Most operators fall back on AutoForwardDiff.","category":"page"},{"location":"explanation/backends/#ReverseDiff","page":"Backends","title":"ReverseDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Wherever possible, preparation records a tape of the function's execution. This tape is computed from the arguments x and contexts... provided at preparation time. It is control-flow dependent, so only one branch is recorded at each if statement.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nIf your function has value-specific control flow (like if x[1] > 0 or if c == 1), you may get silently wrong results whenever it takes new branches that were not taken during preparation. You must make sure to run preparation with an input and contexts whose values trigger the correct control flow for future executions.","category":"page"},{"location":"explanation/backends/#Symbolics","page":"Backends","title":"Symbolics","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For all operators, preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"warning: Warning\nPreparation can be very slow for symbolic AD.","category":"page"},{"location":"explanation/backends/#Mooncake","page":"Backends","title":"Mooncake","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For pullback, preparation builds the reverse rule of the function.","category":"page"},{"location":"explanation/backends/#Tracker","page":"Backends","title":"Tracker","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We implement pullback based on Tracker.back. Same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"explanation/backends/#Zygote","page":"Backends","title":"Zygote","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We implement pullback based on Zygote.pullback. Same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"","category":"page"},{"location":"tutorials/basic/#Basic-tutorial","page":"Basic tutorial","title":"Basic tutorial","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"We present the main features of DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"using DifferentiationInterface","category":"page"},{"location":"tutorials/basic/#Computing-a-gradient","page":"Basic tutorial","title":"Computing a gradient","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"A common use case of automatic differentiation (AD) is optimizing real-valued functions with first- or second-order methods. Let's define a simple objective (the squared norm) and a random input vector","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"f(x) = sum(abs2, x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"x = collect(1.0:5.0)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"To compute its gradient, we need to choose a \"backend\", i.e. an AD package to call under the hood. Most backend types are defined by ADTypes.jl and re-exported by DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"ForwardDiff.jl is very generic and efficient for low-dimensional inputs, so it's a good starting point:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"import ForwardDiff\n\nbackend = AutoForwardDiff()","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"tip: Tip\nTo avoid name conflicts, load AD packages with import instead of using. Indeed, most AD packages also export operators like gradient and jacobian, but you only want to use the ones from DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Now you can use the following syntax to compute the gradient:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"gradient(f, backend, x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Was that fast? BenchmarkTools.jl helps you answer that question.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"using BenchmarkTools\n\n@benchmark gradient($f, $backend, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Not bad, but you can do better.","category":"page"},{"location":"tutorials/basic/#Overwriting-a-gradient","page":"Basic tutorial","title":"Overwriting a gradient","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Since you know how much space your gradient will occupy (the same as your input x), you can pre-allocate that memory and offer it to AD. Some backends get a speed boost from this trick.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"grad = similar(x)\ngradient!(f, grad, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"The bang indicates that one of the arguments of gradient! might be mutated. More precisely, our convention is that every positional argument between the function and the backend is mutated.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"@benchmark gradient!($f, $grad, $backend, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"For some reason the in-place version is not much better than your first attempt. However, it makes fewer allocations, thanks to the gradient vector you provided. Don't worry, you can get even more performance.","category":"page"},{"location":"tutorials/basic/#Preparing-for-multiple-gradients","page":"Basic tutorial","title":"Preparing for multiple gradients","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Internally, ForwardDiff.jl creates some data structures to keep track of things. These objects can be reused between gradient computations, even on different input values. We abstract away the preparation step behind a backend-agnostic syntax:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"prep = prepare_gradient(f, backend, zero(x))","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"You don't need to know what this object is, you just need to pass it to the gradient operator. Note that preparation does not depend on the actual components of the vector x, just on its type and size. You can thus reuse the prep for different values of the input.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"grad = similar(x)\ngradient!(f, grad, prep, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Preparation makes the gradient computation much faster, and (in this case) allocation-free.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"@benchmark gradient!($f, $grad, $prep, $backend, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Beware that the prep object is nearly always mutated by differentiation operators, even though it is given as the last positional argument.","category":"page"},{"location":"tutorials/basic/#Switching-backends","page":"Basic tutorial","title":"Switching backends","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"The whole point of DifferentiationInterface.jl is that you can easily experiment with different AD solutions. Typically, for gradients, reverse mode AD might be a better fit, so let's try the state-of-the-art Enzyme.jl!","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"import Enzyme\n\nbackend2 = AutoEnzyme()","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Once the backend is created, things run smoothly with exactly the same syntax as before:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"gradient(f, backend2, x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"And you can run the same benchmarks to see what you gained (although such a small input may not be realistic):","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"prep2 = prepare_gradient(f, backend2, zero(x))\n\n@benchmark gradient!($f, $grad, $prep2, $backend2, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"In short, DifferentiationInterface.jl allows for easy testing and comparison of AD backends. If you want to go further, check out the documentation of DifferentiationInterfaceTest.jl. This related package provides benchmarking utilities to compare backends and help you select the one that is best suited for your problem.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"","category":"page"}]
}
