var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#DifferentiationInterface","page":"API","title":"DifferentiationInterface","text":"DifferentiationInterface\n\nAn interface to various automatic differentiation backends in Julia.\n\n\n\n\n\n","category":"module"},{"location":"api/#Argument-wrappers","page":"API","title":"Argument wrappers","text":"","category":"section"},{"location":"api/#DifferentiationInterface.Context","page":"API","title":"DifferentiationInterface.Context","text":"Context\n\nAbstract supertype for additional context arguments, which can be passed to differentiation operators after the active input x but are not differentiated.\n\nSubtypes\n\nConstant\nCache\nConstantOrCache\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.Constant","page":"API","title":"DifferentiationInterface.Constant","text":"Constant\n\nConcrete type of Context argument which is kept constant during differentiation.\n\nNote that an operator can be prepared with an arbitrary value of the constant. However, same-point preparation must occur with the exact value that will be reused later.\n\nwarning: Warning\nSome backends require any Constant context to be a Number or an AbstractArray.\n\nExample\n\njulia> using DifferentiationInterface\n\njulia> using ForwardDiff: ForwardDiff\n\njulia> f(x, c) = c * sum(abs2, x);\n\njulia> gradient(f, AutoForwardDiff(), [1.0, 2.0], Constant(10))\n2-element Vector{Float64}:\n 20.0\n 40.0\n\njulia> gradient(f, AutoForwardDiff(), [1.0, 2.0], Constant(100))\n2-element Vector{Float64}:\n 200.0\n 400.0\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.Cache","page":"API","title":"DifferentiationInterface.Cache","text":"Cache\n\nConcrete type of Context argument which can be mutated with active values during differentiation.\n\nThe initial values present inside the cache do not matter.\n\nFor some backends, preparation allocates the required memory for Cache contexts with the right element type, similar to PreallocationTools.jl.\n\nwarning: Warning\nSome backends require any Cache context to be an AbstractArray, others accept nested (named) tuples of AbstractArrays.\n\nExample\n\njulia> using DifferentiationInterface\n\njulia> using ForwardDiff: ForwardDiff\n\njulia> f(x, c) = sum(copyto!(c, x));\n\njulia> prep = prepare_gradient(f, AutoForwardDiff(), [1.0, 2.0], Cache(zeros(2)));\n\njulia> gradient(f, prep, AutoForwardDiff(), [3.0, 4.0], Cache(zeros(2)))\n2-element Vector{Float64}:\n 1.0\n 1.0\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ConstantOrCache","page":"API","title":"DifferentiationInterface.ConstantOrCache","text":"ConstantOrCache\n\nConcrete type of Context argument which can contain a mixture of constants and caches, passed along to the backend without modification.\n\nUnlike for Cache, it is up to the user to ensure that the internal storage can adapt to the required element types, for instance by using PreallocationTools.jl directly.\n\n\n\n\n\n","category":"type"},{"location":"api/#First-order","page":"API","title":"First order","text":"","category":"section"},{"location":"api/#Pushforward","page":"API","title":"Pushforward","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_pushforward","page":"API","title":"DifferentiationInterface.prepare_pushforward","text":"prepare_pushforward(f,     backend, x, tx, [contexts...]; strict=Val(true)) -> prep\nprepare_pushforward(f!, y, backend, x, tx, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to pushforward and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nFor in-place functions, y is mutated by f! during preparation.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to pushforward do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pushforward_same_point","page":"API","title":"DifferentiationInterface.prepare_pushforward_same_point","text":"prepare_pushforward_same_point(f,     backend, x, tx, [contexts...]; strict=Val(true)) -> prep_same\nprepare_pushforward_same_point(f!, y, backend, x, tx, [contexts...]; strict=Val(true)) -> prep_same\n\nCreate a prep object that can be given to pushforward and its variants to speed them up, if they are applied at the same point x and with the same contexts.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nFor in-place functions, y is mutated by f! during preparation.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to pushforward do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward","page":"API","title":"DifferentiationInterface.pushforward","text":"pushforward(f,     [prep,] backend, x, tx, [contexts...]) -> ty\npushforward(f!, y, [prep,] backend, x, tx, [contexts...]) -> ty\n\nCompute the pushforward of the function f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named jvp.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward!","page":"API","title":"DifferentiationInterface.pushforward!","text":"pushforward!(f,     dy, [prep,] backend, x, tx, [contexts...]) -> ty\npushforward!(f!, y, dy, [prep,] backend, x, tx, [contexts...]) -> ty\n\nCompute the pushforward of the function f at point x with a tuple of tangents tx, overwriting ty.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named jvp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward","page":"API","title":"DifferentiationInterface.value_and_pushforward","text":"value_and_pushforward(f,     [prep,] backend, x, tx, [contexts...]) -> (y, ty)\nvalue_and_pushforward(f!, y, [prep,] backend, x, tx, [contexts...]) -> (y, ty)\n\nCompute the value and the pushforward of the function f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named value_and_jvp.\n\ninfo: Info\nRequired primitive for forward mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward!","page":"API","title":"DifferentiationInterface.value_and_pushforward!","text":"value_and_pushforward!(f,     dy, [prep,] backend, x, tx, [contexts...]) -> (y, ty)\nvalue_and_pushforward!(f!, y, dy, [prep,] backend, x, tx, [contexts...]) -> (y, ty)\n\nCompute the value and the pushforward of the function f at point x with a tuple of tangents tx, overwriting ty.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named value_and_jvp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#Pullback","page":"API","title":"Pullback","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_pullback","page":"API","title":"DifferentiationInterface.prepare_pullback","text":"prepare_pullback(f,     backend, x, ty, [contexts...]; strict=Val(true)) -> prep\nprepare_pullback(f!, y, backend, x, ty, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to pullback and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nFor in-place functions, y is mutated by f! during preparation.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to pullback do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pullback_same_point","page":"API","title":"DifferentiationInterface.prepare_pullback_same_point","text":"prepare_pullback_same_point(f,     backend, x, ty, [contexts...]; strict=Val(true)) -> prep_same\nprepare_pullback_same_point(f!, y, backend, x, ty, [contexts...]; strict=Val(true)) -> prep_same\n\nCreate a prep object that can be given to pullback and its variants to speed them up, if they are applied at the same point x and with the same contexts.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nFor in-place functions, y is mutated by f! during preparation.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to pullback do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback","page":"API","title":"DifferentiationInterface.pullback","text":"pullback(f,     [prep,] backend, x, ty, [contexts...]) -> tx\npullback(f!, y, [prep,] backend, x, ty, [contexts...]) -> tx\n\nCompute the pullback of the function f at point x with a tuple of tangents ty.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named vjp.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback!","page":"API","title":"DifferentiationInterface.pullback!","text":"pullback!(f,     dx, [prep,] backend, x, ty, [contexts...]) -> tx\npullback!(f!, y, dx, [prep,] backend, x, ty, [contexts...]) -> tx\n\nCompute the pullback of the function f at point x with a tuple of tangents ty, overwriting dx.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named vjp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback","page":"API","title":"DifferentiationInterface.value_and_pullback","text":"value_and_pullback(f,     [prep,] backend, x, ty, [contexts...]) -> (y, tx)\nvalue_and_pullback(f!, y, [prep,] backend, x, ty, [contexts...]) -> (y, tx)\n\nCompute the value and the pullback of the function f at point x with a tuple of tangents ty.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named value_and_vjp.\n\ninfo: Info\nRequired primitive for reverse mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback!","page":"API","title":"DifferentiationInterface.value_and_pullback!","text":"value_and_pullback!(f,     dx, [prep,] backend, x, ty, [contexts...]) -> (y, tx)\nvalue_and_pullback!(f!, y, dx, [prep,] backend, x, ty, [contexts...]) -> (y, tx)\n\nCompute the value and the pullback of the function f at point x with a tuple of tangents ty, overwriting dx.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named value_and_vjp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#Derivative","page":"API","title":"Derivative","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_derivative","page":"API","title":"DifferentiationInterface.prepare_derivative","text":"prepare_derivative(f,     backend, x, [contexts...]; strict=Val(true)) -> prep\nprepare_derivative(f!, y, backend, x, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to derivative and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nFor in-place functions, y is mutated by f! during preparation.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to derivative do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative","page":"API","title":"DifferentiationInterface.derivative","text":"derivative(f,     [prep,] backend, x, [contexts...]) -> der\nderivative(f!, y, [prep,] backend, x, [contexts...]) -> der\n\nCompute the derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative!","page":"API","title":"DifferentiationInterface.derivative!","text":"derivative!(f,     der, [prep,] backend, x, [contexts...]) -> der\nderivative!(f!, y, der, [prep,] backend, x, [contexts...]) -> der\n\nCompute the derivative of the function f at point x, overwriting der.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative","page":"API","title":"DifferentiationInterface.value_and_derivative","text":"value_and_derivative(f,     [prep,] backend, x, [contexts...]) -> (y, der)\nvalue_and_derivative(f!, y, [prep,] backend, x, [contexts...]) -> (y, der)\n\nCompute the value and the derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative!","page":"API","title":"DifferentiationInterface.value_and_derivative!","text":"value_and_derivative!(f,     der, [prep,] backend, x, [contexts...]) -> (y, der)\nvalue_and_derivative!(f!, y, der, [prep,] backend, x, [contexts...]) -> (y, der)\n\nCompute the value and the derivative of the function f at point x, overwriting der.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#Gradient","page":"API","title":"Gradient","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_gradient","page":"API","title":"DifferentiationInterface.prepare_gradient","text":"prepare_gradient(f, backend, x, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to gradient and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to gradient do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient","page":"API","title":"DifferentiationInterface.gradient","text":"gradient(f, [prep,] backend, x, [contexts...]) -> grad\n\nCompute the gradient of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient!","page":"API","title":"DifferentiationInterface.gradient!","text":"gradient!(f, grad, [prep,] backend, x, [contexts...]) -> grad\n\nCompute the gradient of the function f at point x, overwriting grad.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient","page":"API","title":"DifferentiationInterface.value_and_gradient","text":"value_and_gradient(f, [prep,] backend, x, [contexts...]) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient!","page":"API","title":"DifferentiationInterface.value_and_gradient!","text":"value_and_gradient!(f, grad, [prep,] backend, x, [contexts...]) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x, overwriting grad.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jacobian","page":"API","title":"Jacobian","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_jacobian","page":"API","title":"DifferentiationInterface.prepare_jacobian","text":"prepare_jacobian(f,     backend, x, [contexts...]; strict=Val(true)) -> prep\nprepare_jacobian(f!, y, backend, x, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to jacobian and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nFor in-place functions, y is mutated by f! during preparation.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to jacobian do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian","page":"API","title":"DifferentiationInterface.jacobian","text":"jacobian(f,     [prep,] backend, x, [contexts...]) -> jac\njacobian(f!, y, [prep,] backend, x, [contexts...]) -> jac\n\nCompute the Jacobian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian!","page":"API","title":"DifferentiationInterface.jacobian!","text":"jacobian!(f,     jac, [prep,] backend, x, [contexts...]) -> jac\njacobian!(f!, y, jac, [prep,] backend, x, [contexts...]) -> jac\n\nCompute the Jacobian matrix of the function f at point x, overwriting jac.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian","page":"API","title":"DifferentiationInterface.value_and_jacobian","text":"value_and_jacobian(f,     [prep,] backend, x, [contexts...]) -> (y, jac)\nvalue_and_jacobian(f!, y, [prep,] backend, x, [contexts...]) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian!","page":"API","title":"DifferentiationInterface.value_and_jacobian!","text":"value_and_jacobian!(f,     jac, [prep,] backend, x, [contexts...]) -> (y, jac)\nvalue_and_jacobian!(f!, y, jac, [prep,] backend, x, [contexts...]) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x, overwriting jac.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#Second-order","page":"API","title":"Second order","text":"","category":"section"},{"location":"api/#DifferentiationInterface.SecondOrder","page":"API","title":"DifferentiationInterface.SecondOrder","text":"SecondOrder\n\nCombination of two backends for second-order differentiation.\n\ndanger: Danger\nSecondOrder backends do not support first-order operators.\n\nConstructor\n\nSecondOrder(outer_backend, inner_backend)\n\nFields\n\nouter::AbstractADType: backend for the outer differentiation\ninner::AbstractADType: backend for the inner differentiation\n\n\n\n\n\n","category":"type"},{"location":"api/#Second-derivative","page":"API","title":"Second derivative","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_second_derivative","page":"API","title":"DifferentiationInterface.prepare_second_derivative","text":"prepare_second_derivative(f, backend, x, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to second_derivative and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to second_derivative do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative","page":"API","title":"DifferentiationInterface.second_derivative","text":"second_derivative(f, [prep,] backend, x, [contexts...]) -> der2\n\nCompute the second derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative!","page":"API","title":"DifferentiationInterface.second_derivative!","text":"second_derivative!(f, der2, [prep,] backend, x, [contexts...]) -> der2\n\nCompute the second derivative of the function f at point x, overwriting der2.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_derivative_and_second_derivative","page":"API","title":"DifferentiationInterface.value_derivative_and_second_derivative","text":"value_derivative_and_second_derivative(f, [prep,] backend, x, [contexts...]) -> (y, der, der2)\n\nCompute the value, first derivative and second derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_derivative_and_second_derivative!","page":"API","title":"DifferentiationInterface.value_derivative_and_second_derivative!","text":"value_derivative_and_second_derivative!(f, der, der2, [prep,] backend, x, [contexts...]) -> (y, der, der2)\n\nCompute the value, first derivative and second derivative of the function f at point x, overwriting der and der2.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian-vector-product","page":"API","title":"Hessian-vector product","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_hvp","page":"API","title":"DifferentiationInterface.prepare_hvp","text":"prepare_hvp(f, backend, x, tx, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to hvp and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to hvp do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_hvp_same_point","page":"API","title":"DifferentiationInterface.prepare_hvp_same_point","text":"prepare_hvp_same_point(f, backend, x, tx, [contexts...]; strict=Val(true)) -> prep_same\n\nCreate a prep object that can be given to hvp and its variants to speed them up, if they are applied at the same point x and with the same contexts.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to hvp do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp","page":"API","title":"DifferentiationInterface.hvp","text":"hvp(f, [prep,] backend, x, tx, [contexts...]) -> tg\n\nCompute the Hessian-vector product of f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp!","page":"API","title":"DifferentiationInterface.hvp!","text":"hvp!(f, tg, [prep,] backend, x, tx, [contexts...]) -> tg\n\nCompute the Hessian-vector product of f at point x with a tuple of tangents tx, overwriting tg.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient_and_hvp","page":"API","title":"DifferentiationInterface.gradient_and_hvp","text":"gradient_and_hvp(f, [prep,] backend, x, tx, [contexts...]) -> (grad, tg)\n\nCompute the gradient and the Hessian-vector product of f at point x with a tuple of tangents tx.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient_and_hvp!","page":"API","title":"DifferentiationInterface.gradient_and_hvp!","text":"gradient_and_hvp!(f, grad, tg, [prep,] backend, x, tx, [contexts...]) -> (grad, tg)\n\nCompute the gradient and the Hessian-vector product of f at point x with a tuple of tangents tx, overwriting grad and tg.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian","page":"API","title":"Hessian","text":"","category":"section"},{"location":"api/#DifferentiationInterface.prepare_hessian","page":"API","title":"DifferentiationInterface.prepare_hessian","text":"prepare_hessian(f, backend, x, [contexts...]; strict=Val(true)) -> prep\n\nCreate a prep object that can be given to hessian and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to hessian do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian","page":"API","title":"DifferentiationInterface.hessian","text":"hessian(f, [prep,] backend, x, [contexts...]) -> hess\n\nCompute the Hessian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian!","page":"API","title":"DifferentiationInterface.hessian!","text":"hessian!(f, hess, [prep,] backend, x, [contexts...]) -> hess\n\nCompute the Hessian matrix of the function f at point x, overwriting hess.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_gradient_and_hessian","page":"API","title":"DifferentiationInterface.value_gradient_and_hessian","text":"value_gradient_and_hessian(f, [prep,] backend, x, [contexts...]) -> (y, grad, hess)\n\nCompute the value, gradient vector and Hessian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_gradient_and_hessian!","page":"API","title":"DifferentiationInterface.value_gradient_and_hessian!","text":"value_gradient_and_hessian!(f, grad, hess, [prep,] backend, x, [contexts...]) -> (y, grad, hess)\n\nCompute the value, gradient vector and Hessian matrix of the function f at point x, overwriting grad and hess.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#Backend-queries","page":"API","title":"Backend queries","text":"","category":"section"},{"location":"api/#DifferentiationInterface.check_available","page":"API","title":"DifferentiationInterface.check_available","text":"check_available(backend)\n\nCheck whether backend is available (i.e. whether the extension is loaded) and return a Bool.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.check_inplace","page":"API","title":"DifferentiationInterface.check_inplace","text":"check_inplace(backend)\n\nCheck whether backend supports differentiation of in-place functions and return a Bool.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.outer","page":"API","title":"DifferentiationInterface.outer","text":"outer(backend::SecondOrder)\nouter(backend::AbstractADType)\n\nReturn the outer backend of a SecondOrder object, tasked with differentiation at the second order.\n\nFor any other backend type, this function acts like the identity.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.inner","page":"API","title":"DifferentiationInterface.inner","text":"inner(backend::SecondOrder)\ninner(backend::AbstractADType)\n\nReturn the inner backend of a SecondOrder object, tasked with differentiation at the first order.\n\nFor any other backend type, this function acts like the identity.\n\n\n\n\n\n","category":"function"},{"location":"api/#Backend-switch","page":"API","title":"Backend switch","text":"","category":"section"},{"location":"api/#DifferentiationInterface.DifferentiateWith","page":"API","title":"DifferentiationInterface.DifferentiateWith","text":"DifferentiateWith\n\nFunction wrapper that enforces differentiation with a \"substitute\" AD backend, possible different from the \"true\" AD backend that is called.\n\nFor instance, suppose a function f is not differentiable with Zygote because it involves mutation, but you know that it is differentiable with Enzyme. Then f2 = DifferentiateWith(f, AutoEnzyme()) is a new function that behaves like f, except that f2 is differentiable with Zygote (thanks to a chain rule which calls Enzyme under the hood). Moreover, any larger algorithm alg that calls f2 instead of f will also be differentiable with Zygote (as long as f was the only Zygote blocker).\n\ntip: Tip\nThis is mainly relevant for package developers who want to produce differentiable code at low cost, without writing the differentiation rules themselves. If you sprinkle a few DifferentiateWith in places where some AD backends may struggle, end users can pick from a wider variety of packages to differentiate your algorithms.\n\nwarning: Warning\nDifferentiateWith only supports out-of-place functions y = f(x) without additional context arguments. It only makes these functions differentiable if the true backend is either ForwardDiff, reverse-mode Mooncake, or if it automatically importing rules from ChainRules (e.g. Zygote). Some backends are also able to manually import rules from ChainRules. For any other true backend, the differentiation behavior is not altered by DifferentiateWith (it becomes a transparent wrapper).\n\nwarning: Warning\nWhen using DifferentiateWith(f, AutoSomething()), the function f must not close over any active data. As of now, we cannot differentiate with respect to parameters stored inside f.\n\nFields\n\nf: the function in question, with signature f(x)\nbackend::AbstractADType: the substitute backend to use for differentiation\n\nnote: Note\nFor the substitute AD backend to be called under the hood, its package needs to be loaded in addition to the package of the true AD backend.\n\nConstructor\n\nDifferentiateWith(f, backend)\n\nExample\n\njulia> using DifferentiationInterface\n\njulia> using FiniteDiff: FiniteDiff\n       using ForwardDiff: ForwardDiff\n       using Zygote: Zygote\n\njulia> function f(x::Vector{Float64})\n           a = Vector{Float64}(undef, 1)  # type constraint breaks ForwardDiff\n           a[1] = sum(abs2, x)  # mutation breaks Zygote\n           return a[1]\n       end;\n\njulia> f2 = DifferentiateWith(f, AutoFiniteDiff());\n\njulia> f([3.0, 5.0]) == f2([3.0, 5.0])\ntrue\n\njulia> alg(x) = 7 * f2(x);\n\njulia> ForwardDiff.gradient(alg, [3.0, 5.0])\n2-element Vector{Float64}:\n 42.0\n 70.0\n\njulia> Zygote.gradient(alg, [3.0, 5.0])[1]\n2-element Vector{Float64}:\n 42.0\n 70.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Sparsity-tools","page":"API","title":"Sparsity tools","text":"","category":"section"},{"location":"api/#DifferentiationInterface.MixedMode","page":"API","title":"DifferentiationInterface.MixedMode","text":"MixedMode\n\nCombination of a forward and a reverse mode backend for mixed-mode sparse Jacobian computation.\n\ndanger: Danger\nMixedMode backends only support jacobian and its variants, and it should be used inside an AutoSparse wrapper.\n\nConstructor\n\nMixedMode(forward_backend, reverse_backend)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.DenseSparsityDetector","page":"API","title":"DifferentiationInterface.DenseSparsityDetector","text":"DenseSparsityDetector\n\nSparsity pattern detector satisfying the detection API of ADTypes.jl.\n\nThe nonzeros in a Jacobian or Hessian are detected by computing the relevant matrix with dense AD, and thresholding the entries with a given tolerance (which can be numerically inaccurate). This process can be very slow, and should only be used if its output can be exploited multiple times to compute many sparse matrices.\n\ndanger: Danger\nIn general, the sparsity pattern you obtain can depend on the provided input x. If you want to reuse the pattern, make sure that it is input-agnostic.\n\nwarning: Warning\nDenseSparsityDetector functionality is now located in a package extension, please load the SparseArrays.jl standard library before you use it.\n\nFields\n\nbackend::AbstractADType is the dense AD backend used under the hood\natol::Float64 is the minimum magnitude of a matrix entry to be considered nonzero\n\nConstructor\n\nDenseSparsityDetector(backend; atol, method=:iterative)\n\nThe keyword argument method::Symbol can be either:\n\n:iterative: compute the matrix in a sequence of matrix-vector products (memory-efficient)\n:direct: compute the matrix all at once (memory-hungry but sometimes faster).\n\nNote that the constructor is type-unstable because method ends up being a type parameter of the DenseSparsityDetector object (this is not part of the API and might change).\n\nExamples\n\nusing ADTypes, DifferentiationInterface, SparseArrays\nusing ForwardDiff: ForwardDiff\n\ndetector = DenseSparsityDetector(AutoForwardDiff(); atol=1e-5, method=:direct)\n\nADTypes.jacobian_sparsity(diff, rand(5), detector)\n\n# output\n\n4×5 SparseMatrixCSC{Bool, Int64} with 8 stored entries:\n 1  1  ⋅  ⋅  ⋅\n ⋅  1  1  ⋅  ⋅\n ⋅  ⋅  1  1  ⋅\n ⋅  ⋅  ⋅  1  1\n\nSometimes the sparsity pattern is input-dependent:\n\nADTypes.jacobian_sparsity(x -> [prod(x)], rand(2), detector)\n\n# output\n\n1×2 SparseMatrixCSC{Bool, Int64} with 2 stored entries:\n 1  1\n\nADTypes.jacobian_sparsity(x -> [prod(x)], [0, 1], detector)\n\n# output\n\n1×2 SparseMatrixCSC{Bool, Int64} with 1 stored entry:\n 1  ⋅\n\n\n\n\n\n","category":"type"},{"location":"api/#From-primitive","page":"API","title":"From primitive","text":"","category":"section"},{"location":"api/#DifferentiationInterface.AutoForwardFromPrimitive","page":"API","title":"DifferentiationInterface.AutoForwardFromPrimitive","text":"AutoForwardFromPrimitive(backend::AbstractADType)\n\nWrapper which forces a given backend to act as a forward-mode backend, using only its native value_and_pushforward primitive and re-implementing the rest from scratch.\n\ntip: Tip\nThis can be useful to circumvent high-level operators when they have impractical limitations. For instance, ForwardDiff.jl's jacobian does not support GPU arrays but its pushforward does, so AutoForwardFromPrimitive(AutoForwardDiff()) has a GPU-friendly jacobian.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.AutoReverseFromPrimitive","page":"API","title":"DifferentiationInterface.AutoReverseFromPrimitive","text":"AutoReverseFromPrimitive(backend::AbstractADType)\n\nWrapper which forces a given backend to act as a reverse-mode backend, using only its native value_and_pullback implementation and rebuilding the rest from scratch.\n\n\n\n\n\n","category":"type"},{"location":"api/#Internals","page":"API","title":"Internals","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The following is not part of the public API.","category":"page"},{"location":"api/#DifferentiationInterface.AutoSimpleFiniteDiff","page":"API","title":"DifferentiationInterface.AutoSimpleFiniteDiff","text":"AutoSimpleFiniteDiff <: ADTypes.AbstractADType\n\nForward mode backend based on the finite difference (f(x + ε) - f(x)) / ε, with artificial chunk size to mimick ForwardDiff.\n\nConstructor\n\nAutoSimpleFiniteDiff(ε=1e-5; chunksize=nothing)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.AutoZeroForward","page":"API","title":"DifferentiationInterface.AutoZeroForward","text":"AutoZeroForward <: ADTypes.AbstractADType\n\nTrivial backend that sets all derivatives to zero. Used in testing and benchmarking.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.AutoZeroReverse","page":"API","title":"DifferentiationInterface.AutoZeroReverse","text":"AutoZeroReverse <: ADTypes.AbstractADType\n\nTrivial backend that sets all derivatives to zero. Used in testing and benchmarking.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.BatchSizeSettings","page":"API","title":"DifferentiationInterface.BatchSizeSettings","text":"BatchSizeSettings{B,singlebatch,aligned}\n\nConfiguration for the batch size deduced from a backend and a sample array of length N.\n\nType parameters\n\nB::Int: batch size\nsinglebatch::Bool: whether B == N (B > N is only allowed when N == 0)\naligned::Bool: whether N % B == 0\n\nFields\n\nN::Int: array length\nA::Int: number of batches A = div(N, B, RoundUp)\nB_last::Int: size of the last batch (if aligned is false)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.DerivativePrep","page":"API","title":"DifferentiationInterface.DerivativePrep","text":"DerivativePrep\n\nAbstract type for additional information needed by derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.DontPrepareInner","page":"API","title":"DifferentiationInterface.DontPrepareInner","text":"DontPrepareInner\n\nTrait identifying outer backends for which the inner backend in second-order autodiff should not be prepared at all.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.FixTail","page":"API","title":"DifferentiationInterface.FixTail","text":"FixTail\n\nClosure around a function f and a set of tail argument tail_args such that\n\n(ft::FixTail)(args...) = ft.f(args..., ft.tail_args...)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardAndReverseMode","page":"API","title":"DifferentiationInterface.ForwardAndReverseMode","text":"ForwardAndReverseMode <: ADTypes.AbstractMode\n\nAppropriate mode type for MixedMode backends.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverForward","page":"API","title":"DifferentiationInterface.ForwardOverForward","text":"ForwardOverForward\n\nTraits identifying second-order backends that compute HVPs in forward over forward mode (inefficient).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverReverse","page":"API","title":"DifferentiationInterface.ForwardOverReverse","text":"ForwardOverReverse\n\nTraits identifying second-order backends that compute HVPs in forward over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.FunctionContext","page":"API","title":"DifferentiationInterface.FunctionContext","text":"FunctionContext\n\nPrivate type of Context argument used for passing functions inside second-order differentiation.\n\nBehaves differently for Enzyme only, where the function can be annotated.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.GradientPrep","page":"API","title":"DifferentiationInterface.GradientPrep","text":"GradientPrep\n\nAbstract type for additional information needed by gradient and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HVPPrep","page":"API","title":"DifferentiationInterface.HVPPrep","text":"HVPPrep\n\nAbstract type for additional information needed by hvp and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HessianPrep","page":"API","title":"DifferentiationInterface.HessianPrep","text":"HessianPrep\n\nAbstract type for additional information needed by hessian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.InPlaceNotSupported","page":"API","title":"DifferentiationInterface.InPlaceNotSupported","text":"InPlaceNotSupported\n\nTrait identifying backends that do not support in-place functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.InPlaceSupported","page":"API","title":"DifferentiationInterface.InPlaceSupported","text":"InPlaceSupported\n\nTrait identifying backends that support in-place functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.JacobianPrep","page":"API","title":"DifferentiationInterface.JacobianPrep","text":"JacobianPrep\n\nAbstract type for additional information needed by jacobian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PrepareInnerOverload","page":"API","title":"DifferentiationInterface.PrepareInnerOverload","text":"PrepareInnerOverload\n\nTrait identifying outer backends for which the inner backend in second-order autodiff should be prepared with an overloaded input type.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PrepareInnerSimple","page":"API","title":"DifferentiationInterface.PrepareInnerSimple","text":"PrepareInnerSimple\n\nTrait identifying outer backends for which the inner backend in second-order autodiff should be prepared with the same input type.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackFast","page":"API","title":"DifferentiationInterface.PullbackFast","text":"PullbackFast\n\nTrait identifying backends that support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackPrep","page":"API","title":"DifferentiationInterface.PullbackPrep","text":"PullbackPrep\n\nAbstract type for additional information needed by pullback and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackSlow","page":"API","title":"DifferentiationInterface.PullbackSlow","text":"PullbackSlow\n\nTrait identifying backends that do not support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardFast","page":"API","title":"DifferentiationInterface.PushforwardFast","text":"PushforwardFast\n\nTrait identifying backends that support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardPrep","page":"API","title":"DifferentiationInterface.PushforwardPrep","text":"PushforwardPrep\n\nAbstract type for additional information needed by pushforward and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardSlow","page":"API","title":"DifferentiationInterface.PushforwardSlow","text":"PushforwardSlow\n\nTrait identifying backends that do not support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverForward","page":"API","title":"DifferentiationInterface.ReverseOverForward","text":"ReverseOverForward\n\nTraits identifying second-order backends that compute HVPs in reverse over forward mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverReverse","page":"API","title":"DifferentiationInterface.ReverseOverReverse","text":"ReverseOverReverse\n\nTraits identifying second-order backends that compute HVPs in reverse over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.Rewrap","page":"API","title":"DifferentiationInterface.Rewrap","text":"Rewrap\n\nUtility for recording context types of additional arguments (e.g. Constant or Cache) and re-wrapping them into their types after they have been unwrapped.\n\nUseful for second-order differentiation.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.SecondDerivativePrep","page":"API","title":"DifferentiationInterface.SecondDerivativePrep","text":"SecondDerivativePrep\n\nAbstract type for additional information needed by second_derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADTypes.mode-Tuple{SecondOrder}","page":"API","title":"ADTypes.mode","text":"mode(backend::SecondOrder)\n\nReturn the outer mode of the second-order backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.basis-Tuple{AbstractArray, Any}","page":"API","title":"DifferentiationInterface.basis","text":"basis(a::AbstractArray, i)\n\nConstruct the i-th standard basis array in the vector space of a.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.fix_tail-Tuple{F} where F","page":"API","title":"DifferentiationInterface.fix_tail","text":"fix_tail(f, tail_args...)\n\nConvenience for constructing a FixTail, with a shortcut when there are no tail arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.forward_backend-Tuple{MixedMode}","page":"API","title":"DifferentiationInterface.forward_backend","text":"forward_backend(m::MixedMode)\n\nReturn the forward-mode part of a MixedMode backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.get_pattern-Tuple{AbstractMatrix}","page":"API","title":"DifferentiationInterface.get_pattern","text":"get_pattern(M::AbstractMatrix)\n\nReturn the Bool-valued sparsity pattern for a given matrix.\n\nOnly specialized on SparseMatrixCSC because it is used with symbolic backends, and at the moment their sparse Jacobian/Hessian utilities return a SparseMatrixCSC.\n\nThe trivial dense fallback is designed to protect against a change of format in these packages.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.hessian_sparsity_with_contexts-Union{Tuple{C}, Tuple{F}, Tuple{F, ADTypes.AbstractSparsityDetector, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.hessian_sparsity_with_contexts","text":"hessian_sparsity_with_contexts(f, detector, x, contexts...)\n\nWrapper around ADTypes.hessian_sparsity enabling the allocation of caches with proper element types.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.hvp_mode-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.hvp_mode","text":"hvp_mode(backend)\n\nReturn the best combination of modes for hvp and its variants, among the following options:\n\nForwardOverForward\nForwardOverReverse\nReverseOverForward\nReverseOverReverse\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.inner_preparation_behavior-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.inner_preparation_behavior","text":"inner_preparation_behavior(backend::AbstractADType)\n\nReturn PrepareInnerSimple, PrepareInnerOverload or DontPrepareInner in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.inplace_support-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.inplace_support","text":"inplace_support(backend)\n\nReturn InPlaceSupported or InPlaceNotSupported in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.ismutable_array-Tuple{Type}","page":"API","title":"DifferentiationInterface.ismutable_array","text":"ismutable_array(x)\n\nCheck whether x is a mutable array and return a Bool.\n\nAt the moment, this only returns false for StaticArrays.SArray.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.jacobian_sparsity_with_contexts-Union{Tuple{C}, Tuple{F}, Tuple{F, ADTypes.AbstractSparsityDetector, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.jacobian_sparsity_with_contexts","text":"jacobian_sparsity_with_contexts(f, detector, x, contexts...)\njacobian_sparsity_with_contexts(f!, y, detector, x, contexts...)\n\nWrapper around ADTypes.jacobian_sparsity enabling the allocation of caches with proper element types.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.multibasis-Tuple{AbstractArray, Any}","page":"API","title":"DifferentiationInterface.multibasis","text":"multibasis(a::AbstractArray, inds)\n\nConstruct the sum of the i-th standard basis arrays in the vector space of a for all i ∈ inds.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.overloaded_input_type","page":"API","title":"DifferentiationInterface.overloaded_input_type","text":"overloaded_input_type(prep)\n\nIf it exists, return the overloaded input type which will be passed to the differentiated function when preparation result prep is reused.\n\ndanger: Danger\nThis function is experimental and not part of the public API.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pick_batchsize-Tuple{ADTypes.AbstractADType, AbstractArray}","page":"API","title":"DifferentiationInterface.pick_batchsize","text":"pick_batchsize(backend, x_or_y::AbstractArray)\n\nReturn a BatchSizeSettings appropriate for arrays of the same length as x_or_y with a given backend.\n\nNote that the array in question can be either the input or the output of the function, depending on whether the backend performs forward- or reverse-mode AD.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pick_batchsize-Tuple{ADTypes.AbstractADType, Integer}","page":"API","title":"DifferentiationInterface.pick_batchsize","text":"pick_batchsize(backend, N::Integer)\n\nReturn a BatchSizeSettings appropriate for arrays of length N with a given backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_derivative-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.DerivativePrep, ADTypes.AbstractADType, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_derivative","text":"prepare!_derivative(f,     prep, backend, x, [contexts...]) -> new_prep\nprepare!_derivative(f!, y, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_derivative but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_gradient-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.GradientPrep, ADTypes.AbstractADType, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_gradient","text":"prepare!_gradient(f, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_gradient but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_hessian-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.HessianPrep, ADTypes.AbstractADType, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_hessian","text":"prepare!_hessian(f, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_hessian but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_hvp-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.HVPPrep, ADTypes.AbstractADType, Any, NTuple{N, T} where {N, T}, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_hvp","text":"prepare!_hvp(f, backend, x, tx, [contexts...]) -> new_prep\n\nCreate a prep object that can be given to hvp and its variants to speed them up.\n\nDepending on the backend, this can have several effects (preallocating memory, recording an execution trace) which are transparent to the user.\n\nwarning: Warning\nThe preparation result prep is only reusable as long as the arguments to hvp do not change type or size, and the function and backend themselves are not modified. Otherwise, preparation becomes invalid and you need to run it again. In some settings, invalid preparations may still give correct results (e.g. for backends that require no preparation), but this is not a semantic guarantee and should not be relied upon.\n\ndanger: Danger\nThe preparation result prep is not thread-safe. Sharing it between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.\n\nWhen strict=Val(true) (the default), type checking is enforced between preparation and execution (but size checking is left to the user). While your code may work for different types by setting strict=Val(false), this is not guaranteed by the API and can break without warning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_jacobian-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.JacobianPrep, ADTypes.AbstractADType, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_jacobian","text":"prepare!_jacobian(f,     prep, backend, x, [contexts...]) -> new_prep\nprepare!_jacobian(f!, y, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_jacobian but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_pullback-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.PullbackPrep, ADTypes.AbstractADType, Any, NTuple{N, T} where {N, T}, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_pullback","text":"prepare!_pullback(f,     prep, backend, x, ty, [contexts...]) -> new_prep\nprepare!_pullback(f!, y, prep, backend, x, ty, [contexts...]) -> new_prep\n\nSame behavior as prepare_pullback but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_pushforward-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.PushforwardPrep, ADTypes.AbstractADType, Any, NTuple{N, T} where {N, T}, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_pushforward","text":"prepare!_pushforward(f,     prep, backend, x, tx, [contexts...]) -> new_prep\nprepare!_pushforward(f!, y, prep, backend, x, tx, [contexts...]) -> new_prep\n\nSame behavior as prepare_pushforward but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.prepare!_second_derivative-Union{Tuple{C}, Tuple{F}, Tuple{F, DifferentiationInterface.SecondDerivativePrep, ADTypes.AbstractADType, Any, Vararg{Context, C}}} where {F, C}","page":"API","title":"DifferentiationInterface.prepare!_second_derivative","text":"prepare!_second_derivative(f, prep, backend, x, [contexts...]) -> new_prep\n\nSame behavior as prepare_second_derivative but can resize the contents of an existing prep object to avoid some allocations.\n\nThere is no guarantee that prep will be mutated, or that performance will be improved compared to preparation from scratch.\n\ndanger: Danger\nCompared to when prep was first created, the only authorized modification is a size change for input x or output y. Any other modification (like a change of type for the input) is not supported and will give erroneous results.\n\ndanger: Danger\nFor efficiency, this function needs to rely on backend package internals, therefore it not protected by semantic versioning.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pullback_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pullback_performance","text":"pullback_performance(backend)\n\nReturn PullbackFast or PullbackSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pushforward_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pushforward_performance","text":"pushforward_performance(backend)\n\nReturn PushforwardFast or PushforwardSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.reasonable_batchsize-Tuple{Integer, Integer}","page":"API","title":"DifferentiationInterface.reasonable_batchsize","text":"reasonable_batchsize(N::Integer, Bmax::Integer)\n\nReproduces the heuristic from ForwardDiff to minimize\n\nthe number of batches necessary to cover an array of length N\nthe number of leftover indices in the last partial batch\n\nSource: https://github.com/JuliaDiff/ForwardDiff.jl/blob/ec74fbc32b10bbf60b3c527d8961666310733728/src/prelude.jl#L19-L29\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.recursive_similar-Union{Tuple{T}, Tuple{AbstractArray, Type{T}}} where T","page":"API","title":"DifferentiationInterface.recursive_similar","text":"recursive_similar(x, T)\n\nApply similar(_, T) recursively to x or its components.\n\nWorks if x is an AbstractArray or a (nested) NTuple / NamedTuple of AbstractArrays.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.reverse_backend-Tuple{MixedMode}","page":"API","title":"DifferentiationInterface.reverse_backend","text":"reverse_backend(m::MixedMode)\n\nReturn the reverse-mode part of a MixedMode backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.threshold_batchsize","page":"API","title":"DifferentiationInterface.threshold_batchsize","text":"threshold_batchsize(backend::AbstractADType, B::Integer)\n\nIf the backend object has a fixed batch size B0, return a new backend where the fixed batch size is min(B0, B). Otherwise, act as the identity.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"faq/limitations/#Limitations","page":"Limitations","title":"Limitations","text":"","category":"section"},{"location":"faq/limitations/#Multiple-active-arguments","page":"Limitations","title":"Multiple active arguments","text":"","category":"section"},{"location":"faq/limitations/","page":"Limitations","title":"Limitations","text":"At the moment, most backends cannot work with multiple active (differentiated) arguments. As a result, DifferentiationInterface only supports a single active argument, called x in the documentation.","category":"page"},{"location":"faq/limitations/#Complex-numbers","page":"Limitations","title":"Complex numbers","text":"","category":"section"},{"location":"faq/limitations/","page":"Limitations","title":"Limitations","text":"Complex derivatives are only handled by a few AD backends, sometimes using different conventions. To find the easiest common ground, DifferentiationInterface assumes that whenever complex numbers are involved, the function to differentiate is holomorphic. This functionality is still considered experimental and not yet part of the public API guarantees. If you work with non-holomorphic functions, you will need to manually separate real and imaginary parts.","category":"page"},{"location":"faq/limitations/","page":"Limitations","title":"Limitations","text":"","category":"page"},{"location":"tutorials/advanced/#Advanced-tutorial","page":"Advanced tutorial","title":"Advanced tutorial","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"We present contexts and sparsity handling with DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"using ADTypes\nusing BenchmarkTools\nusing DifferentiationInterface\nusing ForwardDiff: ForwardDiff\nusing Zygote: Zygote\nusing Random\nusing SparseConnectivityTracer\nusing SparseMatrixColorings","category":"page"},{"location":"tutorials/advanced/#Contexts","page":"Advanced tutorial","title":"Contexts","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Assume you want differentiate a multi-argument function with respect to the first argument.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"f_multiarg(x, c) = c * sum(abs2, x)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"The first way, which works with every backend, is to create a closure:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"f_singlearg(c) = x -> f_multiarg(x, c)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Let's see it in action:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"backend = AutoForwardDiff()\nx = float.(1:3)\n\ngradient(f_singlearg(10), backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"However, for performance reasons, it is sometimes preferrable to avoid closures and pass all arguments to the original function. We can do this by wrapping c into a Constant and giving this constant to the gradient operator.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"gradient(f_multiarg, backend, x, Constant(10))","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Preparation also works in this case, even if the constant changes before execution:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"prep_other_constant = prepare_gradient(f_multiarg, backend, x, Constant(-1))\ngradient(f_multiarg, prep_other_constant, backend, x, Constant(10))","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"For additional arguments which act as mutated buffers, the Cache wrapper is the appropriate choice instead of Constant.","category":"page"},{"location":"tutorials/advanced/#Sparsity","page":"Advanced tutorial","title":"Sparsity","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"tip: Tip\nIf you use DifferentiationInterface's Sparse AD functionality in your research, please cite our preprint Sparser, Better, Faster, Stronger: Efficient Automatic Differentiation for Sparse Jacobians and Hessians.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Sparse AD is very useful when Jacobian or Hessian matrices have a lot of zeros. So let us write functions that satisfy this property.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"f_sparse_vector(x::AbstractVector) = diff(x .^ 2) + diff(reverse(x .^ 2))\nf_sparse_scalar(x::AbstractVector) = sum(f_sparse_vector(x) .^ 2)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/#Dense-backends","page":"Advanced tutorial","title":"Dense backends","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"When we use the jacobian or hessian operator with a dense backend, we get a dense matrix with plenty of zeros.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"x = float.(1:8);","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"dense_forward_backend = AutoForwardDiff()\nJ_dense = jacobian(f_sparse_vector, dense_forward_backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"dense_second_order_backend = SecondOrder(AutoForwardDiff(), AutoZygote())\nH_dense = hessian(f_sparse_scalar, dense_second_order_backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"The results are correct but the procedure is very slow. By using a sparse backend, we can get the runtime to increase with the number of nonzero elements, instead of the total number of elements.","category":"page"},{"location":"tutorials/advanced/#Sparse-backends","page":"Advanced tutorial","title":"Sparse backends","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Recipe to create a sparse backend: combine a dense backend, a sparsity detector and a compatible coloring algorithm inside AutoSparse. The following are reasonable defaults:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"sparse_forward_backend = AutoSparse(\n    dense_forward_backend;  # any object from ADTypes\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\n\nsparse_second_order_backend = AutoSparse(\n    dense_second_order_backend;  # any object from ADTypes or a SecondOrder from DI\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Now the resulting matrices are sparse:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jacobian(f_sparse_vector, sparse_forward_backend, x)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"hessian(f_sparse_scalar, sparse_second_order_backend, x)","category":"page"},{"location":"tutorials/advanced/#Sparse-preparation","page":"Advanced tutorial","title":"Sparse preparation","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In the examples above, we didn't use preparation. Sparse preparation is more costly than dense preparation, but it is even more essential. Indeed, once preparation is done, sparse differentiation is much faster than dense differentiation, because it makes fewer calls to the underlying function.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Some result analysis functions from SparseMatrixColorings.jl can help you figure out what the preparation contains. First, it records the sparsity pattern itself (the one returned by the detector).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep = prepare_jacobian(f_sparse_vector, sparse_forward_backend, x)\nsparsity_pattern(jac_prep)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In forward mode, each column of the sparsity pattern gets a color.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"column_colors(jac_prep)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"And the colors in turn define non-overlapping groups (for Jacobians at least, Hessians are a bit more complicated).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"column_groups(jac_prep)","category":"page"},{"location":"tutorials/advanced/#Sparsity-speedup","page":"Advanced tutorial","title":"Sparsity speedup","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"When preparation is used, the speedup due to sparsity becomes very visible in large dimensions.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"xbig = rand(1000)\nnothing  # hide","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep_dense = prepare_jacobian(f_sparse_vector, dense_forward_backend, zero(xbig))\n@benchmark jacobian($f_sparse_vector, $jac_prep_dense, $dense_forward_backend, $xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep_sparse = prepare_jacobian(f_sparse_vector, sparse_forward_backend, zero(xbig))\n@benchmark jacobian($f_sparse_vector, $jac_prep_sparse, $sparse_forward_backend, $xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Better memory use can be achieved by pre-allocating the matrix from the preparation result (so that it has the correct structure).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_buffer = similar(sparsity_pattern(jac_prep_sparse), eltype(xbig))\n@benchmark jacobian!(\n    $f_sparse_vector, $jac_buffer, $jac_prep_sparse, $sparse_forward_backend, $xbig\n)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"And for optimal speed, one should write non-allocating and type-stable functions.","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"function f_sparse_vector!(y::AbstractVector, x::AbstractVector)\n    n = length(x)\n    for i in eachindex(y)\n        y[i] = abs2(x[i + 1]) - abs2(x[i]) + abs2(x[n - i]) - abs2(x[n - i + 1])\n    end\n    return nothing\nend\n\nybig = zeros(length(xbig) - 1)\nf_sparse_vector!(ybig, xbig)\nybig ≈ f_sparse_vector(xbig)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In this case, the sparse Jacobian should also become non-allocating (for our specific choice of backend).","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"jac_prep_sparse_nonallocating = prepare_jacobian(\n    f_sparse_vector!, zero(ybig), sparse_forward_backend, zero(xbig)\n)\njac_buffer = similar(sparsity_pattern(jac_prep_sparse_nonallocating), eltype(xbig))\n@benchmark jacobian!(\n    $f_sparse_vector!,\n    $ybig,\n    $jac_buffer,\n    $jac_prep_sparse_nonallocating,\n    $sparse_forward_backend,\n    $xbig,\n)","category":"page"},{"location":"tutorials/advanced/#Mixed-mode","page":"Advanced tutorial","title":"Mixed mode","text":"","category":"section"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"Some Jacobians have a structure which includes dense rows and dense columns, like this one:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"arrowhead(x) = x .+ x[1] .+ vcat(sum(x), zeros(eltype(x), length(x)-1))\n\njacobian_sparsity(arrowhead, x, TracerSparsityDetector())","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"In such cases, sparse AD is only beneficial in \"mixed mode\", where we combine a forward and a reverse backend. This is achieved using the MixedMode wrapper, for which we recommend a random coloring order (see RandomOrder):","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"sparse_mixed_backend = AutoSparse(\n    MixedMode(AutoForwardDiff(), AutoZygote());\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(RandomOrder(MersenneTwister(), 0)),\n)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"It unlocks a large speedup compared to pure forward mode, and the same would be true compared to reverse mode:","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"@benchmark jacobian($arrowhead, prep, $sparse_forward_backend, $xbig) setup=(\n    prep=prepare_jacobian(arrowhead, sparse_forward_backend, xbig)\n)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"@benchmark jacobian($arrowhead, prep, $sparse_mixed_backend, $xbig) setup=(\n    prep=prepare_jacobian(arrowhead, sparse_mixed_backend, xbig)\n)","category":"page"},{"location":"tutorials/advanced/","page":"Advanced tutorial","title":"Advanced tutorial","text":"","category":"page"},{"location":"faq/differentiability/#Differentiability","page":"Differentiability","title":"Differentiability","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"DifferentiationInterface.jl and its sibling package DifferentiationInterfaceTest.jl allow you to try out differentiation of existing code with a variety of AD backends. However, they will not help you write differentiable code in the first place. To make your functions compatible with several backends, you need to mind the restrictions imposed by each one.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"The list of backends available at juliadiff.org is split into 2 main families: operator overloading and source transformation. Writing differentiable code requires a specific approach in each paradigm:","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"For operator overloading, ensure type-genericity.\nFor source transformation, rely on existing rules or write your own.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"tip: Tip\nDepending on your intended use case, you may not need to ensure compatibility with every single backend. In particular, some applications strongly suggest a specific \"mode\" of AD (forward or reverse), in which case backends limited to the other mode are mostly irrelevant.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"In what follows, we do not discuss AD with finite differences (FiniteDiff.jl and FiniteDifferences.jl) because those packages will work as long as your function itself can run, which is obviously a prerequisite.","category":"page"},{"location":"faq/differentiability/#Operator-overloading","page":"Differentiability","title":"Operator overloading","text":"","category":"section"},{"location":"faq/differentiability/#ForwardDiff","page":"Differentiability","title":"ForwardDiff","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"One of the most common backends in the ecosystem is ForwardDiff.jl. It performs AD at a scalar level by replacing plain numbers with Dual numbers, which carry derivative information. As explained in the limitations of ForwardDiff, this will only work if the differentiated code does not restrict number types too much. Otherwise, you may encounter errors like this one:","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"MethodError: no method matching Float64(::ForwardDiff.Dual{...})","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"To prevent them, here are a few things to look out for:","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Avoid functions with overly specific type annotations.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"f(x::Vector{Float64}) = x # bad\nf(x::AbstractVector{<:Real}) = x # good","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"When creating new containers or buffers, adapt to the input number type if necessary.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"tmp = zeros(length(x))  # bad\ntmp = zeros(eltype(x), length(x))  # good\ntmp = similar(x)  # best when possible","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"In some situations, manually writing overloads for x::Dual or x::AbstractArray{<:Dual} can be necessary.","category":"page"},{"location":"faq/differentiability/#ReverseDiff","page":"Differentiability","title":"ReverseDiff","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"ReverseDiff.jl relies on operator overloading for scalars, but also for arrays. The relevant types are called TrackedReal and TrackedArray, they have a set of limitations very similar to that of ForwardDiff.jl's Dual and will cause similar errors.","category":"page"},{"location":"faq/differentiability/#Symbolic-backends","page":"Differentiability","title":"Symbolic backends","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Symbolics.jl and FastDifferentiation.jl are also based on operator overloading. However, their respective number types are a bit different because they represent symbolic variables instead of numerical values. The operator overloading aims at reconstructing a symbolic representation of the function (an equation, more or less), which means certain language constructs will not be tolerated even though ForwardDiff.jl or ReverseDiff.jl could handle them.","category":"page"},{"location":"faq/differentiability/#Source-transformation","page":"Differentiability","title":"Source transformation","text":"","category":"section"},{"location":"faq/differentiability/#Zygote","page":"Differentiability","title":"Zygote","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Zygote.jl can differentiate a lot of Julia code, but it does have some major limitations. The most frequently encountered is the lack of support for mutation: if you try to modify the contents of an array during differentiation, you will get an error like","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"ERROR: Mutating arrays is not supported","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Mutations and some other language constructs (exceptions, foreign calls) will make a function incompatible with Zygote. In such cases, the proper workaround is to define a reverse rule (rrule) for that function using ChainRulesCore.jl. You can find a pedagogical example for rule-writing in the documentation of ChainRulesCore.jl.","category":"page"},{"location":"faq/differentiability/#Enzyme","page":"Differentiability","title":"Enzyme","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"By targeting a lower-level code representation than Zygote.jl, Enzyme.jl is able to differentiate a much wider set of functions. The FAQ gives some details on the breadth of coverage, but it should be enough for a lot of use cases.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Enzyme.jl also has an extensible rule system which you can use to circumvent differentiation errors. Note that its rule writing is very different from ChainRulesCore.jl due to the presence of input activity annotations.","category":"page"},{"location":"faq/differentiability/#Mooncake","page":"Differentiability","title":"Mooncake","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Mooncake.jl is a recent package which also handles a large subset of all Julia programs out-of-the-box.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Its rule system is less expressive than that of Enzyme.jl, which might make it easier to start with.","category":"page"},{"location":"faq/differentiability/#A-rule-mayhem?","page":"Differentiability","title":"A rule mayhem?","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"To summarize, here are the main rule systems which coexist at the moment:","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Dual numbers in ForwardDiff.jl\nChainRulesCore.jl\nEnzyme.jl\nMooncake.jl","category":"page"},{"location":"faq/differentiability/#Rule-translation","page":"Differentiability","title":"Rule translation","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"This split situation is unfortunate, but AD packages are so complex that making a cross-backend rule system is a very ambitious endeavor. ChainRulesCore.jl is the closest thing we have to a standard, but it does not handle mutation. As a result, Enzyme.jl and Mooncake.jl both rolled out their own designs, which are not mutually compatible. There are, however, translation utilities:","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"from ChainRulesCore.jl to ForwardDiff.jl with ForwardDiffChainRules.jl\nfrom ChainRulesCore.jl to Enzyme.jl with Enzyme.@import_rrule\nfrom ChainRulesCore.jl to Mooncake.jl with Mooncake.@from_rrule","category":"page"},{"location":"faq/differentiability/#Backend-switch","page":"Differentiability","title":"Backend switch","text":"","category":"section"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Also note the existence of DifferentiationInterface.DifferentiateWith, which allows the user to wrap a function that should be differentiated with a specific backend.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"Right now, it only targets ForwardDiff.jl, Mooncake.jl, ChainRules.jl-compatible backends (e.g., Zygote.jl), but PRs are welcome to define Enzyme.jl rules for this object.","category":"page"},{"location":"faq/differentiability/","page":"Differentiability","title":"Differentiability","text":"","category":"page"},{"location":"explanation/operators/#Operators","page":"Operators","title":"Operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"tip: Tip\nIf there are some concepts you do not understand, take a look at the book The Elements of Differentiable Programming (Blondel and Roulet, 2024).","category":"page"},{"location":"explanation/operators/#List-of-operators","page":"Operators","title":"List of operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Given a function f(x) = y, there are several differentiation operators available. The terminology depends on:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"the type and shape of the input x\nthe type and shape of the output y\nthe order of differentiation","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Below we list and describe all the operators we support.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe package is thoroughly tested with inputs and outputs of the following types: Float64, Vector{Float64} and Matrix{Float64}. We also expect it to work on most kinds of Number and AbstractArray variables. Beyond that, you are in uncharted territory. We voluntarily keep the type annotations minimal, so that passing more complex objects or custom structs might work in some cases, but we make no guarantees about that yet.","category":"page"},{"location":"explanation/operators/#High-level-operators","page":"Operators","title":"High-level operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"These operators are computed using only the input x.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"operator order input x output y operator result type operator result shape\nderivative 1 Number Any similar to y size(y)\nsecond_derivative 2 Number Any similar to y size(y)\ngradient 1 Any Number similar to x size(x)\njacobian 1 AbstractArray AbstractArray AbstractMatrix (length(y), length(x))\nhessian 2 AbstractArray Number AbstractMatrix (length(x), length(x))","category":"page"},{"location":"explanation/operators/#Low-level-operators","page":"Operators","title":"Low-level operators","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"These operators are computed using the input x and another argument t of type NTuple, which contains one or more tangents. You can think of tangents as perturbations propagated through the function; they live either in the same space as x or in the same space as y.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"operator order input x output y element type of t operator result type operator result shape\npushforward (JVP) 1 Any Any similar to x similar to y size(y)\npullback (VJP) 1 Any Any similar to y similar to x size(x)\nhvp 2 Any Number similar to x similar to x size(x)","category":"page"},{"location":"explanation/operators/#Variants","page":"Operators","title":"Variants","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Several variants of each operator are defined:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"out-of-place operators return a new derivative object\nin-place operators mutate the provided derivative object","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"out-of-place in-place out-of-place + primal in-place + primal\nderivative derivative! value_and_derivative value_and_derivative!\nsecond_derivative second_derivative! value_derivative_and_second_derivative value_derivative_and_second_derivative!\ngradient gradient! value_and_gradient value_and_gradient!\nhessian hessian! value_gradient_and_hessian value_gradient_and_hessian!\njacobian jacobian! value_and_jacobian value_and_jacobian!\npushforward pushforward! value_and_pushforward value_and_pushforward!\npullback pullback! value_and_pullback value_and_pullback!\nhvp hvp! gradient_and_hvp gradient_and_hvp!","category":"page"},{"location":"explanation/operators/#Mutation-and-signatures","page":"Operators","title":"Mutation and signatures","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Two kinds of functions are supported:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"out-of-place functions f(x) = y\nin-place functions f!(y, x) = nothing","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nIn-place functions only work with pushforward, pullback, derivative and jacobian. The other operators hvp, gradient and hessian require scalar outputs, so it makes no sense to mutate the number y.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"This results in various operator signatures (the necessary arguments and their order):","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"function signature out-of-place operator (returns result) in-place  operator (mutates result)\nout-of-place function f op(f, backend, x, [t]) op!(f, result, backend, x, [t])\nin-place function f! op(f!, y, backend, x, [t]) op!(f!, y, result, backend, x, [t])","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe positional arguments between f/f! and backend are always mutated, regardless of the bang ! in the operator name. In particular, for in-place functions f!(y, x), every variant of every operator will mutate y.","category":"page"},{"location":"explanation/operators/#Preparation","page":"Operators","title":"Preparation","text":"","category":"section"},{"location":"explanation/operators/#Principle","page":"Operators","title":"Principle","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"In many cases, AD can be accelerated if the function has been called at least once (e.g. to record a tape) or if some cache objects are pre-allocated. This preparation procedure is backend-specific, but we expose a common syntax to achieve it.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"operator preparation (different point) preparation (same point)\nderivative prepare_derivative -\ngradient prepare_gradient -\njacobian prepare_jacobian -\nsecond_derivative prepare_second_derivative -\nhessian prepare_hessian -\npushforward prepare_pushforward prepare_pushforward_same_point\npullback prepare_pullback prepare_pullback_same_point\nhvp prepare_hvp prepare_hvp_same_point","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"In addition, the preparation syntax depends on the number of arguments accepted by the function.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"function signature preparation signature\nout-of-place function prepare_op(f, backend, x, [t])\nin-place function prepare_op(f!, y, backend, x, [t])","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"Preparation creates an object called prep which contains the the necessary information to speed up an operator and its variants. The idea is that you prepare only once, which can be costly, but then call the operator several times while reusing the same prep.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"op(f, backend, x, [t])  # slow because it includes preparation\nop(f, prep, backend, x, [t])  # fast because it skips preparation","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe prep object is the last argument before backend and it is always mutated, regardless of the bang ! in the operator name. As a consequence, preparation is not thread-safe and  sharing prep objects between threads may lead to unexpected behavior. If you need to run differentiation concurrently, prepare separate prep objects for each thread.","category":"page"},{"location":"explanation/operators/#Reusing-preparation","page":"Operators","title":"Reusing preparation","text":"","category":"section"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"It is not always safe to reuse the results of preparation. For different-point preparation, the output prep of","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"prepare_op(f, [y], backend, x, [t, contexts...])","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"can be reused in subsequent calls to","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"op(f, prep, [other_y], backend, other_x, [other_t, other_contexts...])","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"provided that the following conditions all hold:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"f and backend remain the same\nother_x has the same type and size as x\nother_y has the same type and size as y\nother_t has the same type and size as t\nall the elements of other_contexts have the same type and size as the corresponding elements of contexts","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"For same-point preparation, the same rules hold with two modifications:","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"other_x must be equal to x\nany element of other_contexts with type Constant must be equal to the corresponding element of contexts","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"danger: Danger\nReusing preparation with different types or sizes may work with some backends and error with others, so it is not allowed by the API of DifferentiationInterface.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThese rules hold for the majority of backends, but there are some exceptions. The most important exception is ReverseDiff.jl and its taping mechanism, which is sensitive to control flow inside the function.","category":"page"},{"location":"explanation/operators/","page":"Operators","title":"Operators","text":"","category":"page"},{"location":"dev_guide/#Dev-guide","page":"Dev guide","title":"Dev guide","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"This page is important reading if you want to contribute to DifferentiationInterface.jl. It is not part of the public API and the content below may become outdated, in which case you should refer to the source code as the ground truth.","category":"page"},{"location":"dev_guide/#General-principles","page":"Dev guide","title":"General principles","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"The package is structured around 8 operators:","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"derivative\nsecond_derivative\ngradient\njacobian\nhessian\npushforward\npullback\nhvp","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Most operators have 4 variants, which look like this in the first order: operator, operator!, value_and_operator, value_and_operator!.","category":"page"},{"location":"dev_guide/#New-operator","page":"Dev guide","title":"New operator","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"To implement a new operator for an existing backend, you need to write 5 methods: 1 for preparation and 4 corresponding to the variants of the operator (see above). For first-order operators, you may also want to support in-place functions, which requires another 5 methods (defined on f! instead of f).","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"The method prepare_operator_nokwarg must output a prep object of the correct type. For instance, prepare_gradient(strict, f, backend, x) must return a DifferentiationInterface.GradientPrep. Assuming you don't need any preparation for said operator, you can use the trivial prep that are already defined, like DifferentiationInterface.NoGradientPrep{SIG}. Otherwise, define a custom struct like MyGradientPrep{SIG} <: DifferentiationInterface.GradientPrep{SIG} and put the necessary storage in there.","category":"page"},{"location":"dev_guide/#New-backend","page":"Dev guide","title":"New backend","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"If you want to implement a new backend, for instance because you developed a new AD package called SuperDiff, please open a pull request to DifferentiationInterface.jl. Your AD package needs to be registered first.","category":"page"},{"location":"dev_guide/#Core-code","page":"Dev guide","title":"Core code","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"In the main package, you should define a new struct SuperDiffBackend which subtypes ADTypes.AbstractADType, and endow it with the fields you need to parametrize your differentiation routines. You also have to define ADTypes.mode and DifferentiationInterface.inplace_support on SuperDiffBackend.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"info: Info\nIn the end, this backend struct will need to be contributed to ADTypes.jl. However, putting it in the DifferentiationInterface.jl PR is a good first step for debugging.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"In a package extension named DifferentiationInterfaceSuperDiffExt, you need to implement at least pushforward or pullback (and their variants). The exact requirements depend on the differentiation mode you chose:","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"backend mode pushforward necessary pullback necessary\nADTypes.ForwardMode yes no\nADTypes.ReverseMode no yes\nADTypes.ForwardOrReverseMode yes yes\nADTypes.SymbolicMode yes yes","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Every other operator can be deduced from these two, but you can gain efficiency by implementing additional operators.","category":"page"},{"location":"dev_guide/#Tests-and-docs","page":"Dev guide","title":"Tests and docs","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Once that is done, you need to add your new backend to the test suite. Test files should be gathered in a folder named SuperDiff inside DifferentiationInterface/test/Back. They should use DifferentiationInterfaceTest.jl to check correctness against the default scenarios. Take inspiration from the tests of other backends to write your own. To activate tests in CI, modify the test workflow and add your package to the list. To run the tests locally, replace the following line in DifferentiationInterface/test/runtests.jl","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"GROUP = get(ENV, \"JULIA_DI_TEST_GROUP\", \"All\")","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"with the much cheaper version","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"GROUP = get(ENV, \"JULIA_DI_TEST_GROUP\", \"Back/SuperDiff\")","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"but don't forget to switch it back before pushing.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Finally, you need to add your backend to the documentation, modifying every page that involves a list of backends (including the README.md).","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"","category":"page"},{"location":"explanation/advanced/#Advanced-features","page":"Advanced features","title":"Advanced features","text":"","category":"section"},{"location":"explanation/advanced/#Contexts","page":"Advanced features","title":"Contexts","text":"","category":"section"},{"location":"explanation/advanced/#Additional-arguments","page":"Advanced features","title":"Additional arguments","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"For all operators provided DifferentiationInterface, there can be only one differentiated (or \"active\") argument, which we call x. However, the release v0.6 introduced the possibility of additional \"context\" arguments, which are not differentiated but still passed to the function after x.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"Contexts can be useful if you have a function y = f(x, a, b, c, ...) or f!(y, x, a, b, c, ...) and you want derivatives of y with respect to x only. Another option would be creating a closure, but that is sometimes undesirable.","category":"page"},{"location":"explanation/advanced/#Types-of-contexts","page":"Advanced features","title":"Types of contexts","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"Every context argument must be wrapped in a subtype of Context and come after the differentiated input x. Right now, there are two kinds of context: Constant and Cache.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"warning: Warning\nNot every backend supports every type of context. See the documentation on Backends for more details.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"Semantically, both of these calls compute the partial gradient of f(x, c) with respect to x, but they consider c differently:","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"gradient(f, backend, x, Constant(c))\ngradient(f, backend, x, Cache(c))","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"In the first call, c is kept unchanged throughout the function evaluation. In the second call, c can be mutated with values computed during the function.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"Importantly, one can prepare an operator with an arbitrary value c' of the Constant (subject to the usual restrictions on preparation). The values in a provided Cache never matter anyway.","category":"page"},{"location":"explanation/advanced/#Sparsity","page":"Advanced features","title":"Sparsity","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"When faced with sparse Jacobian or Hessian matrices, one can take advantage of their sparsity pattern to speed up the computation. DifferentiationInterface does this automatically if you pass a backend of type AutoSparse.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"tip: Tip\nTo know more about sparse AD, read the survey What Color Is Your Jacobian? Graph Coloring for Computing Derivatives (Gebremedhin et al., 2005).","category":"page"},{"location":"explanation/advanced/#AutoSparse-object","page":"Advanced features","title":"AutoSparse object","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"AutoSparse backends only support jacobian and hessian (as well as their variants), because other operators do not output matrices. An AutoSparse backend must be constructed from three ingredients:","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"An underlying (dense) backend, which can be SecondOrder or anything from ADTypes.jl\nA sparsity pattern detector like:\nTracerSparsityDetector from SparseConnectivityTracer.jl\nSymbolicsSparsityDetector from Symbolics.jl\nDenseSparsityDetector from DifferentiationInterface.jl (beware that this detector only gives a locally valid pattern)\nKnownJacobianSparsityDetector or KnownHessianSparsityDetector from ADTypes.jl (if you already know the pattern)\nA coloring algorithm from SparseMatrixColorings.jl, such as:\nGreedyColoringAlgorithm (our generic recommendation)\nConstantColoringAlgorithm (if you have already computed the optimal coloring and always want to return it)","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"note: Note\nSymbolic backends have built-in sparsity handling, so AutoSparse(AutoSymbolics()) and AutoSparse(AutoFastDifferentiation()) do not need additional configuration for pattern detection or coloring.","category":"page"},{"location":"explanation/advanced/#Cost-of-sparse-preparation","page":"Advanced features","title":"Cost of sparse preparation","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"The preparation step of jacobian or hessian with an AutoSparse backend can be long, because it needs to detect the sparsity pattern and perform a matrix coloring. But after preparation, the more zeros are present in the matrix, the greater the speedup will be compared to dense differentiation.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"danger: Danger\nThe result of preparation for an AutoSparse backend cannot be reused if the sparsity pattern changes.","category":"page"},{"location":"explanation/advanced/#Tuning-the-coloring-algorithm","page":"Advanced features","title":"Tuning the coloring algorithm","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"The complexity of sparse Jacobians or Hessians grows with the number of distinct colors in a coloring of the sparsity pattern. To reduce this number of colors, GreedyColoringAlgorithm has two main settings: the order used for vertices and the decompression method. Depending on your use case, you may want to modify either of these options to increase performance. See the documentation of SparseMatrixColorings.jl for details.","category":"page"},{"location":"explanation/advanced/#Mixed-mode","page":"Advanced features","title":"Mixed mode","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"When a Jacobian matrix has both dense rows and dense columns, it can be more efficient to use \"mixed-mode\" differentiation, a mixture of forward and reverse. The associated bidirectional coloring algorithm automatically decides how to cover the Jacobian using a set of columns (computed in forward mode) plus a set of rows (computed in reverse mode). This behavior is triggered as soon as you put a MixedMode object inside AutoSparse, like so:","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"AutoSparse(\n    MixedMode(forward_backend, reverse_backend); sparsity_detector, coloring_algorithm\n)","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"At the moment, mixed mode tends to work best (output fewer colors) when the GreedyColoringAlgorithm is provided with a RandomOrder instead of the usual NaturalOrder, and when \"post-processing\" is activated after coloring. For full reproducibility, you should use a random number generator from StableRNGs.jl. Thus, the right setup looks like:","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"using StableRNGs\n\nseed = 3\ncoloring_algorithm = GreedyColoringAlgorithm(\n    RandomOrder(StableRNG(seed), seed); postprocessing=true\n)","category":"page"},{"location":"explanation/advanced/#Batch-mode","page":"Advanced features","title":"Batch mode","text":"","category":"section"},{"location":"explanation/advanced/#Multiple-tangents","page":"Advanced features","title":"Multiple tangents","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"The jacobian and hessian operators compute matrices by repeatedly applying lower-level operators (pushforward, pullback or hvp) to a set of tangents. The tangents usually correspond to basis elements of the appropriate vector space. We could call the lower-level operator on each tangent separately, but some packages (ForwardDiff.jl and Enzyme.jl) have optimized implementations to handle multiple tangents at once.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"This behavior is often called \"vector mode\" AD, but we call it \"batch mode\" to avoid confusion with Julia's Vector type. As a matter of fact, the optimal batch size B (number of simultaneous tangents) is usually very small, so tangents are passed within an NTuple and not a Vector. When the underlying vector space has dimension N, the operators jacobian and hessian process lceil N  B rceil batches of size B each.","category":"page"},{"location":"explanation/advanced/#Optimal-batch-size","page":"Advanced features","title":"Optimal batch size","text":"","category":"section"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"For every backend which does not support batch mode, the batch size is set to B = 1. But for AutoForwardDiff and AutoEnzyme, more complicated rules apply. If the backend object has a pre-determined batch size B_0, then we always set B = B_0. In particular, this will throw errors when N  B_0. On the other hand, without a pre-determined batch size, we apply backend-specific heuristics to pick B based on N.","category":"page"},{"location":"explanation/advanced/","page":"Advanced features","title":"Advanced features","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: DifferentiationInterface Logo)","category":"page"},{"location":"#DifferentiationInterface","page":"Home","title":"DifferentiationInterface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages) (Image: DOI)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Package Docs\nDifferentiationInterface (Image: Stable)     (Image: Dev)\nDifferentiationInterfaceTest (Image: Stable) (Image: Dev)","category":"page"},{"location":"","page":"Home","title":"Home","text":"An interface to various automatic differentiation (AD) backends in Julia.","category":"page"},{"location":"#Goal","page":"Home","title":"Goal","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a unified syntax to differentiate functions, including:","category":"page"},{"location":"","page":"Home","title":"Home","text":"First- and second-order operators (gradients, Jacobians, Hessians and more)\nIn-place and out-of-place differentiation\nPreparation mechanism (e.g. to pre-allocate a cache or record a tape)\nBuilt-in sparsity handling\nThorough validation on standard inputs and outputs (numbers, vectors, matrices)\nTesting and benchmarking utilities accessible to users with DifferentiationInterfaceTest","category":"page"},{"location":"#Compatibility","page":"Home","title":"Compatibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We support the following backends defined by ADTypes.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ChainRulesCore.jl\nDiffractor.jl (currently broken)\nEnzyme.jl (see below)\nFastDifferentiation.jl\nFiniteDiff.jl\nFiniteDifferences.jl\nForwardDiff.jl\nGTPSA.jl\nMooncake.jl\nPolyesterForwardDiff.jl\nReverseDiff.jl\nSymbolics.jl\nTracker.jl\nZygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"warning: Warning\nNote that in some cases, going through DifferentiationInterface.jl might be slower or cause more errors than a direct call to the backend's API. This is especially true for Enzyme.jl, whose handling of activities and multiple arguments is not fully supported here. We are working on this challenge, and welcome any suggestions or contributions. Meanwhile, if differentiation fails or takes too long, consider using Enzyme.jl through its native API instead.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the stable version of the package, run the following code in a Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\"DifferentiationInterface\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install the development version, run this instead:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(;\n    url=\"https://github.com/JuliaDiff/DifferentiationInterface.jl\",\n    subdir=\"DifferentiationInterface\",\n)","category":"page"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using DifferentiationInterface\nusing ForwardDiff: ForwardDiff\nusing Enzyme: Enzyme\nusing Zygote: Zygote  # AD backends you want to use\n\nf(x) = sum(abs2, x)\n\nx = [1.0, 2.0]\n\nvalue_and_gradient(f, AutoForwardDiff(), x) # returns (5.0, [2.0, 4.0]) with ForwardDiff.jl\nvalue_and_gradient(f, AutoEnzyme(), x) # returns (5.0, [2.0, 4.0]) with Enzyme.jl\nvalue_and_gradient(f, AutoZygote(), x) # returns (5.0, [2.0, 4.0]) with Zygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"To improve your performance by up to several orders of magnitude compared to this example, take a look at the tutorial and its section on operator preparation.","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Whenever you refer to this package or the ideas it contains, please cite:","category":"page"},{"location":"","page":"Home","title":"Home","text":"our preprint A Common Interface for Automatic Differentiation;\nour inspiration AbstractDifferentiation.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can use the provided CITATION.cff file or the following BibTeX entries:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@misc{dalle2025commoninterfaceautomaticdifferentiation,\n      title={A Common Interface for Automatic Differentiation},\n      author={Guillaume Dalle and Adrian Hill},\n      year={2025},\n      eprint={2505.05542},\n      archivePrefix={arXiv},\n      primaryClass={cs.MS},\n      url={https://arxiv.org/abs/2505.05542},\n}\n\n@misc{schäfer2022abstractdifferentiationjlbackendagnosticdifferentiableprogramming,\n      title={AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia},\n      author={Frank Schäfer and Mohamed Tarek and Lyndon White and Chris Rackauckas},\n      year={2022},\n      eprint={2109.12449},\n      archivePrefix={arXiv},\n      primaryClass={cs.MS},\n      url={https://arxiv.org/abs/2109.12449},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you use the software, additionally cite us using the precise Zenodo DOI of the package version you used, or the BibTeX entry below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@software{dalleDifferentiationInterface2025,\n      author={Dalle, Guillaume and Hill, Adrian},\n      title={Differentiation{I}nterface.jl},\n      year={2024},\n      publisher={Zenodo},\n      doi={10.5281/zenodo.11092033},\n      url={https://doi.org/10.5281/zenodo.11092033},\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"explanation/backends/#Backends","page":"Backends","title":"Backends","text":"","category":"section"},{"location":"explanation/backends/#List","page":"Backends","title":"List","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We support the following dense backend choices from ADTypes.jl:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"AutoChainRules\nAutoDiffractor\nAutoEnzyme\nAutoFastDifferentiation\nAutoFiniteDiff\nAutoFiniteDifferences\nAutoForwardDiff\nAutoGTPSA\nAutoMooncake and AutoMooncakeForward (the latter is experimental)\nAutoPolyesterForwardDiff\nAutoReverseDiff\nAutoSymbolics\nAutoTracker\nAutoZygote","category":"page"},{"location":"explanation/backends/#Features","page":"Backends","title":"Features","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Given a backend object, you can use:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"check_available to know whether the required AD package is loaded\ncheck_inplace to know whether the backend supports in-place functions (all backends support out-of-place functions)","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"In theory, all we need from each backend is either a pushforward or a pullback: we can deduce every other operator from these two. In practice, many AD backends have custom implementations for high-level operators like gradient or jacobian, which we reuse whenever possible.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"details: Details\nIn the rough summary table below,✅ means that we reuse the custom implementation from the backend;\n❌ means that a custom implementation doesn't exist, so we use our default fallbacks;\n🔀 means it's complicated or not done yet. pf pb der grad jac hess hvp der2\nAutoChainRules ❌ ✅ ❌ ❌ ❌ ❌ ❌ ❌\nAutoDiffractor ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌\nAutoEnzyme (forward) ✅ ❌ ❌ ✅ ✅ ❌ ❌ ❌\nAutoEnzyme (reverse) ❌ ✅ ❌ ✅ ✅ ❌ 🔀 ❌\nAutoFastDifferentiation ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅\nAutoFiniteDiff 🔀 ❌ ✅ ✅ ✅ ✅ ❌ ❌\nAutoFiniteDifferences 🔀 ❌ ❌ ✅ ✅ ❌ ❌ ❌\nAutoForwardDiff ✅ ❌ ✅ ✅ ✅ ✅ ✅ ✅\nAutoGTPSA ✅ ❌ ❌ ✅ ✅ ✅ ✅ ✅\nAutoMooncake ❌ ✅ ❌ ❌ ❌ ❌ ❌ ❌\nAutoMooncakeForward ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌\nAutoPolyesterForwardDiff 🔀 ❌ 🔀 ✅ ✅ 🔀 🔀 🔀\nAutoReverseDiff ❌ 🔀 ❌ ✅ ✅ ✅ ❌ ❌\nAutoSymbolics ✅ ❌ ✅ ✅ ✅ ✅ ✅ ✅\nAutoTracker ❌ ✅ ❌ ✅ ❌ ❌ ❌ ❌\nAutoZygote ❌ ✅ ❌ ✅ ✅ ✅ 🔀 ❌","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Moreover, each context type is supported by a specific subset of backends:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":" Constant Cache\nAutoChainRules ✅ ❌\nAutoDiffractor ❌ ❌\nAutoEnzyme (forward) ✅ ✅\nAutoEnzyme (reverse) ✅ ❌ (soon)\nAutoFastDifferentiation ✅ ✅\nAutoFiniteDiff ✅ ✅\nAutoFiniteDifferences ✅ ✅\nAutoForwardDiff ✅ ✅\nAutoGTPSA ✅ ❌\nAutoMooncake ✅ ✅\nAutoMooncakeForward ✅ ✅\nAutoPolyesterForwardDiff ✅ ✅\nAutoReverseDiff ✅ ❌\nAutoSymbolics ✅ ✅\nAutoTracker ✅ ❌\nAutoZygote ✅ 🔀","category":"page"},{"location":"explanation/backends/#Second-order","page":"Backends","title":"Second order","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For second-order operators like second_derivative, hessian and hvp, there are two main options. You can either use a single backend, or combine two of them within the SecondOrder struct:","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"backend = SecondOrder(outer_backend, inner_backend)","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"The inner backend will be called first, and the outer backend will differentiate the generated code. In general, using a forward outer backend over a reverse inner backend will yield the best performance.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nSecond-order AD is tricky, and many backend combinations will fail (even if you combine a backend with itself). Be ready to experiment and open issues if necessary.","category":"page"},{"location":"explanation/backends/#Backend-switch","page":"Backends","title":"Backend switch","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"The wrapper DifferentiateWith allows you to switch between backends. It takes a function f and specifies that f should be differentiated with the substitute backend of your choice, instead of whatever true backend the surrounding code is trying to use. In other words, when someone tries to differentiate dw = DifferentiateWith(f, substitute_backend) with true_backend, then substitute_backend steps in and true_backend does not dive into the function f itself. At the moment, DifferentiateWith only works when true_backend is either ForwardDiff.jl, reverse-mode Mooncake.jl, or a ChainRules.jl-compatible backend (e.g., Zygote.jl).","category":"page"},{"location":"explanation/backends/#Implementations","page":"Backends","title":"Implementations","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"What follows is a list of implementation details from the package extensions of DifferentiationInterface.jl It is not part of the public API or protected by semantic versioning, and it may become outdated. When in doubt, refer to the code itself.","category":"page"},{"location":"explanation/backends/#ChainRulesCore","page":"Backends","title":"ChainRulesCore","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We only implement pullback, using the RuleConfig mechanism to call back into AD. Same-point preparation runs the forward sweep and returns the pullback closure.","category":"page"},{"location":"explanation/backends/#Diffractor","page":"Backends","title":"Diffractor","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We only implement pushforward.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nThe latest releases of Diffractor broke DifferentiationInterface.","category":"page"},{"location":"explanation/backends/#Enzyme","page":"Backends","title":"Enzyme","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Depending on the mode attribute inside AutoEnzyme, we implement either pushforward or pullback based on Enzyme.autodiff. When necessary, preparation chooses a number of chunks (for gradient and jacobian in forward mode, for jacobian only in reverse mode).","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"warning: Warning\nEnzyme.jl's handling of activities and multiple arguments is not fully supported here, which can cause slowdowns or errors. If differentiation fails or takes too long, consider using Enzyme.jl through its native API instead.","category":"page"},{"location":"explanation/backends/#FastDifferentiation","page":"Backends","title":"FastDifferentiation","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For every operator, preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"warning: Warning\nPreparation can be very slow for symbolic AD.","category":"page"},{"location":"explanation/backends/#FiniteDiff","page":"Backends","title":"FiniteDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Whenever possible, preparation creates a cache object. Pushforward is implemented rather slowly using a closure.","category":"page"},{"location":"explanation/backends/#FiniteDifferences","page":"Backends","title":"FiniteDifferences","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Nothing specific to mention.","category":"page"},{"location":"explanation/backends/#ForwardDiff","page":"Backends","title":"ForwardDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We implement pushforward directly using Dual numbers, and preparation allocates the necessary space. For higher level operators, preparation creates a config object, which can be type-unstable.","category":"page"},{"location":"explanation/backends/#GTPSA","page":"Backends","title":"GTPSA","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For all operators, preparation preallocates the input TPSs, and for in-place functions the output TPSs as well. For minimal allocations of TPS temporaries inside of a function, the @FastGTPSA/@FastGTPSA! macros are recommended.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"If a GTPSA Descriptor is not provided to AutoGTPSA, then a Descriptor will be generated in preparation based on the context.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nWhen providing a custom GTPSA Descriptor to AutoGTPSA, it is the responsibility of the user to ensure that the number of GTPSA \"variables\" specified in the Descriptor is consistent with the number of inputs of the provided function. Undefined behavior and crashes may occur if this is not the case.","category":"page"},{"location":"explanation/backends/#PolyesterForwardDiff","page":"Backends","title":"PolyesterForwardDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Most operators fall back on AutoForwardDiff.","category":"page"},{"location":"explanation/backends/#ReverseDiff","page":"Backends","title":"ReverseDiff","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"With AutoReverseDiff(compile=false), preparation preallocates a config.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"With AutoReverseDiff(compile=true), preparation records a tape of the function's execution. This tape is computed from the input x provided at preparation time. It is control-flow dependent, so only one branch is recorded at each if statement.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"danger: Danger\nIf your function has value-specific control flow (like if x[1] > 0 or if c == 1), you may get silently wrong results whenever it takes new branches that were not taken during preparation. You must make sure to run preparation with an input and contexts whose values trigger the correct control flow for future executions.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"Whenever contexts are provided, tape recording is deactivated in all cases, because otherwise the context values would be hardcoded into a tape.","category":"page"},{"location":"explanation/backends/#Symbolics","page":"Backends","title":"Symbolics","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For all operators, preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"warning: Warning\nPreparation can be very slow for symbolic AD.","category":"page"},{"location":"explanation/backends/#Mooncake","page":"Backends","title":"Mooncake","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"For pullback, preparation builds the reverse rule of the function.","category":"page"},{"location":"explanation/backends/#Tracker","page":"Backends","title":"Tracker","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We implement pullback based on Tracker.back. Same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"explanation/backends/#Zygote","page":"Backends","title":"Zygote","text":"","category":"section"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"We implement pullback based on Zygote.pullback. Same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"explanation/backends/","page":"Backends","title":"Backends","text":"","category":"page"},{"location":"tutorials/basic/#Basic-tutorial","page":"Basic tutorial","title":"Basic tutorial","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"We present the main features of DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"using DifferentiationInterface","category":"page"},{"location":"tutorials/basic/#Computing-a-gradient","page":"Basic tutorial","title":"Computing a gradient","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"A common use case of automatic differentiation (AD) is optimizing real-valued functions with first- or second-order methods. Let's define a simple objective (the squared norm) and a random input vector","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"f(x) = sum(abs2, x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"x = collect(1.0:5.0)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"To compute its gradient, we need to choose a \"backend\", i.e. an AD package to call under the hood. Most backend types are defined by ADTypes.jl and re-exported by DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"ForwardDiff.jl is very generic and efficient for low-dimensional inputs, so it's a good starting point:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"using ForwardDiff: ForwardDiff\n\nbackend = AutoForwardDiff()","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"tip: Tip\nTo avoid name conflicts, load AD packages with import instead of using. Indeed, most AD packages also export operators like gradient and jacobian, but you only want to use the ones from DifferentiationInterface.jl.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Now you can use the following syntax to compute the gradient:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"gradient(f, backend, x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Was that fast? BenchmarkTools.jl helps you answer that question.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"using BenchmarkTools\n\n@benchmark gradient($f, $backend, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Not bad, but you can do better.","category":"page"},{"location":"tutorials/basic/#Overwriting-a-gradient","page":"Basic tutorial","title":"Overwriting a gradient","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Since you know how much space your gradient will occupy (the same as your input x), you can pre-allocate that memory and offer it to AD. Some backends get a speed boost from this trick.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"grad = similar(x)\ngradient!(f, grad, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"The bang indicates that one of the arguments of gradient! might be mutated. More precisely, our convention is that every positional argument between the function and the backend is mutated.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"@benchmark gradient!($f, $grad, $backend, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"For some reason the in-place version is not much better than your first attempt. However, it makes fewer allocations, thanks to the gradient vector you provided. Don't worry, you can get even more performance.","category":"page"},{"location":"tutorials/basic/#Preparing-for-multiple-gradients","page":"Basic tutorial","title":"Preparing for multiple gradients","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Internally, ForwardDiff.jl creates some data structures to keep track of things. These objects can be reused between gradient computations, even on different input values. We abstract away the preparation step behind a backend-agnostic syntax:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"prep = prepare_gradient(f, backend, zero(x))","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"You don't need to know what this object is, you just need to pass it to the gradient operator. Note that preparation does not depend on the actual components of the vector x, just on its type and size. You can thus reuse the prep for different values of the input.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"grad = similar(x)\ngradient!(f, grad, prep, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Preparation makes the gradient computation much faster, and (in this case) allocation-free.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"@benchmark gradient!($f, $grad, $prep, $backend, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Beware that the prep object is nearly always mutated by differentiation operators, even though it is given as the last positional argument.","category":"page"},{"location":"tutorials/basic/#Switching-backends","page":"Basic tutorial","title":"Switching backends","text":"","category":"section"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"The whole point of DifferentiationInterface.jl is that you can easily experiment with different AD solutions. Typically, for gradients, reverse mode AD might be a better fit, so let's try Zygote.jl!","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"using Zygote: Zygote\n\nbackend2 = AutoZygote()","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"Once the backend is created, things run smoothly with exactly the same syntax as before:","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"gradient(f, backend2, x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"And you can run the same benchmarks to see what you gained (although such a small input may not be realistic):","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"prep2 = prepare_gradient(f, backend2, zero(x))\n\n@benchmark gradient!($f, $grad, $prep2, $backend2, $x)","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"In short, DifferentiationInterface.jl allows for easy testing and comparison of AD backends. If you want to go further, check out the documentation of DifferentiationInterfaceTest.jl. This related package provides benchmarking utilities to compare backends and help you select the one that is best suited for your problem.","category":"page"},{"location":"tutorials/basic/","page":"Basic tutorial","title":"Basic tutorial","text":"","category":"page"}]
}
