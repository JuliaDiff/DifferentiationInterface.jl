var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/","page":"API","title":"API","text":"DifferentiationInterface","category":"page"},{"location":"api/#DifferentiationInterface","page":"API","title":"DifferentiationInterface","text":"DifferentiationInterface\n\nAn interface to various automatic differentiation backends in Julia.\n\n\n\n\n\n","category":"module"},{"location":"api/#First-order","page":"API","title":"First order","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Tangents","category":"page"},{"location":"api/#DifferentiationInterface.Tangents","page":"API","title":"DifferentiationInterface.Tangents","text":"Tangents{B}\n\nStorage for a batch of B tangents (NTuple wrapper).\n\nMust be passed as an argument to pushforward, pullback and hvp, in addition to the input x.\n\nConstructors\n\nTangents(d)\nTangents(d1, d2, ..., dB)\n\nExample\n\njulia> using DifferentiationInterface\n\njulia> t = Tangents([2.0])\nTangents{1, Vector{Float64}}(([2.0],))\n\njulia> length(t)\n1\n\njulia> only(t)\n1-element Vector{Float64}:\n 2.0\n\njulia> t = Tangents([2.0], [4.0], [6.0])\nTangents{3, Vector{Float64}}(([2.0], [4.0], [6.0]))\n\njulia> length(t)\n3\n\njulia> t[2]\n1-element Vector{Float64}:\n 4.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Pushforward","page":"API","title":"Pushforward","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_pushforward\nprepare_pushforward_same_point\npushforward\npushforward!\nvalue_and_pushforward\nvalue_and_pushforward!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_pushforward","page":"API","title":"DifferentiationInterface.prepare_pushforward","text":"prepare_pushforward(f,     backend, x, tx) -> extras\nprepare_pushforward(f!, y, backend, x, tx) -> extras\n\nCreate an extras object that can be given to pushforward and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pushforward_same_point","page":"API","title":"DifferentiationInterface.prepare_pushforward_same_point","text":"prepare_pushforward_same_point(f,     backend, x, tx) -> extras_same\nprepare_pushforward_same_point(f!, y, backend, x, tx) -> extras_same\n\nCreate an extras_same object that can be given to pushforward and its variants if they are applied at the same point x.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward","page":"API","title":"DifferentiationInterface.pushforward","text":"pushforward(f,     [extras,] backend, x, tx) -> ty\npushforward(f!, y, [extras,] backend, x, tx) -> ty\n\nCompute the pushforward of the function f at point x with tangent tx of type Tangents.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named jvp.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pushforward!","page":"API","title":"DifferentiationInterface.pushforward!","text":"pushforward!(f,     dy, [extras,] backend, x, tx) -> ty\npushforward!(f!, y, dy, [extras,] backend, x, tx) -> ty\n\nCompute the pushforward of the function f at point x with tangent tx of type Tangents, overwriting ty.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named jvp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward","page":"API","title":"DifferentiationInterface.value_and_pushforward","text":"value_and_pushforward(f,     [extras,] backend, x, tx) -> (y, ty)\nvalue_and_pushforward(f!, y, [extras,] backend, x, tx) -> (y, ty)\n\nCompute the value and the pushforward of the function f at point x with tangent tx of type Tangents.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named value_and_jvp.\n\ninfo: Info\nRequired primitive for forward mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pushforward!","page":"API","title":"DifferentiationInterface.value_and_pushforward!","text":"value_and_pushforward!(f,     dy, [extras,] backend, x, tx) -> (y, ty)\nvalue_and_pushforward!(f!, y, dy, [extras,] backend, x, tx) -> (y, ty)\n\nCompute the value and the pushforward of the function f at point x with tangent tx of type Tangents, overwriting ty.\n\nTo improve performance via operator preparation, refer to prepare_pushforward and prepare_pushforward_same_point.\n\ntip: Tip\nPushforwards are also commonly called Jacobian-vector products or JVPs. This function could have been named value_and_jvp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#Pullback","page":"API","title":"Pullback","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_pullback\nprepare_pullback_same_point\npullback\npullback!\nvalue_and_pullback\nvalue_and_pullback!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_pullback","page":"API","title":"DifferentiationInterface.prepare_pullback","text":"prepare_pullback(f,     backend, x, ty) -> extras\nprepare_pullback(f!, y, backend, x, ty) -> extras\n\nCreate an extras object that can be given to pullback and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_pullback_same_point","page":"API","title":"DifferentiationInterface.prepare_pullback_same_point","text":"prepare_pullback_same_point(f,     backend, x, ty) -> extras_same\nprepare_pullback_same_point(f!, y, backend, x, ty) -> extras_same\n\nCreate an extras_same object that can be given to pullback and its variants if they are applied at the same point x.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback","page":"API","title":"DifferentiationInterface.pullback","text":"pullback(f,     [extras,] backend, x, ty) -> tx\npullback(f!, y, [extras,] backend, x, ty) -> tx\n\nCompute the pullback of the function f at point x with tangent ty of type Tangents.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named vjp.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.pullback!","page":"API","title":"DifferentiationInterface.pullback!","text":"pullback!(f,     dx, [extras,] backend, x, ty) -> tx\npullback!(f!, y, dx, [extras,] backend, x, ty) -> tx\n\nCompute the pullback of the function f at point x with tangent ty of type Tangents, overwriting dx.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named vjp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback","page":"API","title":"DifferentiationInterface.value_and_pullback","text":"value_and_pullback(f,     [extras,] backend, x, ty) -> (y, tx)\nvalue_and_pullback(f!, y, [extras,] backend, x, ty) -> (y, tx)\n\nCompute the value and the pullback of the function f at point x with tangent ty of type Tangents.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named value_and_vjp.\n\ninfo: Info\nRequired primitive for reverse mode backends.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_pullback!","page":"API","title":"DifferentiationInterface.value_and_pullback!","text":"value_and_pullback!(f,     dx, [extras,] backend, x, ty) -> (y, tx)\nvalue_and_pullback!(f!, y, dx, [extras,] backend, x, ty) -> (y, tx)\n\nCompute the value and the pullback of the function f at point x with tangent ty of type Tangents, overwriting dx.\n\nTo improve performance via operator preparation, refer to prepare_pullback and prepare_pullback_same_point.\n\ntip: Tip\nPullbacks are also commonly called vector-Jacobian products or VJPs. This function could have been named value_and_vjp!.\n\n\n\n\n\n","category":"function"},{"location":"api/#Derivative","page":"API","title":"Derivative","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_derivative\nderivative\nderivative!\nvalue_and_derivative\nvalue_and_derivative!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_derivative","page":"API","title":"DifferentiationInterface.prepare_derivative","text":"prepare_derivative(f,     backend, x) -> extras\nprepare_derivative(f!, y, backend, x) -> extras\n\nCreate an extras object that can be given to derivative and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative","page":"API","title":"DifferentiationInterface.derivative","text":"derivative(f,     [extras,] backend, x) -> der\nderivative(f!, y, [extras,] backend, x) -> der\n\nCompute the derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.derivative!","page":"API","title":"DifferentiationInterface.derivative!","text":"derivative!(f,     der, [extras,] backend, x) -> der\nderivative!(f!, y, der, [extras,] backend, x) -> der\n\nCompute the derivative of the function f at point x, overwriting der.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative","page":"API","title":"DifferentiationInterface.value_and_derivative","text":"value_and_derivative(f,     [extras,] backend, x) -> (y, der)\nvalue_and_derivative(f!, y, [extras,] backend, x) -> (y, der)\n\nCompute the value and the derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_derivative!","page":"API","title":"DifferentiationInterface.value_and_derivative!","text":"value_and_derivative!(f,     der, [extras,] backend, x) -> (y, der)\nvalue_and_derivative!(f!, y, der, [extras,] backend, x) -> (y, der)\n\nCompute the value and the derivative of the function f at point x, overwriting der.\n\nTo improve performance via operator preparation, refer to prepare_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#Gradient","page":"API","title":"Gradient","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_gradient\ngradient\ngradient!\nvalue_and_gradient\nvalue_and_gradient!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_gradient","page":"API","title":"DifferentiationInterface.prepare_gradient","text":"prepare_gradient(f, backend, x) -> extras\n\nCreate an extras object that can be given to gradient and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient","page":"API","title":"DifferentiationInterface.gradient","text":"gradient(f, [extras,] backend, x) -> grad\n\nCompute the gradient of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.gradient!","page":"API","title":"DifferentiationInterface.gradient!","text":"gradient!(f, grad, [extras,] backend, x) -> grad\n\nCompute the gradient of the function f at point x, overwriting grad.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient","page":"API","title":"DifferentiationInterface.value_and_gradient","text":"value_and_gradient(f, [extras,] backend, x) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_gradient!","page":"API","title":"DifferentiationInterface.value_and_gradient!","text":"value_and_gradient!(f, grad, [extras,] backend, x) -> (y, grad)\n\nCompute the value and the gradient of the function f at point x, overwriting grad.\n\nTo improve performance via operator preparation, refer to prepare_gradient.\n\n\n\n\n\n","category":"function"},{"location":"api/#Jacobian","page":"API","title":"Jacobian","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_jacobian\njacobian\njacobian!\nvalue_and_jacobian\nvalue_and_jacobian!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_jacobian","page":"API","title":"DifferentiationInterface.prepare_jacobian","text":"prepare_jacobian(f,     backend, x) -> extras\nprepare_jacobian(f!, y, backend, x) -> extras\n\nCreate an extras object that can be given to jacobian and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again. For in-place functions, y is mutated by f! during preparation.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian","page":"API","title":"DifferentiationInterface.jacobian","text":"jacobian(f,     [extras,] backend, x) -> jac\njacobian(f!, y, [extras,] backend, x) -> jac\n\nCompute the Jacobian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.jacobian!","page":"API","title":"DifferentiationInterface.jacobian!","text":"jacobian!(f,     jac, [extras,] backend, x) -> jac\njacobian!(f!, y, jac, [extras,] backend, x) -> jac\n\nCompute the Jacobian matrix of the function f at point x, overwriting jac.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian","page":"API","title":"DifferentiationInterface.value_and_jacobian","text":"value_and_jacobian(f,     [extras,] backend, x) -> (y, jac)\nvalue_and_jacobian(f!, y, [extras,] backend, x) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_and_jacobian!","page":"API","title":"DifferentiationInterface.value_and_jacobian!","text":"value_and_jacobian!(f,     jac, [extras,] backend, x) -> (y, jac)\nvalue_and_jacobian!(f!, y, jac, [extras,] backend, x) -> (y, jac)\n\nCompute the value and the Jacobian matrix of the function f at point x, overwriting jac.\n\nTo improve performance via operator preparation, refer to prepare_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/#Second-order","page":"API","title":"Second order","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SecondOrder","category":"page"},{"location":"api/#DifferentiationInterface.SecondOrder","page":"API","title":"DifferentiationInterface.SecondOrder","text":"SecondOrder\n\nCombination of two backends for second-order differentiation.\n\ndanger: Danger\nSecondOrder backends do not support first-order operators.\n\nConstructor\n\nSecondOrder(outer_backend, inner_backend)\n\nFields\n\nouter::AbstractADType: backend for the outer differentiation\ninner::AbstractADType: backend for the inner differentiation\n\n\n\n\n\n","category":"type"},{"location":"api/#Second-derivative","page":"API","title":"Second derivative","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_second_derivative\nsecond_derivative\nsecond_derivative!\nvalue_derivative_and_second_derivative\nvalue_derivative_and_second_derivative!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_second_derivative","page":"API","title":"DifferentiationInterface.prepare_second_derivative","text":"prepare_second_derivative(f, backend, x) -> extras\n\nCreate an extras object that can be given to second_derivative and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative","page":"API","title":"DifferentiationInterface.second_derivative","text":"second_derivative(f, [extras,] backend, x) -> der2\n\nCompute the second derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.second_derivative!","page":"API","title":"DifferentiationInterface.second_derivative!","text":"second_derivative!(f, der2, [extras,] backend, x) -> der2\n\nCompute the second derivative of the function f at point x, overwriting der2.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_derivative_and_second_derivative","page":"API","title":"DifferentiationInterface.value_derivative_and_second_derivative","text":"value_derivative_and_second_derivative(f, [extras,] backend, x) -> (y, der, der2)\n\nCompute the value, first derivative and second derivative of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_derivative_and_second_derivative!","page":"API","title":"DifferentiationInterface.value_derivative_and_second_derivative!","text":"value_derivative_and_second_derivative!(f, der, der2, [extras,] backend, x) -> (y, der, der2)\n\nCompute the value, first derivative and second derivative of the function f at point x, overwriting der and der2.\n\nTo improve performance via operator preparation, refer to prepare_second_derivative.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian-vector-product","page":"API","title":"Hessian-vector product","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_hvp\nprepare_hvp_same_point\nhvp\nhvp!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_hvp","page":"API","title":"DifferentiationInterface.prepare_hvp","text":"prepare_hvp(f, backend, x, tx) -> extras\n\nCreate an extras object that can be given to hvp and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.prepare_hvp_same_point","page":"API","title":"DifferentiationInterface.prepare_hvp_same_point","text":"prepare_hvp_same_point(f, backend, x, tx) -> extras_same\n\nCreate an extras_same object that can be given to hvp and its variants if they are applied at the same point x.\n\nwarning: Warning\nIf the function or the point changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp","page":"API","title":"DifferentiationInterface.hvp","text":"hvp(f, [extras,] backend, x, tx) -> tg\n\nCompute the Hessian-vector product of f at point x with tangent tx of type Tangents.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hvp!","page":"API","title":"DifferentiationInterface.hvp!","text":"hvp!(f, dg, [extras,] backend, x, tx) -> tg\n\nCompute the Hessian-vector product of f at point x with tangent tx of type Tangents, overwriting tg.\n\nTo improve performance via operator preparation, refer to prepare_hvp and prepare_hvp_same_point.\n\n\n\n\n\n","category":"function"},{"location":"api/#Hessian","page":"API","title":"Hessian","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"prepare_hessian\nhessian\nhessian!\nvalue_gradient_and_hessian\nvalue_gradient_and_hessian!","category":"page"},{"location":"api/#DifferentiationInterface.prepare_hessian","page":"API","title":"DifferentiationInterface.prepare_hessian","text":"prepare_hessian(f, backend, x) -> extras\n\nCreate an extras object that can be given to hessian and its variants.\n\nwarning: Warning\nIf the function changes in any way, the result of preparation will be invalidated, and you will need to run it again.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian","page":"API","title":"DifferentiationInterface.hessian","text":"hessian(f, backend, x, [extras]) -> hess\n\nCompute the Hessian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.hessian!","page":"API","title":"DifferentiationInterface.hessian!","text":"hessian!(f, hess, backend, x, [extras]) -> hess\n\nCompute the Hessian matrix of the function f at point x, overwriting hess.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_gradient_and_hessian","page":"API","title":"DifferentiationInterface.value_gradient_and_hessian","text":"value_gradient_and_hessian(f, [extras,] backend, x) -> (y, grad, hess)\n\nCompute the value, gradient vector and Hessian matrix of the function f at point x.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.value_gradient_and_hessian!","page":"API","title":"DifferentiationInterface.value_gradient_and_hessian!","text":"value_gradient_and_hessian!(f, grad, hess, [extras,] backend, x) -> (y, grad, hess)\n\nCompute the value, gradient vector and Hessian matrix of the function f at point x, overwriting grad and hess.\n\nTo improve performance via operator preparation, refer to prepare_hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#Backend-queries","page":"API","title":"Backend queries","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"check_available\ncheck_inplace\nDifferentiationInterface.outer\nDifferentiationInterface.inner","category":"page"},{"location":"api/#DifferentiationInterface.check_available","page":"API","title":"DifferentiationInterface.check_available","text":"check_available(backend)\n\nCheck whether backend is available (i.e. whether the extension is loaded).\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.check_inplace","page":"API","title":"DifferentiationInterface.check_inplace","text":"check_inplace(backend)\n\nCheck whether backend supports differentiation of in-place functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.outer","page":"API","title":"DifferentiationInterface.outer","text":"outer(backend::SecondOrder)\n\nReturn the outer backend of a SecondOrder object, tasked with differentiation at the second order.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiationInterface.inner","page":"API","title":"DifferentiationInterface.inner","text":"inner(backend::SecondOrder)\n\nReturn the inner backend of a SecondOrder object, tasked with differentiation at the first order.\n\n\n\n\n\n","category":"function"},{"location":"api/#Backend-switch","page":"API","title":"Backend switch","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DifferentiateWith","category":"page"},{"location":"api/#DifferentiationInterface.DifferentiateWith","page":"API","title":"DifferentiationInterface.DifferentiateWith","text":"DifferentiateWith\n\nCallable function wrapper that enforces differentiation with a specified (inner) backend.\n\nThis works by defining new rules overriding the behavior of the outer backend that would normally be used.\n\nwarning: Warning\nThis is an experimental functionality, whose API cannot yet be considered stable. It only supports out-of-place functions, and rules are only defined for ChainRules.jl-compatible outer backends.\n\nFields\n\nf: the function in question\nbackend::AbstractADType: the inner backend to use for differentiation\n\nConstructor\n\nDifferentiateWith(f, backend)\n\nExample\n\nusing DifferentiationInterface\nimport ForwardDiff, Zygote\n\nfunction f(x)\n    a = Vector{eltype(x)}(undef, 1)\n    a[1] = sum(x)  # mutation that breaks Zygote\n    return a[1]\nend\n\ndw = DifferentiateWith(f, AutoForwardDiff());\n\ngradient(dw, AutoZygote(), [2.0])  # calls ForwardDiff instead\n\n# output\n\n1-element Vector{Float64}:\n 1.0\n\n\n\n\n\n","category":"type"},{"location":"api/#Sparsity-detection","page":"API","title":"Sparsity detection","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DenseSparsityDetector","category":"page"},{"location":"api/#DifferentiationInterface.DenseSparsityDetector","page":"API","title":"DifferentiationInterface.DenseSparsityDetector","text":"DenseSparsityDetector\n\nSparsity pattern detector satisfying the detection API of ADTypes.jl.\n\nThe nonzeros in a Jacobian or Hessian are detected by computing the relevant matrix with dense AD, and thresholding the entries with a given tolerance (which can be numerically inaccurate). This process can be very slow, and should only be used if its output can be exploited multiple times to compute many sparse matrices.\n\ndanger: Danger\nIn general, the sparsity pattern you obtain can depend on the provided input x. If you want to reuse the pattern, make sure that it is input-agnostic.\n\nwarning: Warning\nDenseSparsityDetector functionality is now located in a package extension, please load the SparseArrays.jl standard library before you use it.\n\nFields\n\nbackend::AbstractADType is the dense AD backend used under the hood\natol::Float64 is the minimum magnitude of a matrix entry to be considered nonzero\n\nConstructor\n\nDenseSparsityDetector(backend; atol, method=:iterative)\n\nThe keyword argument method::Symbol can be either:\n\n:iterative: compute the matrix in a sequence of matrix-vector products (memory-efficient)\n:direct: compute the matrix all at once (memory-hungry but sometimes faster).\n\nNote that the constructor is type-unstable because method ends up being a type parameter of the DenseSparsityDetector object (this is not part of the API and might change).\n\nExamples\n\nusing ADTypes, DifferentiationInterface, SparseArrays\nimport ForwardDiff\n\ndetector = DenseSparsityDetector(AutoForwardDiff(); atol=1e-5, method=:direct)\n\nADTypes.jacobian_sparsity(diff, rand(5), detector)\n\n# output\n\n4×5 SparseMatrixCSC{Bool, Int64} with 8 stored entries:\n 1  1  ⋅  ⋅  ⋅\n ⋅  1  1  ⋅  ⋅\n ⋅  ⋅  1  1  ⋅\n ⋅  ⋅  ⋅  1  1\n\nSometimes the sparsity pattern is input-dependent:\n\nADTypes.jacobian_sparsity(x -> [prod(x)], rand(2), detector)\n\n# output\n\n1×2 SparseMatrixCSC{Bool, Int64} with 2 stored entries:\n 1  1\n\nADTypes.jacobian_sparsity(x -> [prod(x)], [0, 1], detector)\n\n# output\n\n1×2 SparseMatrixCSC{Bool, Int64} with 1 stored entry:\n 1  ⋅\n\n\n\n\n\n","category":"type"},{"location":"api/#Internals","page":"API","title":"Internals","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The following is not part of the public API.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Modules = [DifferentiationInterface]\nPublic = false\nFilter = t -> !(Symbol(t) in [:outer, :inner])","category":"page"},{"location":"api/#DifferentiationInterface.AutoZeroForward","page":"API","title":"DifferentiationInterface.AutoZeroForward","text":"AutoZeroForward <: ADTypes.AbstractADType\n\nTrivial backend that sets all derivatives to zero. Used in testing and benchmarking.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.AutoZeroReverse","page":"API","title":"DifferentiationInterface.AutoZeroReverse","text":"AutoZeroReverse <: ADTypes.AbstractADType\n\nTrivial backend that sets all derivatives to zero. Used in testing and benchmarking.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.DerivativeExtras","page":"API","title":"DifferentiationInterface.DerivativeExtras","text":"DerivativeExtras\n\nAbstract type for additional information needed by derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverForward","page":"API","title":"DifferentiationInterface.ForwardOverForward","text":"ForwardOverForward\n\nTraits identifying second-order backends that compute HVPs in forward over forward mode (inefficient).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ForwardOverReverse","page":"API","title":"DifferentiationInterface.ForwardOverReverse","text":"ForwardOverReverse\n\nTraits identifying second-order backends that compute HVPs in forward over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.Gradient","page":"API","title":"DifferentiationInterface.Gradient","text":"Gradient\n\nFunctor computing the gradient of f with a fixed backend.\n\nwarning: Warning\nThis type is not part of the public API.\n\nConstructor\n\nGradient(f, backend, extras=nothing)\n\nIf extras is provided, the gradient closure will skip preparation.\n\nExample\n\nusing DifferentiationInterface\nimport Zygote\n\ng = DifferentiationInterface.Gradient(x -> sum(abs2, x), AutoZygote())\ng([2.0, 3.0])\n\n# output\n\n2-element Vector{Float64}:\n 4.0\n 6.0\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.GradientExtras","page":"API","title":"DifferentiationInterface.GradientExtras","text":"GradientExtras\n\nAbstract type for additional information needed by gradient and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HVPExtras","page":"API","title":"DifferentiationInterface.HVPExtras","text":"HVPExtras\n\nAbstract type for additional information needed by hvp and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.HessianExtras","page":"API","title":"DifferentiationInterface.HessianExtras","text":"HessianExtras\n\nAbstract type for additional information needed by hessian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.InPlaceNotSupported","page":"API","title":"DifferentiationInterface.InPlaceNotSupported","text":"InPlaceNotSupported\n\nTrait identifying backends that do not support in-place functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.InPlaceSupported","page":"API","title":"DifferentiationInterface.InPlaceSupported","text":"InPlaceSupported\n\nTrait identifying backends that support in-place functions f!(y, x).\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.JacobianExtras","page":"API","title":"DifferentiationInterface.JacobianExtras","text":"JacobianExtras\n\nAbstract type for additional information needed by jacobian and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackExtras","page":"API","title":"DifferentiationInterface.PullbackExtras","text":"PullbackExtras\n\nAbstract type for additional information needed by pullback and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackFast","page":"API","title":"DifferentiationInterface.PullbackFast","text":"PullbackFast\n\nTrait identifying backends that support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PullbackSlow","page":"API","title":"DifferentiationInterface.PullbackSlow","text":"PullbackSlow\n\nTrait identifying backends that do not support efficient pullbacks.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardExtras","page":"API","title":"DifferentiationInterface.PushforwardExtras","text":"PushforwardExtras\n\nAbstract type for additional information needed by pushforward and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardFast","page":"API","title":"DifferentiationInterface.PushforwardFast","text":"PushforwardFast\n\nTrait identifying backends that support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.PushforwardSlow","page":"API","title":"DifferentiationInterface.PushforwardSlow","text":"PushforwardSlow\n\nTrait identifying backends that do not support efficient pushforwards.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverForward","page":"API","title":"DifferentiationInterface.ReverseOverForward","text":"ReverseOverForward\n\nTraits identifying second-order backends that compute HVPs in reverse over forward mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.ReverseOverReverse","page":"API","title":"DifferentiationInterface.ReverseOverReverse","text":"ReverseOverReverse\n\nTraits identifying second-order backends that compute HVPs in reverse over reverse mode.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiationInterface.SecondDerivativeExtras","page":"API","title":"DifferentiationInterface.SecondDerivativeExtras","text":"SecondDerivativeExtras\n\nAbstract type for additional information needed by second_derivative and its variants.\n\n\n\n\n\n","category":"type"},{"location":"api/#ADTypes.mode-Tuple{SecondOrder}","page":"API","title":"ADTypes.mode","text":"mode(backend::SecondOrder)\n\nReturn the outer mode of the second-order backend.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.basis-Tuple{ADTypes.AbstractADType, AbstractArray, Any}","page":"API","title":"DifferentiationInterface.basis","text":"basis(backend, a::AbstractArray, i)\n\nConstruct the i-th standard basis array in the vector space of a with element type eltype(a).\n\nNote\n\nIf an AD backend benefits from a more specialized basis array implementation, this function can be extended on the backend type.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.inplace_support-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.inplace_support","text":"inplace_support(backend)\n\nReturn InPlaceSupported or InPlaceNotSupported in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.multibasis-Tuple{ADTypes.AbstractADType, AbstractArray, Any}","page":"API","title":"DifferentiationInterface.multibasis","text":"multibasis(backend, a::AbstractArray, inds::AbstractVector)\n\nConstruct the sum of the i-th standard basis arrays in the vector space of a with element type eltype(a), for all i ∈ inds.\n\nNote\n\nIf an AD backend benefits from a more specialized basis array implementation, this function can be extended on the backend type.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.nested-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.nested","text":"nested(backend)\n\nReturn a possibly modified backend that can work while nested inside another differentiation procedure.\n\nAt the moment, this is only useful for Enzyme, which needs autodiff_deferred to be compatible with higher-order differentiation.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pick_batchsize-Tuple{ADTypes.AbstractADType, Integer}","page":"API","title":"DifferentiationInterface.pick_batchsize","text":"pick_batchsize(backend::AbstractADType, dimension::Integer)\n\nPick a reasonable batch size for batched derivative evaluation with a given total dimension.\n\nReturns 1 for backends which have not overloaded it.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pullback_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pullback_performance","text":"pullback_performance(backend)\n\nReturn PullbackFast or PullbackSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiationInterface.pushforward_performance-Tuple{ADTypes.AbstractADType}","page":"API","title":"DifferentiationInterface.pushforward_performance","text":"pushforward_performance(backend)\n\nReturn PushforwardFast or PushforwardSlow in a statically predictable way.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"implementations/#Implementations","page":"Implementations","title":"Implementations","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"DifferentiationInterface.jl provides a handful of operators like gradient or jacobian, each with several variants: out-of-place or in-place behavior, with or without primal output value.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"While it is possible to define every operator using just pushforward and pullback, some backends have more efficient implementations of high-level operators. When they are available, we nearly always call these backend-specific overloads. We also adapt the preparation phase accordingly. This page gives details on each backend's bindings.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"The tables below summarize all implemented overloads for each backend. The cells can have three values:","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"❌: the operator is not overloaded because the backend does not support it\n✅: the operator is overloaded\nNA: the operator does not exist","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"tip: Tip\nCheck marks (✅) are clickable and link to the source code.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"using ADTypes: AbstractADType\nusing DifferentiationInterface\nusing DifferentiationInterface: inplace_support, InPlaceSupported\nusing Markdown: Markdown\n\nusing Diffractor: Diffractor\nusing Enzyme: Enzyme\nusing FastDifferentiation: FastDifferentiation\nusing FiniteDiff: FiniteDiff\nusing FiniteDifferences: FiniteDifferences\nusing ForwardDiff: ForwardDiff\nusing PolyesterForwardDiff: PolyesterForwardDiff\nusing ReverseDiff: ReverseDiff\nusing Symbolics: Symbolics\nusing Tapir: Tapir\nusing Tracker: Tracker\nusing Zygote: Zygote\n\nfunction operators_and_types_f(backend::T) where {T<:AbstractADType}\n    return (\n        # (op,          types_op), \n        # (op!,         types_op!), \n        # (val_and_op,  types_val_and_op),\n        # (val_and_op!, types_val_and_op!),\n        (\n            (:derivative, (Any, Any, T, Any)),\n            (:derivative!, (Any, Any, Any, T, Any)),\n            (:value_and_derivative, (Any, Any, T, Any)),\n            (:value_and_derivative!, (Any, Any, Any, T, Any)),\n        ),\n        (\n            (:gradient, (Any, Any, T, Any)),\n            (:gradient!, (Any, Any, Any, T, Any)),\n            (:value_and_gradient, (Any, Any, T, Any)),\n            (:value_and_gradient!, (Any, Any, Any, T, Any)),\n        ),\n        (\n            (:jacobian, (Any, Any, T, Any)),\n            (:jacobian!, (Any, Any, Any, T, Any)),\n            (:value_and_jacobian, (Any, Any, T, Any)),\n            (:value_and_jacobian!, (Any, Any, Any, T, Any)),\n        ),\n        (\n            (:hessian, (Any, Any, T, Any)),\n            (:hessian!, (Any, Any, Any, T, Any)),\n            (nothing, nothing),\n            (nothing, nothing),\n        ),\n        (\n            (:hvp, (Any, Any, T, Any, Any)),\n            (:hvp!, (Any, Any, Any, T, Any, Any)),\n            (nothing, nothing),\n            (nothing, nothing),\n        ),\n        (\n            (:pullback, (Any, Any, T, Any, Any)),\n            (:pullback!, (Any, Any, Any, T, Any, Any)),\n            (:value_and_pullback, (Any, Any, T, Any, Any)),\n            (:value_and_pullback!, (Any, Any, Any, T, Any, Any)),\n        ),\n        (\n            (:pushforward, (Any, Any, T, Any, Any)),\n            (:pushforward!, (Any, Any, Any, T, Any, Any)),\n            (:value_and_pushforward, (Any, Any, T, Any, Any)),\n            (:value_and_pushforward!, (Any, Any, Any, T, Any, Any)),\n        ),\n    )\nend\n\nfunction operators_and_types_f!(backend::T) where {T<:AbstractADType}\n    return (\n        (\n            (:derivative, (Any, Any, Any, T, Any)),\n            (:derivative!, (Any, Any, Any, Any, T, Any)),\n            (:value_and_derivative, (Any, Any, Any, T, Any)),\n            (:value_and_derivative!, (Any, Any, Any, Any, T, Any)),\n        ),\n        (\n            (:jacobian, (Any, Any, Any, T, Any)),\n            (:jacobian!, (Any, Any, Any, Any, T, Any)),\n            (:value_and_jacobian, (Any, Any, Any, T, Any)),\n            (:value_and_jacobian!, (Any, Any, Any, Any, T, Any)),\n        ),\n        (\n            (:pullback, (Any, Any, Any, T, Any, Any)),\n            (:pullback!, (Any, Any, Any, Any, T, Any, Any)),\n            (:value_and_pullback, (Any, Any, Any, T, Any, Any)),\n            (:value_and_pullback!, (Any, Any, Any, Any, T, Any, Any)),\n        ),\n        (\n            (:pushforward, (Any, Any, Any, T, Any, Any)),\n            (:pushforward!, (Any, Any, Any, Any, T, Any, Any)),\n            (:value_and_pushforward, (Any, Any, Any, T, Any, Any)),\n            (:value_and_pushforward!, (Any, Any, Any, Any, T, Any, Any)),\n        ),\n    )\nend\n\nfunction method_overloaded(operator::Symbol, argtypes, ext::Module)\n    f = @eval DifferentiationInterface.$operator\n    ms = methods(f, argtypes, ext)\n\n    n = length(ms)\n    n == 0 && return \"❌\"\n    n == 1 && return \"[✅]($(Base.url(only(ms))))\"\n    return \"[✅]($(Base.url(first(ms))))\" # Optional TODO: return all URLs?\nend\n\nfunction print_overload_table(io::IO, operators_and_types, ext::Module)\n    println(io, \"| Operator | `op` | `op!` | `value_and_op` | `value_and_op!` |\")\n    println(io, \"|:---------|:----:|:-----:|:--------------:|:---------------:|\")\n    for operator_variants in operators_and_types\n        opname = first(first(operator_variants))\n        print(io, \"| `$opname` |\")\n        for (op, type_signature) in operator_variants\n            if isnothing(op)\n                print(io, \"NA\")\n            else\n                print(io, method_overloaded(op, type_signature, ext))\n            end\n            print(io, '|')\n        end\n        println(io)\n    end\nend\n\nfunction print_overloads(backend, ext::Symbol)\n    io = IOBuffer()\n    ext = Base.get_extension(DifferentiationInterface, ext)\n\n    println(io, \"#### Out-of-place functions `f(x) = y`\")\n    println(io)\n    print_overload_table(io, operators_and_types_f(backend), ext)\n\n    println(io, \"#### In-place functions `f!(y, x) = nothing`\")\n    println(io)\n    if inplace_support(backend) == InPlaceSupported()\n        print_overload_table(io, operators_and_types_f!(backend), ext)\n    else\n        println(io, \"Backend doesn't support in-place functions.\")\n    end\n\n    return Markdown.parse(String(take!(io)))\nend","category":"page"},{"location":"implementations/#ChainRulesCore","page":"Implementations","title":"ChainRulesCore","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"For pullback, same-point preparation runs the forward sweep and returns the pullback closure.","category":"page"},{"location":"implementations/#Diffractor","page":"Implementations","title":"Diffractor","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoDiffractor(), :DifferentiationInterfaceDiffractorExt) # hide","category":"page"},{"location":"implementations/#Enzyme","page":"Implementations","title":"Enzyme","text":"","category":"section"},{"location":"implementations/#Forward-mode","page":"Implementations","title":"Forward mode","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"In forward mode, for gradient and jacobian, preparation chooses a number of chunks.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoEnzyme(; mode=Enzyme.Forward), :DifferentiationInterfaceEnzymeExt) # hide","category":"page"},{"location":"implementations/#Reverse-mode","page":"Implementations","title":"Reverse mode","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoEnzyme(; mode=Enzyme.Reverse), :DifferentiationInterfaceEnzymeExt) # hide","category":"page"},{"location":"implementations/#FastDifferentiation","page":"Implementations","title":"FastDifferentiation","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"Preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"warning: Warning\nPreparation can be very slow for symbolic AD.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoFastDifferentiation(), :DifferentiationInterfaceFastDifferentiationExt) # hide","category":"page"},{"location":"implementations/#FiniteDiff","page":"Implementations","title":"FiniteDiff","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"Whenever possible, preparation creates a cache object.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoFiniteDiff(), :DifferentiationInterfaceFiniteDiffExt) # hide","category":"page"},{"location":"implementations/#FiniteDifferences","page":"Implementations","title":"FiniteDifferences","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoFiniteDifferences(; fdm=FiniteDifferences.central_fdm(3, 1)), :DifferentiationInterfaceFiniteDifferencesExt) # hide","category":"page"},{"location":"implementations/#ForwardDiff","page":"Implementations","title":"ForwardDiff","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"Wherever possible, preparation creates a config. For pushforward, preparation allocates the necessary space for Dual number computations.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoForwardDiff(), :DifferentiationInterfaceForwardDiffExt) # hide","category":"page"},{"location":"implementations/#PolyesterForwardDiff","page":"Implementations","title":"PolyesterForwardDiff","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoPolyesterForwardDiff(; chunksize=1), :DifferentiationInterfacePolyesterForwardDiffExt) # hide","category":"page"},{"location":"implementations/#ReverseDiff","page":"Implementations","title":"ReverseDiff","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"Wherever possible, preparation records a tape of the function's execution.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"warning: Warning\nThis tape is specific to the control flow inside the function, and cannot be reused if the control flow is value-dependent (like if x[1] > 0).","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoReverseDiff(), :DifferentiationInterfaceReverseDiffExt) # hide","category":"page"},{"location":"implementations/#Symbolics","page":"Implementations","title":"Symbolics","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"Preparation generates an executable function from the symbolic expression of the differentiated function.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"warning: Warning\nPreparation can be very slow for symbolic AD.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoSymbolics(), :DifferentiationInterfaceSymbolicsExt) # hide","category":"page"},{"location":"implementations/#Tapir","page":"Implementations","title":"Tapir","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"For pullback, preparation builds the reverse rule of the function.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoTapir(), :DifferentiationInterfaceTapirExt) # hide","category":"page"},{"location":"implementations/#Tracker","page":"Implementations","title":"Tracker","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"For pullback, same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoTracker(), :DifferentiationInterfaceTrackerExt) # hide","category":"page"},{"location":"implementations/#Zygote","page":"Implementations","title":"Zygote","text":"","category":"section"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"For pullback, same-point preparation runs the forward sweep and returns the pullback closure at x.","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"print_overloads(AutoZygote(), :DifferentiationInterfaceZygoteExt) # hide","category":"page"},{"location":"implementations/","page":"Implementations","title":"Implementations","text":"","category":"page"},{"location":"operators/#Operators","page":"Operators","title":"Operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"DifferentiationInterface.jl is based on two concepts: operators and backends. This page is about the former, check out that page to learn about the latter.","category":"page"},{"location":"operators/#List-of-operators","page":"Operators","title":"List of operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Given a function f(x) = y, there are several differentiation operators available. The terminology depends on:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"the type and shape of the input x\nthe type and shape of the output y\nthe order of differentiation","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"Below we list and describe all the operators we support.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"tip: Tip\nRead the book The Elements of Differentiable Programming for details on these concepts.","category":"page"},{"location":"operators/#High-level-operators","page":"Operators","title":"High-level operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"These operators are computed using only the input x.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator order input  x output   y operator result type operator result shape\nderivative 1 Number Number or AbstractArray same as y size(y)\nsecond_derivative 2 Number Number or AbstractArray same as y size(y)\ngradient 1 AbstractArray Number same as x size(x)\njacobian 1 AbstractArray AbstractArray AbstractMatrix (length(y), length(x))\nhessian 2 AbstractArray Number AbstractMatrix (length(x), length(x))","category":"page"},{"location":"operators/#Low-level-operators","page":"Operators","title":"Low-level operators","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"These operators are computed using the input x and a tangent t of type Tangents. This tangent is essentially an NTuple, whose elements live either","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"in the same space as x (we call it tx)\nor in the same space as y (we call it ty)","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator order input  x output   y tangent t operator result type operator result shape\npushforward (JVP) 1 Any Any tx same as y size(y)\npullback (VJP) 1 Any Any ty same as x size(x)\nhvp 2 AbstractArray Number tx same as x size(x)","category":"page"},{"location":"operators/#Variants","page":"Operators","title":"Variants","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Several variants of each operator are defined:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"out-of-place operators return a new derivative object\nin-place operators mutate the provided derivative object","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"out-of-place in-place out-of-place + primal in-place + primal\nderivative derivative! value_and_derivative value_and_derivative!\nsecond_derivative second_derivative! value_derivative_and_second_derivative value_derivative_and_second_derivative!\ngradient gradient! value_and_gradient value_and_gradient!\nhessian hessian! value_gradient_and_hessian value_gradient_and_hessian!\njacobian jacobian! value_and_jacobian value_and_jacobian!\npushforward pushforward! value_and_pushforward value_and_pushforward!\npullback pullback! value_and_pullback value_and_pullback!\nhvp hvp! NA NA","category":"page"},{"location":"operators/#Mutation-and-signatures","page":"Operators","title":"Mutation and signatures","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Two kinds of functions are supported:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"out-of-place functions f(x) = y\nin-place functions f!(y, x) = nothing","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nIn-place functions only work with pushforward, pullback, derivative and jacobian.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"This results in various operator signatures (the necessary arguments and their order):","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"function signature out-of-place operator in-place  operator\nout-of-place function op(f, backend, x, [t]) op!(f, result, backend, x, [t])\nin-place function op(f!, y, backend, x, [t]) op!(f!, y, result, backend, x, [t])","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe positional arguments between f/f! and backend are always mutated. This convention holds regardless of the bang ! in the operator name. In particular, for in-place functions f!(y, x), every variant of every operator will mutate y.","category":"page"},{"location":"operators/#Preparation","page":"Operators","title":"Preparation","text":"","category":"section"},{"location":"operators/#Principle","page":"Operators","title":"Principle","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"In many cases, AD can be accelerated if the function has been called at least once (e.g. to record a tape) or if some cache objects are provided. This preparation procedure is backend-specific, but we expose a common syntax to achieve it.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"operator preparation (different point) preparation (same point)\nderivative prepare_derivative -\ngradient prepare_gradient -\njacobian prepare_jacobian -\nsecond_derivative prepare_second_derivative -\nhessian prepare_hessian -\npushforward prepare_pushforward prepare_pushforward_same_point\npullback prepare_pullback prepare_pullback_same_point\nhvp prepare_hvp prepare_hvp_same_point","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"In addition, the preparation syntax depends on the number of arguments accepted by the function.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"function signature preparation signature\nout-of-place function prepare_op(f, backend, x, [t])\nin-place function prepare_op(f!, y, backend, x, [t])","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"Preparation creates an object called extras which contains the the necessary information to speed up an operator and its variants. The idea is that you prepare only once, which can be costly, but then call the operator several times while reusing the same extras.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"op(f, backend, x, [t])  # slow because it includes preparation\nop(f, extras, backend, x, [t])  # fast because it skips preparation","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nThe extras object is always mutated, regardless of the bang ! in the operator name.","category":"page"},{"location":"operators/#Reusing-preparation","page":"Operators","title":"Reusing preparation","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Deciding whether it is safe to reuse the results of preparation is not easy. Here are the general rules that we strive to implement:","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":" different point same point\nthe output extras of... prepare_op(f, b, x) prepare_op_same_point(f, b, x, t)\ncan be used in... op(f, extras, b, other_x) op(f, extras, b, x, other_t)\nprovided that... other_x has same type and shape as x other_t has same type and shape as t","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"These rules hold for the majority of backends, but there are some exceptions: see this page to know more.","category":"page"},{"location":"operators/#Second-order","page":"Operators","title":"Second order","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"For second-order operators, there are two options: use a single backend or combine two of them within the SecondOrder struct.","category":"page"},{"location":"operators/#Single-backend","page":"Operators","title":"Single backend","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Some backends natively support a set of second-order operators (typically only the hessian). In that case, it can be advantageous to use the backend on its own. If the operator is not supported natively, we will fall back on SecondOrder(backend, backend) (see below).","category":"page"},{"location":"operators/#Combining-backends","page":"Operators","title":"Combining backends","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"In general, you can use SecondOrder to combine different backends.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"backend = SecondOrder(outer_backend, inner_backend)","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"The inner backend will be called first, and the outer backend will differentiate the generated code.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"There are many possible backend combinations, a lot of which will fail. Usually, the most efficient approach for Hessians is forward-over-reverse, i.e. a forward-mode outer backend and a reverse-mode inner backend.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"danger: Danger\nSecondOrder backends do not support first-order operators.","category":"page"},{"location":"operators/#Sparsity","page":"Operators","title":"Sparsity","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"When computing sparse Jacobians or Hessians (with a lot of zeros in the matrix), it is possible to take advantage of their sparsity pattern to speed things up. For this to work, three ingredients are needed (read this survey to understand why):","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"An underlying (dense) backend\nA sparsity pattern detector like:\nTracerSparsityDetector from SparseConnectivityTracer.jl\nSymbolicsSparsityDetector from Symbolics.jl\nDenseSparsityDetector from DifferentiationInterface.jl (beware that this detector only gives a locally valid pattern)\nA coloring algorithm: GreedyColoringAlgorithm from SparseMatrixColorings.jl is the only one we support.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"warning: Warning\nGeneric sparse AD is now located in a package extension which depends on SparseMatrixColorings.jl.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"These ingredients can be combined within the AutoSparse wrapper, which DifferentiationInterface.jl re-exports. AutoSparse backends only support operators jacobian and hessian (as well as their variants). Note that for sparse Hessians, you need to put the SecondOrder backend inside AutoSparse, and not the other way around.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"The preparation step of jacobian or hessian with an AutoSparse backend can be long, because it needs to detect the sparsity pattern and color the resulting sparse matrix. But after preparation, the more zeros are present in the matrix, the greater the speedup will be compared to dense differentiation.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"danger: Danger\nThe result of preparation for an AutoSparse backend cannot be reused if the sparsity pattern changes.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"info: Info\nSymbolic backends have built-in sparsity handling, so AutoSparse(AutoSymbolics()) and AutoSparse(AutoFastDifferentiation()) do not need additional configuration for pattern detection or coloring. However they still benefit from preparation.","category":"page"},{"location":"operators/#Going-further","page":"Operators","title":"Going further","text":"","category":"section"},{"location":"operators/#Non-standard-types","page":"Operators","title":"Non-standard types","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"The package is thoroughly tested with inputs and outputs of the following types: Float64, Vector{Float64} and Matrix{Float64}. We also expect it to work on most kinds of Number and AbstractArray variables. Beyond that, you are in uncharted territory. We voluntarily keep the type annotations minimal, so that passing more complex objects or custom structs might work with some backends, but we make no guarantees about that.","category":"page"},{"location":"operators/#Multiple-inputs/outputs","page":"Operators","title":"Multiple inputs/outputs","text":"","category":"section"},{"location":"operators/","page":"Operators","title":"Operators","text":"Restricting the API to one input and one output has many coding advantages, but it is not very flexible. If you need more than that, try using ComponentArrays.jl to wrap several objects inside a single ComponentVector.","category":"page"},{"location":"operators/","page":"Operators","title":"Operators","text":"","category":"page"},{"location":"dev_guide/#Dev-guide","page":"Dev guide","title":"Dev guide","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"This page is important reading if you want to contribute to DifferentiationInterface.jl. It is not part of the public API.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"warning: Warning\nThe content below may become outdated, in which case you should refer to the source code as the ground truth.","category":"page"},{"location":"dev_guide/#General-principles","page":"Dev guide","title":"General principles","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"The package is structured around 8 operators:","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"derivative\nsecond_derivative\ngradient\njacobian\nhessian\npushforward\npullback\nhvp","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Most operators have 4 variants, which look like this in the first order: operator, operator!, value_and_operator, value_and_operator!.","category":"page"},{"location":"dev_guide/#New-operator","page":"Dev guide","title":"New operator","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"To implement a new operator for an existing backend, you need to write 5 methods: 1 for preparation and 4 corresponding to the variants of the operator (see above). For first-order operators, you may also want to support in-place functions, which requires another 5 methods (defined on f! instead of f).","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"The method prepare_operator must output an extras object of the correct type. For instance, prepare_gradient(f, backend, x) must return a DifferentiationInterface.GradientExtras. Assuming you don't need any preparation for said operator, you can use the trivial extras that are already defined, like DifferentiationInterface.NoGradientExtras. Otherwise, define a custom struct like MyGradientExtras <: DifferentiationInterface.GradientExtras and put the necessary storage in there.","category":"page"},{"location":"dev_guide/#New-backend","page":"Dev guide","title":"New backend","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"If you want to implement a new backend, for instance because you developed a new AD package called SuperDiff, please open a pull request to DifferentiationInterface.jl. Your AD package needs to be registered first.","category":"page"},{"location":"dev_guide/#Core-code","page":"Dev guide","title":"Core code","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"In the main package, you should define a new struct SuperDiffBackend which subtypes ADTypes.AbstractADType, and endow it with the fields you need to parametrize your differentiation routines. You also have to define ADTypes.mode and DifferentiationInterface.inplace_support on SuperDiffBackend.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"info: Info\nIn the end, this backend struct will need to be contributed to ADTypes.jl. However, putting it in the DifferentiationInterface.jl PR is a good first step for debugging.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"In a package extension named DifferentiationInterfaceSuperDiffExt, you need to implement at least pushforward or pullback (and their variants). The exact requirements depend on the differentiation mode you chose:","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"backend mode pushforward necessary pullback necessary\nADTypes.ForwardMode yes no\nADTypes.ReverseMode no yes\nADTypes.ForwardOrReverseMode yes yes\nADTypes.SymbolicMode yes yes","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Every other operator can be deduced from these two, but you can gain efficiency by implementing additional operators.","category":"page"},{"location":"dev_guide/#Tests-and-docs","page":"Dev guide","title":"Tests and docs","text":"","category":"section"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Once that is done, you need to add your new backend to the test suite. Test files should be gathered in a folder named SuperDiff inside DifferentiationInterface/test/Back. They should use DifferentiationInterfaceTest.jl to check correctness against the default scenarios. Take inspiration from the tests of other backends to write your own. To activate tests in CI, modify the test workflow and add your package to the list. To run the tests locally, replace the following line in DifferentiationInterface/test/runtests.jl","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"GROUP = get(ENV, \"JULIA_DI_TEST_GROUP\", \"All\")","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"with the much cheaper version","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"GROUP = get(ENV, \"JULIA_DI_TEST_GROUP\", \"Back/SuperDiff\")","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"but don't forget to switch it back before pushing.","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"Finally, you need to add your backend to the documentation, modifying every page that involves a list of backends (including the README.md).","category":"page"},{"location":"dev_guide/","page":"Dev guide","title":"Dev guide","text":"","category":"page"},{"location":"tutorial1/#Basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"We present the main features of DifferentiationInterface.jl.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"using DifferentiationInterface","category":"page"},{"location":"tutorial1/#Computing-a-gradient","page":"Basics","title":"Computing a gradient","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"A common use case of automatic differentiation (AD) is optimizing real-valued functions with first- or second-order methods. Let's define a simple objective (the squared norm) and a random input vector","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"f(x) = sum(abs2, x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"x = collect(1.0:5.0)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"To compute its gradient, we need to choose a \"backend\", i.e. an AD package to call under the hood. Most backend types are defined by ADTypes.jl and re-exported by DifferentiationInterface.jl.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"ForwardDiff.jl is very generic and efficient for low-dimensional inputs, so it's a good starting point:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"import ForwardDiff\n\nbackend = AutoForwardDiff()","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"tip: Tip\nTo avoid name conflicts, load AD packages with import instead of using. Indeed, most AD packages also export operators like gradient and jacobian, but you only want to use the ones from DifferentiationInterface.jl.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Now you can use the following syntax to compute the gradient:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"gradient(f, backend, x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Was that fast? BenchmarkTools.jl helps you answer that question.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"using BenchmarkTools\n\n@benchmark gradient($f, $backend, $x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Not bad, but you can do better.","category":"page"},{"location":"tutorial1/#Overwriting-a-gradient","page":"Basics","title":"Overwriting a gradient","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Since you know how much space your gradient will occupy (the same as your input x), you can pre-allocate that memory and offer it to AD. Some backends get a speed boost from this trick.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"grad = similar(x)\ngradient!(f, grad, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"The bang indicates that one of the arguments of gradient! might be mutated. More precisely, our convention is that every positional argument between the function and the backend is mutated.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"@benchmark gradient!($f, $grad, $backend, $x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"For some reason the in-place version is not much better than your first attempt. However, it makes fewer allocations, thanks to the gradient vector you provided. Don't worry, you can get even more performance.","category":"page"},{"location":"tutorial1/#Preparing-for-multiple-gradients","page":"Basics","title":"Preparing for multiple gradients","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Internally, ForwardDiff.jl creates some data structures to keep track of things. These objects can be reused between gradient computations, even on different input values. We abstract away the preparation step behind a backend-agnostic syntax:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"extras = prepare_gradient(f, backend, zero(x))","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"You don't need to know what this object is, you just need to pass it to the gradient operator. Note that preparation does not depend on the actual components of the vector x, just on its type and size. You can thus reuse the extras for different values of the input.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"grad = similar(x)\ngradient!(f, grad, extras, backend, x)\ngrad  # has been mutated","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Preparation makes the gradient computation much faster, and (in this case) allocation-free.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"@benchmark gradient!($f, $grad, $extras, $backend, $x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Beware that the extras object is nearly always mutated by differentiation operators, even though it is given as the last positional argument.","category":"page"},{"location":"tutorial1/#Switching-backends","page":"Basics","title":"Switching backends","text":"","category":"section"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"The whole point of DifferentiationInterface.jl is that you can easily experiment with different AD solutions. Typically, for gradients, reverse mode AD might be a better fit, so let's try the state-of-the-art Enzyme.jl!","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"import Enzyme\n\nbackend2 = AutoEnzyme()","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"Once the backend is created, things run smoothly with exactly the same syntax as before:","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"gradient(f, backend2, x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"And you can run the same benchmarks to see what you gained (although such a small input may not be realistic):","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"extras2 = prepare_gradient(f, backend2, zero(x))\n\n@benchmark gradient!($f, $grad, $extras2, $backend2, $x)","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"In short, DifferentiationInterface.jl allows for easy testing and comparison of AD backends. If you want to go further, check out the documentation of DifferentiationInterfaceTest.jl. This related package provides benchmarking utilities to compare backends and help you select the one that is best suited for your problem.","category":"page"},{"location":"tutorial1/","page":"Basics","title":"Basics","text":"","category":"page"},{"location":"backends/#Backends","page":"Backends","title":"Backends","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"DifferentiationInterface.jl is based on two concepts: operators and backends. This page is about the latter, check out that page to learn about the former.","category":"page"},{"location":"backends/#List-of-backends","page":"Backends","title":"List of backends","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"We support the following dense backend choices from ADTypes.jl:","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"AutoChainRules\nAutoDiffractor\nAutoEnzyme\nAutoFastDifferentiation\nAutoFiniteDiff\nAutoFiniteDifferences\nAutoForwardDiff\nAutoPolyesterForwardDiff\nAutoReverseDiff\nAutoSymbolics\nAutoTapir\nAutoTracker\nAutoZygote","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"We also support the sparse wrapper AutoSparse.","category":"page"},{"location":"backends/#Compatibility","page":"Backends","title":"Compatibility","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"DifferentiationInterface.jl itself is compatible with Julia 1.6, the Long Term Support (LTS) version of the language. However, we were only able to test the following backends on Julia 1.6:","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"AutoFiniteDifferences\nAutoForwardDiff\nAutoReverseDiff\nAutoTracker\nAutoZygote","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"We strongly recommend that users upgrade to Julia 1.10 or above, where all backends are tested.","category":"page"},{"location":"backends/#Features","page":"Backends","title":"Features","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"Given a backend object, you can use:","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"check_available to know whether the required AD package is loaded\ncheck_inplace to know whether the backend supports in-place functions (all backends support out-of-place functions)","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"using ADTypes\nusing DifferentiationInterface\nimport Markdown\n\nimport ChainRulesCore\nimport Diffractor\nimport Enzyme\nimport FastDifferentiation\nimport FiniteDiff\nimport FiniteDifferences\nimport ForwardDiff\nimport PolyesterForwardDiff\nimport ReverseDiff\nimport Symbolics\nimport Tapir\nimport Tracker\nimport Zygote\n\nbackend_examples = [\n    AutoChainRules(; ruleconfig=Zygote.ZygoteRuleConfig()),\n    AutoDiffractor(),\n    AutoEnzyme(),\n    AutoFastDifferentiation(),\n    AutoFiniteDiff(),\n    AutoFiniteDifferences(; fdm=FiniteDifferences.central_fdm(3, 1)),\n    AutoForwardDiff(),\n    AutoPolyesterForwardDiff(; chunksize=1),\n    AutoReverseDiff(),\n    AutoSymbolics(),\n    AutoTapir(; safe_mode=false),\n    AutoTracker(),\n    AutoZygote(),\n]\n\ncheckmark(x::Bool) = x ? '✅' : '❌'\nunicode_check_available(backend) = checkmark(check_available(backend))\nunicode_check_inplace(backend)    = checkmark(check_inplace(backend))\n\nio = IOBuffer()\n\n# Table header \nprintln(io, \"| Backend | Availability | In-place functions |\")\nprintln(io, \"|:--------|:------------:|:----------------------:|\")\n\nfor b in backend_examples\n    join(io, [\"`$(nameof(typeof(b)))`\", unicode_check_available(b), unicode_check_inplace(b)], '|')\n    println(io, '|' )\nend\nbackend_table = Markdown.parse(String(take!(io)))","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"backend_table #hide","category":"page"},{"location":"backends/#Backend-switch","page":"Backends","title":"Backend switch","text":"","category":"section"},{"location":"backends/","page":"Backends","title":"Backends","text":"The wrapper DifferentiateWith allows you to switch between backends. It takes a function f and specifies that f should be differentiated with the backend of your choice, instead of whatever other backend the code is trying to use. In other words, when someone tries to differentiate dw = DifferentiateWith(f, backend1) with backend2, then backend1 steps in and backend2 does nothing. At the moment, DifferentiateWith only works when backend2 supports ChainRules.jl.","category":"page"},{"location":"backends/","page":"Backends","title":"Backends","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: DifferentiationInterface Logo)","category":"page"},{"location":"#DifferentiationInterface","page":"Home","title":"DifferentiationInterface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages) (Image: DOI)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Package Docs\nDifferentiationInterface (Image: Stable)     (Image: Dev)\nDifferentiationInterfaceTest (Image: Stable) (Image: Dev)","category":"page"},{"location":"","page":"Home","title":"Home","text":"An interface to various automatic differentiation (AD) backends in Julia.","category":"page"},{"location":"#Goal","page":"Home","title":"Goal","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a unified syntax to differentiate functions.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First- and second-order operators (gradients, Jacobians, Hessians and more)\nIn-place and out-of-place differentiation\nPreparation mechanism (e.g. to create a config or tape)\nBuilt-in sparsity handling\nThorough validation on standard inputs and outputs (numbers, vectors, matrices)\nTesting and benchmarking utilities accessible to users with DifferentiationInterfaceTest","category":"page"},{"location":"#Compatibility","page":"Home","title":"Compatibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We support all of the backends defined by ADTypes.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ChainRulesCore.jl\nDiffractor.jl\nEnzyme.jl\nFastDifferentiation.jl\nFiniteDiff.jl\nFiniteDifferences.jl\nForwardDiff.jl\nPolyesterForwardDiff.jl\nReverseDiff.jl\nSymbolics.jl\nTapir.jl\nTracker.jl\nZygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that in some cases, going through DifferentiationInterface.jl might be slower than a direct call to the backend's API. This is mostly true for Enzyme.jl, whose handling of activities and multiple arguments unlocks additional performance. We are working on this challenge, and welcome any suggestions or contributions. Meanwhile, if differentiation fails or takes too long, consider using Enzyme.jl directly.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the stable version of the package, run the following code in a Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\"DifferentiationInterface\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install the development version, run this instead:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\n\nPkg.add(\n    url=\"https://github.com/gdalle/DifferentiationInterface.jl\",\n    subdir=\"DifferentiationInterface\"\n)","category":"page"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using DifferentiationInterface\nimport ForwardDiff, Enzyme, Zygote  # AD backends you want to use \n\nf(x) = sum(abs2, x)\n\nx = [1.0, 2.0]\n\nvalue_and_gradient(f, AutoForwardDiff(), x) # returns (5.0, [2.0, 4.0]) with ForwardDiff.jl\nvalue_and_gradient(f, AutoEnzyme(),      x) # returns (5.0, [2.0, 4.0]) with Enzyme.jl\nvalue_and_gradient(f, AutoZygote(),      x) # returns (5.0, [2.0, 4.0]) with Zygote.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"To improve your performance by up to several orders of magnitude compared to this example, take a look at the DifferentiationInterface tutorial and its section on operator preparation.","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please cite both DifferentiationInterface.jl and its inspiration AbstractDifferentiation.jl, using the provided CITATION.bib file.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"tutorial2/#Sparsity","page":"Sparsity","title":"Sparsity","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"We present sparsity handling with DifferentiationInterface.jl.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"using BenchmarkTools\nusing DifferentiationInterface\nimport ForwardDiff, Zygote","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Sparse AD is very useful when Jacobian or Hessian matrices have a lot of zeros. So let us write functions that satisfy this property.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"f_sparse_vector(x::AbstractVector) = diff(x .^ 2) + diff(reverse(x .^ 2))\nf_sparse_scalar(x::AbstractVector) = sum(f_sparse_vector(x) .^ 2)\nnothing  # hide","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Let's also pick a random test vector.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"x = float.(1:8);","category":"page"},{"location":"tutorial2/#Dense-backends","page":"Sparsity","title":"Dense backends","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"When we use the jacobian or hessian operator with a dense backend, we get a dense matrix with plenty of zeros.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"dense_first_order_backend = AutoForwardDiff()\nJ_dense = jacobian(f_sparse_vector, dense_first_order_backend, x)","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"dense_second_order_backend = SecondOrder(AutoForwardDiff(), AutoZygote())\nH_dense = hessian(f_sparse_scalar, dense_second_order_backend, x)","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"The results are correct but the procedure is very slow. By using a sparse backend, we can get the runtime to increase with the number of nonzero elements, instead of the total number of elements.","category":"page"},{"location":"tutorial2/#Sparse-backends","page":"Sparsity","title":"Sparse backends","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Recipe to create a sparse backend: combine a dense backend, a sparsity detector and a compatible coloring algorithm inside AutoSparse. The following are reasonable defaults:","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"using SparseConnectivityTracer: TracerSparsityDetector\nusing SparseMatrixColorings: GreedyColoringAlgorithm\n\nsparse_first_order_backend = AutoSparse(\n    AutoForwardDiff();\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\n\nsparse_second_order_backend = AutoSparse(\n    SecondOrder(AutoForwardDiff(), AutoZygote());\n    sparsity_detector=TracerSparsityDetector(),\n    coloring_algorithm=GreedyColoringAlgorithm(),\n)\nnothing  # hide","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"Now the resulting matrices are sparse:","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"jacobian(f_sparse_vector, sparse_first_order_backend, x)","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"hessian(f_sparse_scalar, sparse_second_order_backend, x)","category":"page"},{"location":"tutorial2/#Sparse-preparation","page":"Sparsity","title":"Sparse preparation","text":"","category":"section"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"In the examples above, we didn't use preparation. Sparse preparation is more costly than dense preparation, but it is even more essential. Indeed, once preparation is done, sparse differentiation is much faster than dense differentiation, because it makes fewer calls to the underlying function. The speedup becomes very visible in large dimensions.","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"n = 1000\njac_extras_dense = prepare_jacobian(f_sparse_vector, dense_first_order_backend, zeros(n))\njac_extras_sparse = prepare_jacobian(f_sparse_vector, sparse_first_order_backend, zeros(n))\nnothing  # hide","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"@benchmark jacobian($f_sparse_vector, $jac_extras_dense, $dense_first_order_backend, $(randn(n)))","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"@benchmark jacobian($f_sparse_vector, $jac_extras_sparse, $sparse_first_order_backend, $(randn(n)))","category":"page"},{"location":"tutorial2/","page":"Sparsity","title":"Sparsity","text":"","category":"page"}]
}
